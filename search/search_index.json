{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"mlrose-ky: Machine Learning, Randomized Optimization and SEarch # mlrose-ky is a Python package for applying some of the most common randomized optimization and search algorithms to a range of different optimization problems, over both discrete- and continuous-valued parameter spaces. User Guide # Project Background Main Features Project Improvements over mlrose-hiive. Installation Licensing, Authors, Acknowledgements Tutorial 1 - Getting Started # What is an Optimization Problem? Why use Randomized Optimization? Solving Optimization Problems with mlrose-ky Define a Fitness Function Object Define an Optimization Problem Object Select and Run a Randomized Optimization Algorithm Summary References Tutorial 2 - Travelling Saleperson Problems # What is a Travelling Salesperson Problem? Solving TSPs with mlrose-ky Define a Fitness Function Object Define an Optimization Problem Object Select and Run a Randomized Optimization Algorithm Summary Tutorial 3 - ML Weight Optimization Problems # What is a Machine Learning Weight Optimization Problem? Solving Machine Learning Weight Optimization Problems with mlrose-ky Data Pre-Processing Neural Networks Linear and Logistic Regression Models Summary Tutorial 4 - Using Runners # An example with RHC runners Use runners to make your own custom wrapper API Reference # Algorithms Decay Schedules Optimization Problem Types Fitness Functions Machine Learning Weight Optimization","title":"\ud83c\udfda\ufe0f Home"},{"location":"#mlrose-ky-machine-learning-randomized-optimization-and-search","text":"mlrose-ky is a Python package for applying some of the most common randomized optimization and search algorithms to a range of different optimization problems, over both discrete- and continuous-valued parameter spaces.","title":"mlrose-ky: Machine Learning, Randomized Optimization and SEarch"},{"location":"#user-guide","text":"Project Background Main Features Project Improvements over mlrose-hiive. Installation Licensing, Authors, Acknowledgements","title":"User Guide"},{"location":"#tutorial-1-getting-started","text":"What is an Optimization Problem? Why use Randomized Optimization? Solving Optimization Problems with mlrose-ky Define a Fitness Function Object Define an Optimization Problem Object Select and Run a Randomized Optimization Algorithm Summary References","title":"Tutorial 1 - Getting Started"},{"location":"#tutorial-2-travelling-saleperson-problems","text":"What is a Travelling Salesperson Problem? Solving TSPs with mlrose-ky Define a Fitness Function Object Define an Optimization Problem Object Select and Run a Randomized Optimization Algorithm Summary","title":"Tutorial 2 - Travelling Saleperson Problems"},{"location":"#tutorial-3-ml-weight-optimization-problems","text":"What is a Machine Learning Weight Optimization Problem? Solving Machine Learning Weight Optimization Problems with mlrose-ky Data Pre-Processing Neural Networks Linear and Logistic Regression Models Summary","title":"Tutorial 3 - ML Weight Optimization Problems"},{"location":"#tutorial-4-using-runners","text":"An example with RHC runners Use runners to make your own custom wrapper","title":"Tutorial 4 - Using Runners"},{"location":"#api-reference","text":"Algorithms Decay Schedules Optimization Problem Types Fitness Functions Machine Learning Weight Optimization","title":"API Reference"},{"location":"about/","text":"Overview # mlrose-ky is a Python package for applying some of the most common randomized optimization and search algorithms to a range of different optimization problems, over both discrete- and continuous-valued parameter spaces. Project Background # mlrose-ky is a fork of the mlrose-hiive repository, which itself was a fork of the original mlrose repository. The original mlrose package was developed to support students of Georgia Tech's OMSCS/OMSA offering of CS 7641: Machine Learning. This repository includes implementations of all randomized optimization algorithms taught in the course, as well as functionality to apply these algorithms to integer-string optimization problems, such as N-Queens and the Knapsack problem; continuous-valued optimization problems, such as the neural network weight problem; and tour optimization problems, such as the Travelling Salesperson problem. It also has the flexibility to solve user-defined optimization problems. Main Features # Randomized Optimization Algorithms * Implementations of: hill climbing, randomized hill climbing, simulated annealing, genetic algorithm and (discrete) MIMIC; * Solve both maximization and minimization problems; * Define the algorithm\u2019s initial state or start from a random state; * Define your own simulated annealing decay schedule or use one of three pre-defined, customizable decay schedules: geometric decay, arithmetic decay or exponential decay. Problem Types * Solve discrete-value (bit-string and integer-string), continuous-value and tour optimization (travelling salesperson) problems; * Define your own fitness function for optimization or use a pre-defined function. * Pre-defined fitness functions exist for solving the: One Max, Flip Flop, Four Peaks, Six Peaks, Continuous Peaks, Knapsack, Travelling Salesperson, N-Queens and Max-K Color optimization problems. Machine Learning Weight Optimization * Optimize the weights of neural networks, linear regression models and logistic regression models using randomized hill climbing, simulated annealing, the genetic algorithm or gradient descent; * Supports classification and regression neural networks. Project Improvements and Updates # The mlrose-ky project is undergoing significant improvements to enhance code quality, documentation, and testing. Below is a list of tasks that have been completed or are in progress: Fix Python Warnings and Errors : All Python warnings and errors have been addressed, except for a few unavoidable ones like \"duplicate code.\" \u2705 Add Python 3.10 Type Hints : Type hints are being added to all function and method definitions, as well as method properties (e.g., self.foo: str = 'bar' ), to improve code clarity and maintainability. Enhance Documentation : NumPy-style docstrings are being added to all functions and methods, with at least a one-line docstring at the top of every file summarizing its contents. This will make the codebase more understandable and easier to use for others. Increase Test Coverage : Tests are being added using Pytest, with a goal of achieving 100% code coverage to ensure the robustness of the codebase. Resolve TODO/FIXME Comments : A thorough search is being conducted for any TODO, FIXME, or similar comments, and their respective issues are being resolved. Optimize Code : Vanilla Python loops are being optimized where possible by vectorizing them with NumPy to enhance performance. Improve Code Quality : Any other sub-optimal code, bugs, or code quality issues are being addressed to ensure a high standard of coding practices. Clean Up Codebase : All commented-out code is being removed to keep the codebase clean and maintainable. Installation # mlrose-ky was written in Python 3 and requires NumPy, SciPy and Scikit-Learn (sklearn). The latest released version is available at the Python package index and can be installed using pip: pip install mlrose-ky Once it is installed, simply import it like so: import mlrose_ky Licensing, Authors, Acknowledgements # mlrose-ky was forked from the mlrose-hiive repository, which was a fork of the original mlrose repository. The original mlrose was written by Genevieve Hayes and is distributed under the 3-Clause BSD license . You can cite mlrose-ky in research publications and reports as follows: Nakamura, K. (2024). mlrose-ky: Machine Learning, Randomized Optimization, and SEarch package for Python . https://github.com/knakamura13/mlrose-ky . Accessed: day month year . Please also keep the original authors' citations: Rollings, A. (2020). mlrose: Machine Learning, Randomized Optimization and SEarch package for Python, hiive extended remix . https://github.com/hiive/mlrose . Accessed: day month year . Hayes, G. (2019). mlrose: Machine Learning, Randomized Optimization and SEarch package for Python . https://github.com/gkhayes/mlrose . Accessed: day month year . Thanks to David S. Park for the MIMIC enhancements (from https://github.com/parkds/mlrose ). BibTeX entry: @misc{Nakamura24, author = {Nakamura, K.}, title = {{mlrose-ky: Machine Learning, Randomized Optimization and SEarch package for Python}}, year = 2024, howpublished = {\\url{https://github.com/knakamura13/mlrose-ky}}, note = {Accessed: day month year} } @misc{Hayes19, author = {Hayes, G}, title = {{mlrose: Machine Learning, Randomized Optimization and SEarch package for Python}}, year = 2019, howpublished = {\\url{https://github.com/gkhayes/mlrose}}, note = {Accessed: day month year} }","title":"\uff1f About"},{"location":"about/#overview","text":"mlrose-ky is a Python package for applying some of the most common randomized optimization and search algorithms to a range of different optimization problems, over both discrete- and continuous-valued parameter spaces.","title":"Overview"},{"location":"about/#project-background","text":"mlrose-ky is a fork of the mlrose-hiive repository, which itself was a fork of the original mlrose repository. The original mlrose package was developed to support students of Georgia Tech's OMSCS/OMSA offering of CS 7641: Machine Learning. This repository includes implementations of all randomized optimization algorithms taught in the course, as well as functionality to apply these algorithms to integer-string optimization problems, such as N-Queens and the Knapsack problem; continuous-valued optimization problems, such as the neural network weight problem; and tour optimization problems, such as the Travelling Salesperson problem. It also has the flexibility to solve user-defined optimization problems.","title":"Project Background"},{"location":"about/#main-features","text":"Randomized Optimization Algorithms * Implementations of: hill climbing, randomized hill climbing, simulated annealing, genetic algorithm and (discrete) MIMIC; * Solve both maximization and minimization problems; * Define the algorithm\u2019s initial state or start from a random state; * Define your own simulated annealing decay schedule or use one of three pre-defined, customizable decay schedules: geometric decay, arithmetic decay or exponential decay. Problem Types * Solve discrete-value (bit-string and integer-string), continuous-value and tour optimization (travelling salesperson) problems; * Define your own fitness function for optimization or use a pre-defined function. * Pre-defined fitness functions exist for solving the: One Max, Flip Flop, Four Peaks, Six Peaks, Continuous Peaks, Knapsack, Travelling Salesperson, N-Queens and Max-K Color optimization problems. Machine Learning Weight Optimization * Optimize the weights of neural networks, linear regression models and logistic regression models using randomized hill climbing, simulated annealing, the genetic algorithm or gradient descent; * Supports classification and regression neural networks.","title":"Main Features"},{"location":"about/#project-improvements-and-updates","text":"The mlrose-ky project is undergoing significant improvements to enhance code quality, documentation, and testing. Below is a list of tasks that have been completed or are in progress: Fix Python Warnings and Errors : All Python warnings and errors have been addressed, except for a few unavoidable ones like \"duplicate code.\" \u2705 Add Python 3.10 Type Hints : Type hints are being added to all function and method definitions, as well as method properties (e.g., self.foo: str = 'bar' ), to improve code clarity and maintainability. Enhance Documentation : NumPy-style docstrings are being added to all functions and methods, with at least a one-line docstring at the top of every file summarizing its contents. This will make the codebase more understandable and easier to use for others. Increase Test Coverage : Tests are being added using Pytest, with a goal of achieving 100% code coverage to ensure the robustness of the codebase. Resolve TODO/FIXME Comments : A thorough search is being conducted for any TODO, FIXME, or similar comments, and their respective issues are being resolved. Optimize Code : Vanilla Python loops are being optimized where possible by vectorizing them with NumPy to enhance performance. Improve Code Quality : Any other sub-optimal code, bugs, or code quality issues are being addressed to ensure a high standard of coding practices. Clean Up Codebase : All commented-out code is being removed to keep the codebase clean and maintainable.","title":"Project Improvements and Updates"},{"location":"about/#installation","text":"mlrose-ky was written in Python 3 and requires NumPy, SciPy and Scikit-Learn (sklearn). The latest released version is available at the Python package index and can be installed using pip: pip install mlrose-ky Once it is installed, simply import it like so: import mlrose_ky","title":"Installation"},{"location":"about/#licensing-authors-acknowledgements","text":"mlrose-ky was forked from the mlrose-hiive repository, which was a fork of the original mlrose repository. The original mlrose was written by Genevieve Hayes and is distributed under the 3-Clause BSD license . You can cite mlrose-ky in research publications and reports as follows: Nakamura, K. (2024). mlrose-ky: Machine Learning, Randomized Optimization, and SEarch package for Python . https://github.com/knakamura13/mlrose-ky . Accessed: day month year . Please also keep the original authors' citations: Rollings, A. (2020). mlrose: Machine Learning, Randomized Optimization and SEarch package for Python, hiive extended remix . https://github.com/hiive/mlrose . Accessed: day month year . Hayes, G. (2019). mlrose: Machine Learning, Randomized Optimization and SEarch package for Python . https://github.com/gkhayes/mlrose . Accessed: day month year . Thanks to David S. Park for the MIMIC enhancements (from https://github.com/parkds/mlrose ). BibTeX entry: @misc{Nakamura24, author = {Nakamura, K.}, title = {{mlrose-ky: Machine Learning, Randomized Optimization and SEarch package for Python}}, year = 2024, howpublished = {\\url{https://github.com/knakamura13/mlrose-ky}}, note = {Accessed: day month year} } @misc{Hayes19, author = {Hayes, G}, title = {{mlrose: Machine Learning, Randomized Optimization and SEarch package for Python}}, year = 2019, howpublished = {\\url{https://github.com/gkhayes/mlrose}}, note = {Accessed: day month year} }","title":"Licensing, Authors, Acknowledgements"},{"location":"algorithms/","text":"Algorithms # Functions to implement the randomized optimization and search algorithms. Recommendation The below functions are implemented within mlrose-ky. However, it is highly recommended to use the Runners for assignment. Hill Climbing # Use standard hill climbing to find the optimum for a given optimization problem. hill_climb ( problem , max_iters = float ( 'inf' ), restarts = 0 , init_state = None , curve = False , random_state = None ) Parameters [source] problem ( optimization object ) \u2013 Object containing fitness function optimization problem to be solved. For example, DiscreteOpt() , ContinuousOpt() or TSPOpt() . max_iters ( int, default: np.inf ) \u2013 Maximum number of iterations of the algorithm for each restart. restarts ( int, default: 0 ) \u2013 Number of random restarts. init_state ( array, default: None ) \u2013 1-D Numpy array containing starting state for algorithm. If None , then a random state is used. curve ( bool, default: False ) \u2013 Boolean to keep fitness values for a curve. If False , then no curve is stored. If True , then a history of fitness values is provided as a third return value. random_state ( int, default: None ) \u2013 If random_state is a positive integer, random_state is the seed used by np.random.seed(); otherwise, the random seed is not set. Returns: best_state ( array ) \u2013 Numpy array containing state that optimizes the fitness function. best_fitness ( float ) \u2013 Value of fitness function at best state. fitness_curve ( array ) \u2013 Numpy array containing the fitness at every iteration. Only returned if input argument curve is True . References # Russell, S. and P. Norvig (2010). Artificial Intelligence: A Modern Approach , 3rd edition. Prentice Hall, New Jersey, USA. Random Hill Climbing # Use randomized hill climbing to find the optimum for a given optimization problem. random_hill_climb ( problem , max_attempts = 10 , max_iters = float ( 'inf' ), restarts = 0 , init_state = None , curve = False , random_state = None ) Parameters [source] problem ( optimization object ) \u2013 Object containing fitness function optimization problem to be solved. For example, DiscreteOpt() , ContinuousOpt() or TSPOpt() . max_attempts ( int, default: 10 ) \u2013 Maximum number of attempts to find a better neighbor at each step. max_iters ( int, default: np.inf ) \u2013 Maximum number of iterations of the algorithm. restarts ( int, default: 0 ) \u2013 Number of random restarts. init_state ( array, default: None ) \u2013 1-D Numpy array containing starting state for algorithm. If None , then a random state is used. curve ( bool, default: False ) \u2013 Boolean to keep fitness values for a curve. If False , then no curve is stored. If True , then a history of fitness values is provided as a third return value. random_state ( int, default: None ) \u2013 If random_state is a positive integer, random_state is the seed used by np.random.seed(); otherwise, the random seed is not set. Returns : best_state ( array ) \u2013 Numpy array containing state that optimizes the fitness function. best_fitness ( float ) \u2013 Value of fitness function at best state. fitness_curve ( array ) \u2013 Numpy array containing the fitness at every iteration. Only returned if input argument curve is True . References # Brownlee, J (2011). Clever Algorithms: Nature-Inspired Programming Recipes . http://www.cleveralgorithms.com . Simulated Annealing # Use simulated annealing to find the optimum for a given optimization problem. simulated_annealing ( problem , schedule =< mlrose_ky . decay . GeomDecay object > , max_attempts = 10 , max_iters = float ( 'inf' ), init_state = None , curve = False , random_state = None ) Parameters [source] problem ( optimization object ) \u2013 Object containing fitness function optimization problem to be solved. For example, DiscreteOpt() , ContinuousOpt() or TSPOpt() . schedule (schedule object, default: mlrose_ky.GeomDecay() ) \u2013 Schedule used to determine the value of the temperature parameter. max_attempts ( int, default: 10 ) \u2013 Maximum number of attempts to find a better neighbor at each step. max_iters ( int, default: np.inf ) \u2013 Maximum number of iterations of the algorithm. init_state ( array, default: None ) \u2013 1-D Numpy array containing starting state for algorithm. If None , then a random state is used. curve ( bool, default: False ) \u2013 Boolean to keep fitness values for a curve. If False , then no curve is stored. If True , then a history of fitness values is provided as a third return value. random_state ( int, default: None ) \u2013 If random_state is a positive integer, random_state is the seed used by np.random.seed(); otherwise, the random seed is not set. Returns : best_state ( array ) \u2013 Numpy array containing state that optimizes the fitness function. best_fitness ( float ) \u2013 Value of fitness function at best state. fitness_curve ( array ) \u2013 Numpy array containing the fitness at every iteration. Only returned if input argument curve is True . References # Russell, S. and P. Norvig (2010). Artificial Intelligence: A Modern Approach , 3rd edition. Prentice Hall, New Jersey, USA. Genetic Algorithms # Use a standard genetic algorithm to find the optimum for a given optimization problem. genetic_alg ( problem , pop_size = 200 , mutation_prob = 0.1 , max_attempts = 10 , max_iters = float ( 'inf' ), curve = False , random_state = None ) Parameters [source] problem ( optimization object ) \u2013 Object containing fitness function optimization problem to be solved. For example, DiscreteOpt() , ContinuousOpt() or TSPOpt() . pop_size ( int, default: 200 ) \u2013 Size of population to be used in genetic algorithm. mutation_prob ( float, default: 0.1 ) \u2013 Probability of a mutation at each element of the state vector during reproduction, expressed as a value between 0 and 1. max_attempts ( int, default: 10 ) \u2013 Maximum number of attempts to find a better state at each step. max_iters ( int, default: np.inf ) \u2013 Maximum number of iterations of the algorithm. curve ( bool, default: False ) \u2013 Boolean to keep fitness values for a curve. If False , then no curve is stored. If True , then a history of fitness values is provided as a third return value. random_state ( int, default: None ) \u2013 If random_state is a positive integer, random_state is the seed used by np.random.seed(); otherwise, the random seed is not set. Returns : best_state ( array ) \u2013 Numpy array containing state that optimizes the fitness function. best_fitness ( float ) \u2013 Value of fitness function at best state. fitness_curve ( array ) \u2013 Numpy array of arrays containing the fitness of the entire population at every iteration. Only returned if input argument curve is True . References # Russell, S. and P. Norvig (2010). Artificial Intelligence: A Modern Approach , 3rd edition. Prentice Hall, New Jersey, USA. MIMIC # Use MIMIC to find the optimum for a given optimization problem. mimic ( problem , pop_size = 200 , keep_pct = 0.2 , max_attempts = 10 , max_iters = float ( 'inf' ), curve = False , random_state = None , fast_mimic = False ) Warning MIMIC cannot be used for solving continuous-state optimization problems. Parameters [source] problem ( optimization object ) \u2013 Object containing fitness function optimization problem to be solved. For example, DiscreteOpt() or TSPOpt() . pop_size ( int, default: 200 ) \u2013 Size of population to be used in algorithm. keep_pct ( float, default: 0.2 ) \u2013 Proportion of samples to keep at each iteration of the algorithm, expressed as a value between 0 and 1. max_attempts ( int, default: 10 ) \u2013 Maximum number of attempts to find a better neighbor at each step. max_iters ( int, default: np.inf ) \u2013 Maximum number of iterations of the algorithm. curve ( bool, default: False ) \u2013 Boolean to keep fitness values for a curve. If False , then no curve is stored. If True , then a history of fitness values is provided as a third return value. random_state ( int, default: None ) \u2013 If random_state is a positive integer, random_state is the seed used by np.random.seed(); otherwise, the random seed is not set. fast_mimic ( bool, default: False ) \u2013 Activate fast mimic mode to compute the mutual information in vectorized form. Faster speed but requires more memory. Returns : best_state ( array ) \u2013 Numpy array containing state that optimizes the fitness function. best_fitness ( float ) \u2013 Value of fitness function at best state. fitness_curve ( array ) \u2013 Numpy array containing the fitness at every iteration. Only returned if input argument curve is True . References # De Bonet, J., C. Isbell, and P. Viola (1997). MIMIC: Finding Optima by Estimating Probability Densities. In Advances in Neural Information Processing Systems (NIPS) 9, pp. 424\u2013430.","title":"\ud83d\udef9 Algorithms"},{"location":"algorithms/#algorithms","text":"Functions to implement the randomized optimization and search algorithms. Recommendation The below functions are implemented within mlrose-ky. However, it is highly recommended to use the Runners for assignment.","title":"Algorithms"},{"location":"algorithms/#hill-climbing","text":"Use standard hill climbing to find the optimum for a given optimization problem. hill_climb ( problem , max_iters = float ( 'inf' ), restarts = 0 , init_state = None , curve = False , random_state = None ) Parameters [source] problem ( optimization object ) \u2013 Object containing fitness function optimization problem to be solved. For example, DiscreteOpt() , ContinuousOpt() or TSPOpt() . max_iters ( int, default: np.inf ) \u2013 Maximum number of iterations of the algorithm for each restart. restarts ( int, default: 0 ) \u2013 Number of random restarts. init_state ( array, default: None ) \u2013 1-D Numpy array containing starting state for algorithm. If None , then a random state is used. curve ( bool, default: False ) \u2013 Boolean to keep fitness values for a curve. If False , then no curve is stored. If True , then a history of fitness values is provided as a third return value. random_state ( int, default: None ) \u2013 If random_state is a positive integer, random_state is the seed used by np.random.seed(); otherwise, the random seed is not set. Returns: best_state ( array ) \u2013 Numpy array containing state that optimizes the fitness function. best_fitness ( float ) \u2013 Value of fitness function at best state. fitness_curve ( array ) \u2013 Numpy array containing the fitness at every iteration. Only returned if input argument curve is True .","title":"Hill Climbing"},{"location":"algorithms/#references","text":"Russell, S. and P. Norvig (2010). Artificial Intelligence: A Modern Approach , 3rd edition. Prentice Hall, New Jersey, USA.","title":"References"},{"location":"algorithms/#random-hill-climbing","text":"Use randomized hill climbing to find the optimum for a given optimization problem. random_hill_climb ( problem , max_attempts = 10 , max_iters = float ( 'inf' ), restarts = 0 , init_state = None , curve = False , random_state = None ) Parameters [source] problem ( optimization object ) \u2013 Object containing fitness function optimization problem to be solved. For example, DiscreteOpt() , ContinuousOpt() or TSPOpt() . max_attempts ( int, default: 10 ) \u2013 Maximum number of attempts to find a better neighbor at each step. max_iters ( int, default: np.inf ) \u2013 Maximum number of iterations of the algorithm. restarts ( int, default: 0 ) \u2013 Number of random restarts. init_state ( array, default: None ) \u2013 1-D Numpy array containing starting state for algorithm. If None , then a random state is used. curve ( bool, default: False ) \u2013 Boolean to keep fitness values for a curve. If False , then no curve is stored. If True , then a history of fitness values is provided as a third return value. random_state ( int, default: None ) \u2013 If random_state is a positive integer, random_state is the seed used by np.random.seed(); otherwise, the random seed is not set. Returns : best_state ( array ) \u2013 Numpy array containing state that optimizes the fitness function. best_fitness ( float ) \u2013 Value of fitness function at best state. fitness_curve ( array ) \u2013 Numpy array containing the fitness at every iteration. Only returned if input argument curve is True .","title":"Random Hill Climbing"},{"location":"algorithms/#references_1","text":"Brownlee, J (2011). Clever Algorithms: Nature-Inspired Programming Recipes . http://www.cleveralgorithms.com .","title":"References"},{"location":"algorithms/#simulated-annealing","text":"Use simulated annealing to find the optimum for a given optimization problem. simulated_annealing ( problem , schedule =< mlrose_ky . decay . GeomDecay object > , max_attempts = 10 , max_iters = float ( 'inf' ), init_state = None , curve = False , random_state = None ) Parameters [source] problem ( optimization object ) \u2013 Object containing fitness function optimization problem to be solved. For example, DiscreteOpt() , ContinuousOpt() or TSPOpt() . schedule (schedule object, default: mlrose_ky.GeomDecay() ) \u2013 Schedule used to determine the value of the temperature parameter. max_attempts ( int, default: 10 ) \u2013 Maximum number of attempts to find a better neighbor at each step. max_iters ( int, default: np.inf ) \u2013 Maximum number of iterations of the algorithm. init_state ( array, default: None ) \u2013 1-D Numpy array containing starting state for algorithm. If None , then a random state is used. curve ( bool, default: False ) \u2013 Boolean to keep fitness values for a curve. If False , then no curve is stored. If True , then a history of fitness values is provided as a third return value. random_state ( int, default: None ) \u2013 If random_state is a positive integer, random_state is the seed used by np.random.seed(); otherwise, the random seed is not set. Returns : best_state ( array ) \u2013 Numpy array containing state that optimizes the fitness function. best_fitness ( float ) \u2013 Value of fitness function at best state. fitness_curve ( array ) \u2013 Numpy array containing the fitness at every iteration. Only returned if input argument curve is True .","title":"Simulated Annealing"},{"location":"algorithms/#references_2","text":"Russell, S. and P. Norvig (2010). Artificial Intelligence: A Modern Approach , 3rd edition. Prentice Hall, New Jersey, USA.","title":"References"},{"location":"algorithms/#genetic-algorithms","text":"Use a standard genetic algorithm to find the optimum for a given optimization problem. genetic_alg ( problem , pop_size = 200 , mutation_prob = 0.1 , max_attempts = 10 , max_iters = float ( 'inf' ), curve = False , random_state = None ) Parameters [source] problem ( optimization object ) \u2013 Object containing fitness function optimization problem to be solved. For example, DiscreteOpt() , ContinuousOpt() or TSPOpt() . pop_size ( int, default: 200 ) \u2013 Size of population to be used in genetic algorithm. mutation_prob ( float, default: 0.1 ) \u2013 Probability of a mutation at each element of the state vector during reproduction, expressed as a value between 0 and 1. max_attempts ( int, default: 10 ) \u2013 Maximum number of attempts to find a better state at each step. max_iters ( int, default: np.inf ) \u2013 Maximum number of iterations of the algorithm. curve ( bool, default: False ) \u2013 Boolean to keep fitness values for a curve. If False , then no curve is stored. If True , then a history of fitness values is provided as a third return value. random_state ( int, default: None ) \u2013 If random_state is a positive integer, random_state is the seed used by np.random.seed(); otherwise, the random seed is not set. Returns : best_state ( array ) \u2013 Numpy array containing state that optimizes the fitness function. best_fitness ( float ) \u2013 Value of fitness function at best state. fitness_curve ( array ) \u2013 Numpy array of arrays containing the fitness of the entire population at every iteration. Only returned if input argument curve is True .","title":"Genetic Algorithms"},{"location":"algorithms/#references_3","text":"Russell, S. and P. Norvig (2010). Artificial Intelligence: A Modern Approach , 3rd edition. Prentice Hall, New Jersey, USA.","title":"References"},{"location":"algorithms/#mimic","text":"Use MIMIC to find the optimum for a given optimization problem. mimic ( problem , pop_size = 200 , keep_pct = 0.2 , max_attempts = 10 , max_iters = float ( 'inf' ), curve = False , random_state = None , fast_mimic = False ) Warning MIMIC cannot be used for solving continuous-state optimization problems. Parameters [source] problem ( optimization object ) \u2013 Object containing fitness function optimization problem to be solved. For example, DiscreteOpt() or TSPOpt() . pop_size ( int, default: 200 ) \u2013 Size of population to be used in algorithm. keep_pct ( float, default: 0.2 ) \u2013 Proportion of samples to keep at each iteration of the algorithm, expressed as a value between 0 and 1. max_attempts ( int, default: 10 ) \u2013 Maximum number of attempts to find a better neighbor at each step. max_iters ( int, default: np.inf ) \u2013 Maximum number of iterations of the algorithm. curve ( bool, default: False ) \u2013 Boolean to keep fitness values for a curve. If False , then no curve is stored. If True , then a history of fitness values is provided as a third return value. random_state ( int, default: None ) \u2013 If random_state is a positive integer, random_state is the seed used by np.random.seed(); otherwise, the random seed is not set. fast_mimic ( bool, default: False ) \u2013 Activate fast mimic mode to compute the mutual information in vectorized form. Faster speed but requires more memory. Returns : best_state ( array ) \u2013 Numpy array containing state that optimizes the fitness function. best_fitness ( float ) \u2013 Value of fitness function at best state. fitness_curve ( array ) \u2013 Numpy array containing the fitness at every iteration. Only returned if input argument curve is True .","title":"MIMIC"},{"location":"algorithms/#references_4","text":"De Bonet, J., C. Isbell, and P. Viola (1997). MIMIC: Finding Optima by Estimating Probability Densities. In Advances in Neural Information Processing Systems (NIPS) 9, pp. 424\u2013430.","title":"References"},{"location":"decay/","text":"Decay Schedules # Classes for defining decay schedules for simulated annealing. Geometric Decay # Schedule for geometrically decaying the simulated annealing temperature parameter T according to the formula: Formula # \\[ T(t) = \\max(T_0 \\times r^t, T_{min}) \\] where - \\( T_0 \\) is the initial temperature (at time \\( t = 0 \\) ); - \\( r \\) is the rate of geometric decay; and - \\( T_{min} \\) is the minimum temperature value. Class declaration # class GeomDecay ( init_temp = 1.0 , decay = 0.99 , min_temp = 0.001 ) Parameters [source] init_temp ( float, default: 1.0 ) \u2013 Initial value of temperature parameter T. Must be greater than 0. decay ( float, default: 0.99 ) \u2013 Temperature decay parameter, r. Must be between 0 and 1. min_temp ( float, default: 0.001 ) \u2013 Minimum value of temperature parameter. Must be greater than 0. Class method # Evaluate the temperature parameter at time t. evaluate ( t ) Parameters : t ( int ) \u2013 Time at which the temperature parameter T is evaluated. Returns : temp ( float ) \u2013 Temperature parameter at time t. Example # >>> import mlrose_ky >>> schedule = mlrose_ky . GeomDecay ( init_temp = 10 , decay = 0.95 , min_temp = 1 ) >>> schedule . evaluate ( 5 ) 7.73780 ... Arithmetic Decay # Schedule for arithmetically decaying the simulated annealing temperature parameter T according to the formula: Formula # \\[ T(t) = \\max(T_{0} - rt, T_{min}) \\] where * \\( T_{0} \\) is the initial temperature (at time t = 0); * \\( r \\) is the rate of arithmetic decay; and * \\( T_{min} \\) is the minimum temperature value. Class declaration # class ArithDecay ( init_temp = 1.0 , decay = 0.0001 , min_temp = 0.001 ) Parameters [source] init_temp ( float, default: 1.0 ) \u2013 Initial value of temperature parameter T. Must be greater than 0. decay ( float, default: 0.0001 ) \u2013 Temperature decay parameter, r. Must be greater than 0. min_temp ( float, default: 0.001 ) \u2013 Minimum value of temperature parameter. Must be greater than 0. Class method # Evaluate the temperature parameter at time t. evaluate ( t ) Parameters : t ( int ) \u2013 Time at which the temperature paramter T is evaluated. Returns : temp ( float ) \u2013 Temperature parameter at time t. Example # >>> import mlrose_ky >>> schedule = mlrose_ky . ArithDecay ( init_temp \\ = 10 , decay = 0.95 , min_temp \\ = 1 ) >>> schedule . evaluate ( 5 ) 5.25 Exponential Decay # Schedule for exponentially decaying the simulated annealing temperature parameter T according to the formula. Formula # \\[ T(t) = \\max(T_{0} e^{-rt}, T_{min}) \\] where: * \\( T_{0} \\) is the initial temperature (at time t = 0); * \\( r \\) is the rate of arithmetic decay; and * \\( T_{min} \\) is the minimum temperature value. Class declaration # class ExpDecay ( init_temp = 1.0 , exp_const = 0.005 , min_temp = 0.001 ) Parameters [source] * init_temp ( float, default: 1.0 ) \u2013 Initial value of temperature parameter T. Must be greater than 0. * exp_const ( float, default: 0.005 ) \u2013 Exponential constant parameter, r. Must be greater than 0. * min_temp ( float, default: 0.001 ) \u2013 Minimum value of temperature parameter. Must be greater than 0. Class method # Evaluate the temperature parameter at time t. evaluate ( t ) Parameters : t ( int ) \u2013 Time at which the temperature paramter T is evaluated. Returns : temp ( float ) \u2013 Temperature parameter at time t. Example # >>> import mlrose_ky >>> schedule = mlrose_ky . ExpDecay ( init_temp = 10 , exp_const = 0.05 , min_temp = 1 ) >>> schedule . evaluate ( 5 ) 7.78800 ... Write your own custom schedule # Class for generating your own temperature schedule. class CustomSchedule ( schedule , ** kwargs ) Parameters [source] * schedule ( callable ) \u2013 Function for calculating the temperature at time t with the signature schedule(t, **kwargs) . * kwargs ( additional arguments ) \u2013 Additional parameters to be passed to schedule. Example # >>> import mlrose_ky >>> def custom ( t , c ): return t + c >>> kwargs \\ = { 'c' : 10 } >>> schedule \\ = mlrose_ky . CustomSchedule ( custom , \\ * \\ * kwargs ) >>> schedule . evaluate ( 5 ) 15 Class method # Evaluate the temperature parameter at time t. evaluate ( t ) Parameters : t ( int ) \u2013 Time at which the temperature paramter T is evaluated. Returns : temp ( float ) \u2013 Temperature parameter at time t.","title":"\ud83d\udcc5 Decay Schedules"},{"location":"decay/#decay-schedules","text":"Classes for defining decay schedules for simulated annealing.","title":"Decay Schedules"},{"location":"decay/#geometric-decay","text":"Schedule for geometrically decaying the simulated annealing temperature parameter T according to the formula:","title":"Geometric Decay"},{"location":"decay/#formula","text":"\\[ T(t) = \\max(T_0 \\times r^t, T_{min}) \\] where - \\( T_0 \\) is the initial temperature (at time \\( t = 0 \\) ); - \\( r \\) is the rate of geometric decay; and - \\( T_{min} \\) is the minimum temperature value.","title":"Formula"},{"location":"decay/#class-declaration","text":"class GeomDecay ( init_temp = 1.0 , decay = 0.99 , min_temp = 0.001 ) Parameters [source] init_temp ( float, default: 1.0 ) \u2013 Initial value of temperature parameter T. Must be greater than 0. decay ( float, default: 0.99 ) \u2013 Temperature decay parameter, r. Must be between 0 and 1. min_temp ( float, default: 0.001 ) \u2013 Minimum value of temperature parameter. Must be greater than 0.","title":"Class declaration"},{"location":"decay/#class-method","text":"Evaluate the temperature parameter at time t. evaluate ( t ) Parameters : t ( int ) \u2013 Time at which the temperature parameter T is evaluated. Returns : temp ( float ) \u2013 Temperature parameter at time t.","title":"Class method"},{"location":"decay/#example","text":">>> import mlrose_ky >>> schedule = mlrose_ky . GeomDecay ( init_temp = 10 , decay = 0.95 , min_temp = 1 ) >>> schedule . evaluate ( 5 ) 7.73780 ...","title":"Example"},{"location":"decay/#arithmetic-decay","text":"Schedule for arithmetically decaying the simulated annealing temperature parameter T according to the formula:","title":"Arithmetic Decay"},{"location":"decay/#formula_1","text":"\\[ T(t) = \\max(T_{0} - rt, T_{min}) \\] where * \\( T_{0} \\) is the initial temperature (at time t = 0); * \\( r \\) is the rate of arithmetic decay; and * \\( T_{min} \\) is the minimum temperature value.","title":"Formula"},{"location":"decay/#class-declaration_1","text":"class ArithDecay ( init_temp = 1.0 , decay = 0.0001 , min_temp = 0.001 ) Parameters [source] init_temp ( float, default: 1.0 ) \u2013 Initial value of temperature parameter T. Must be greater than 0. decay ( float, default: 0.0001 ) \u2013 Temperature decay parameter, r. Must be greater than 0. min_temp ( float, default: 0.001 ) \u2013 Minimum value of temperature parameter. Must be greater than 0.","title":"Class declaration"},{"location":"decay/#class-method_1","text":"Evaluate the temperature parameter at time t. evaluate ( t ) Parameters : t ( int ) \u2013 Time at which the temperature paramter T is evaluated. Returns : temp ( float ) \u2013 Temperature parameter at time t.","title":"Class method"},{"location":"decay/#example_1","text":">>> import mlrose_ky >>> schedule = mlrose_ky . ArithDecay ( init_temp \\ = 10 , decay = 0.95 , min_temp \\ = 1 ) >>> schedule . evaluate ( 5 ) 5.25","title":"Example"},{"location":"decay/#exponential-decay","text":"Schedule for exponentially decaying the simulated annealing temperature parameter T according to the formula.","title":"Exponential Decay"},{"location":"decay/#formula_2","text":"\\[ T(t) = \\max(T_{0} e^{-rt}, T_{min}) \\] where: * \\( T_{0} \\) is the initial temperature (at time t = 0); * \\( r \\) is the rate of arithmetic decay; and * \\( T_{min} \\) is the minimum temperature value.","title":"Formula"},{"location":"decay/#class-declaration_2","text":"class ExpDecay ( init_temp = 1.0 , exp_const = 0.005 , min_temp = 0.001 ) Parameters [source] * init_temp ( float, default: 1.0 ) \u2013 Initial value of temperature parameter T. Must be greater than 0. * exp_const ( float, default: 0.005 ) \u2013 Exponential constant parameter, r. Must be greater than 0. * min_temp ( float, default: 0.001 ) \u2013 Minimum value of temperature parameter. Must be greater than 0.","title":"Class declaration"},{"location":"decay/#class-method_2","text":"Evaluate the temperature parameter at time t. evaluate ( t ) Parameters : t ( int ) \u2013 Time at which the temperature paramter T is evaluated. Returns : temp ( float ) \u2013 Temperature parameter at time t.","title":"Class method"},{"location":"decay/#example_2","text":">>> import mlrose_ky >>> schedule = mlrose_ky . ExpDecay ( init_temp = 10 , exp_const = 0.05 , min_temp = 1 ) >>> schedule . evaluate ( 5 ) 7.78800 ...","title":"Example"},{"location":"decay/#write-your-own-custom-schedule","text":"Class for generating your own temperature schedule. class CustomSchedule ( schedule , ** kwargs ) Parameters [source] * schedule ( callable ) \u2013 Function for calculating the temperature at time t with the signature schedule(t, **kwargs) . * kwargs ( additional arguments ) \u2013 Additional parameters to be passed to schedule.","title":"Write your own custom schedule"},{"location":"decay/#example_3","text":">>> import mlrose_ky >>> def custom ( t , c ): return t + c >>> kwargs \\ = { 'c' : 10 } >>> schedule \\ = mlrose_ky . CustomSchedule ( custom , \\ * \\ * kwargs ) >>> schedule . evaluate ( 5 ) 15","title":"Example"},{"location":"decay/#class-method_3","text":"Evaluate the temperature parameter at time t. evaluate ( t ) Parameters : t ( int ) \u2013 Time at which the temperature paramter T is evaluated. Returns : temp ( float ) \u2013 Temperature parameter at time t.","title":"Class method"},{"location":"fitness/","text":"Fitness Functions # Classes for defining fitness functions. One Max # Formula # Fitness function for One Max optimization problem. Evaluates the fitness of a state vector \\( x = [x_0, x_1, \\dots, x_{n-1}] \\) as: \\[ \\text{Fitness}(x) = \\sum_{i=0}^{n-1} x_i \\] Class declaration # class OneMax Note The One Max fitness function is suitable for use in either discrete or continuous-state optimization problems. Class method # Evaluate the fitness of a state vector. evaluate ( state ) Parameters : state ( array ) \u2013 State array for evaluation. Returns : fitness ( float ) \u2013 Value of fitness function. Example # >>> import mlrose_ky >>> import numpy as np >>> fitness = mlrose_ky.OneMax() >>> state = np.array(\\[0, 1, 0, 1, 1, 1, 1\\]) >>> fitness.evaluate(state) 5 Flip Flops # Formula # Fitness function for Flip Flop optimization problem. Evaluates the fitness of a state vector \\( x \\) as the total number of pairs of consecutive elements of \\( x \\) , \\((x_i\\) and \\(x_{i+1})\\) where \\( x_i \\neq x_{i+1} \\) . Class declaration # class FlipFlop Note The Flip Flop fitness function is suitable for use in discrete-state optimization problems only . Class method # Evaluate the fitness of a state vector. evaluate () Parameters : state ( array ) \u2013 State array for evaluation. Returns : fitness ( float ) \u2013 Value of fitness function. Example # >>> import mlrose_ky >>> import numpy as np >>> fitness = mlrose_ky.FlipFlop() >>> state = np.array([0, 1, 0, 1, 1, 1, 1]) >>> fitness.evaluate(state) 3 Four Peaks # Fitness function for Four Peaks optimization problem. Formula # Evaluates the fitness of an n-dimensional state vector \\( x \\) , given parameter \\( T \\) , as: \\[\\text{Fitness}(x, T) = \\max(\\text{tail}(0, x), \\text{head}(1, x)) + R(x, T)\\] where: \\( \\text{tail}(b, x) \\) is the number of trailing \\( b \\) 's in \\( x \\) ; \\( \\text{head}(b, x) \\) is the number of leading \\( b \\) 's in \\( x \\) ; \\( R(x, T) = n \\) , if \\( \\text{tail}(0, x) > T \\) and \\( \\text{head}(1, x) > T \\) ; and \\( R(x, T) = 0 \\) , otherwise. Class declaration # class FourPeaks ( t_pct = 0.1 ) Note The Four Peaks fitness function is suitable for use in bit-string (discrete-state with max_val = 2 ) optimization problems only . Parameters: t_pct ( float , default: 0.1 ) \u2013 Threshold parameter ( \\( T \\) ) for Four Peaks fitness function, expressed as a percentage of the state space dimension, \\( n \\) (i.e. \\( T = t_{pct} \\times n \\) ). Class method # Evaluate the fitness of a state vector. evaluate ( state ) Parameters : state ( array ) \u2013 State array for evaluation. Returns : fitness ( float ) \u2013 Value of fitness function. Example # >>> import mlrose_ky >>> import numpy as np >>> fitness = mlrose_ky . FourPeaks ( t_pct = 0.15 ) >>> state = np . array ([ 1 , 1 , 1 , 0 , 1 , 0 , 0 , 1 , 0 , 0 , 0 , 0 ]) >>> fitness . evaluate ( state ) 16 References # De Bonet, J., C. Isbell, and P. Viola (1997). MIMIC: Finding Optima by Estimating Probability Densities. In Advances in Neural Information Processing Systems (NIPS) 9, pp. 424\u2013430. Six Peaks # Fitness function for Six Peaks optimization problem. Formula # Evaluates the fitness of an n-dimensional state vector \\( x \\) , given parameter \\( T \\) , as: \\[\\text{Fitness}(x, T) = \\max(\\text{tail}(0, x), \\text{head}(1, x)) + R(x, T)\\] where: \\( \\text{tail}(b, x) \\) is the number of trailing \\( b \\) 's in \\( x \\) ; \\( \\text{head}(b, x) \\) is the number of leading \\( b \\) 's in \\( x \\) ; \\( R(x, T) = n \\) , if \\( \\text{tail}(0, x) > T \\) and \\( \\text{head}(1, x) > T \\) or \\( \\text{tail}(1, x) > T \\) and \\( \\text{head}(0, x) > T \\) ; and \\( R(x, T) = 0 \\) , otherwise. Class declaration # class SixPeaks ( t_pct = 0.1 ) Note The Six Peaks fitness function is suitable for use in bit-string (discrete-state with max_val = 2 ) optimization problems only. Parameters: t_pct ( float , default: 0.1 ) \u2013 Threshold parameter ( \\( T \\) ) for Six Peaks fitness function, expressed as a percentage of the state space dimension, \\( n \\) (i.e. \\( T = t_{pct} \\times n \\) ). Class method # evaluate ( state ) Parameters : state ( array ) \u2013 State array for evaluation. Returns : fitness ( float ) \u2013 Value of fitness function. Example # >>> import mlrose_ky >>> import numpy as np >>> fitness = mlrose_ky . SixPeaks ( t_pct = 0.15 ) >>> state = np . array ([ 0 , 0 , 0 , 1 , 0 , 1 , 1 , 0 , 1 , 1 , 1 , 1 ]) >>> fitness . evaluate ( state ) 12 References # De Bonet, J., C. Isbell, and P. Viola (1997). MIMIC: Finding Optima by Estimating Probability Densities. In Advances in Neural Information Processing Systems (NIPS) 9, pp. 424\u2013430. Continuous Peaks # Fitness function for Continuous Peaks optimization problem. Formula # Evaluates the fitness of an n-dimensional state vector \\( x \\) , given parameter \\( T \\) , as: \\[ \\text{Fitness}(x, T) = \\max(\\text{max\\_run}(0, x), \\text{max\\_run}(1, x)) + R(x, T) \\] where: \\( \\text{max\\_run}(b, x) \\) is the length of the maximum run of \\( b \\) 's in \\( x \\) ; \\( R(x, T) = n \\) , if \\( \\text{max\\_run}(0, x) > T \\) and \\( \\text{max\\_run}(1, x) > T \\) ; and \\( R(x, T) = 0 \\) , otherwise. Class declaration # class ContinuousPeaks ( t_pct = 0.1 ) Note The Continuous Peaks fitness function is suitable for use in bit-string (discrete-state with max_val = 2 ) optimization problems only . Parameters: t_pct ( float , default: 0.1 ) \u2013 Threshold parameter ( \\( T \\) ) for Continuous Peaks fitness function, expressed as a percentage of the state space dimension, \\( n \\) (i.e. \\( T = t_{pct} \\times n \\) ). Class method # evaluate ( state ) Parameters : state ( array ) \u2013 State array for evaluation. Returns: fitness ( float ) \u2013 Value of fitness function. Example # >>> import mlrose_ky >>> import numpy as np >>> fitness = mlrose_ky . ContinuousPeaks ( t_pct = 0.15 ) >>> state = np . array ([ 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 ]) >>> fitness . evaluate ( state ) 17 Knapsack # Fitness function for Knapsack optimization problem. Formula # Given a set of \\( n \\) items, where item \\( i \\) has known weight \\( w_i \\) and known value \\( v_i \\) , and maximum knapsack capacity \\( W \\) , the Knapsack fitness function evaluates the fitness of a state vector \\( x = [x_0, x_1, \\dots, x_{n-1}] \\) as: \\[ \\text{Fitness}(x) = \\sum_{i=0}^{n-1} v_i x_i, \\quad \\text{if} \\quad \\sum_{i=0}^{n-1} w_i x_i \\leq W, \\quad \\text{and } 0, \\text{ otherwise}, \\] where \\( x_i \\) denotes the number of copies of item \\( i \\) included in the knapsack. Class declaration # class Knapsack ( weights , values , max_weight_pct = 0.35 ) Info The Knapsack fitness function is suitable for use in discrete-state optimization problems only . Parameters: - weights ( list ) \u2013 List of weights for each of the \\( n \\) items. - values ( list ) \u2013 List of values for each of the \\( n \\) items. - max_weight_pct ( float , default: 0.35 ) \u2013 Parameter used to set maximum capacity of knapsack ( \\( W \\) ) as a percentage of the total of the weights list ( \\( W = \\text{max_weight_pct} \\times \\text{total_weight} \\) ). Class method # evaluate ( state ) Parameters : state ( array ) \u2013 State array for evaluation. Returns: fitness ( float ) \u2013 Value of fitness function. Example # >>> import mlrose_ky >>> import numpy as np >>> weights = [ 10 , 5 , 2 , 8 , 15 ] >>> values = [ 1 , 2 , 3 , 4 , 5 ] >>> max_weight_pct = 0.6 >>> fitness = mlrose_ky . Knapsack ( weights , values , max_weight_pct ) >>> state = np . array ([ 1 , 0 , 2 , 1 , 0 ]) >>> fitness . evaluate ( state ) 11 Travelling Salesman (TSP) # Fitness function for Travelling Salesman optimization problem. Formula # Evaluates the fitness of a tour of n nodes, represented by state vector [x], giving the order in which the nodes are visited, as the total distance travelled on the tour (including the distance travelled between the final node in the state vector and the first node in the state vector during the return leg of the tour). Each node must be visited exactly once for a tour to be considered valid. Class declaration # class TravellingSales ( coords = None , distances = None ) Note The TravellingSales fitness function is suitable for use in travelling salesperson (tsp) optimization problems only . It is necessary to specify at least one of coords and distances in initializing a TravellingSales fitness function object. Parameters : * coords ( list of pairs, default: None ) \u2013 Ordered list of the (x, y) coordinates of all nodes (where element i gives the coordinates of node i). This assumes that travel between all pairs of nodes is possible. If this is not the case, then use distances instead. * distances ( list of triples, default: None ) \u2013 List giving the distances, d, between all pairs of nodes, u and v, for which travel is possible, with each list item in the form (u, v, d). Order of the nodes does not matter, so (u, v, d) and (v, u, d) are considered to be the same. If a pair is missing from the list, it is assumed that travel between the two nodes is not possible. This argument is ignored if coords is not None . Class method # evaluate ( state ) Parameters : state ( array ) \u2013 State array for evaluation. Returns: fitness ( float ) \u2013 Value of fitness function. Example # >>> import mlrose_ky >>> import numpy as np >>> coords = [( 0 , 0 ), ( 3 , 0 ), ( 3 , 2 ), ( 2 , 4 ), ( 1 , 3 )] >>> dists = [( 0 , 1 , 3 ), ( 0 , 2 , 5 ), ( 0 , 3 , 1 ), ( 0 , 4 , 7 ), ( 1 , 3 , 6 ), ( 4 , 1 , 9 ), ( 2 , 3 , 8 ), ( 2 , 4 , 2 ), ( 3 , 2 , 8 ), ( 3 , 4 , 4 )] >>> fitness_coords = mlrose_ky . TravellingSales ( coords = coords ) >>> state = np . array ([ 0 , 1 , 4 , 3 , 2 ]) >>> fitness_coords . evaluate ( state ) 13.86138 ... >>> fitness_dists = mlrose_ky . TravellingSales ( distances = dists ) >>> fitness_dists . evaluate ( state ) 29 N-Queens # Fitness function for N-Queens optimization problem. Formula # Evaluates the fitness of an n-dimensional state vector \\( x = [x_0, x_1, \\dots, x_{n-1}] \\) , where \\( x_i \\) represents the row position (between 0 and \\( n-1 \\) , inclusive) of the \u2018queen\u2019 in column \\( i \\) , as the number of pairs of attacking queens. Class declaration # class Queens Note The Queens fitness function is suitable for use in discrete-state optimization problem only. Class method # evaluate ( state ) Parameters : state ( array ) \u2013 State array for evaluation. Returns: fitness ( float ) \u2013 Value of fitness function. Example # >>> import mlrose_ky >>> import numpy as np >>> fitness = mlrose_ky . Queens () >>> state = np . array ([ 1 , 4 , 1 , 3 , 5 , 5 , 2 , 7 ]) >>> fitness . evaluate ( state ) 6 References # Russell, S. and P. Norvig (2010). Artificial Intelligence: A Modern Approach , 3rd edition. Prentice Hall, New Jersey, USA. Max K Color # Fitness function for Max-k color optimization problem. Formula # Fitness function for Max-k color optimization problem. Evaluates the fitness of an n-dimensional state vector \\( x = [x_0, x_1, \\dots, x_{n-1}] \\) , where \\( x_i \\) represents the color of node \\( i \\) , as the number of pairs of adjacent nodes of the same color. Class declaration # class MaxKColor ( edges ) Note The MaxKColor fitness function is suitable for use in discrete-state optimization problems only . Parameters : edges ( list of pairs ) \u2013 List of all pairs of connected nodes. Order does not matter, so (a, b) and (b, a) are considered to be the same. Class method # Evaluate the fitness of a state vector. evaluate ( state ) Parameters : state ( array ) \u2013 State array for evaluation. Returns: fitness ( float ) \u2013 Value of fitness function. Example # >>> import mlrose_ky >>> import numpy as np >>> edges = [( 0 , 1 ), ( 0 , 2 ), ( 0 , 4 ), ( 1 , 3 ), ( 2 , 0 ), ( 2 , 3 ), ( 3 , 4 )] >>> fitness = mlrose_ky . MaxKColor ( edges ) >>> state = np . array ([ 0 , 1 , 0 , 1 , 1 ]) >>> fitness . evaluate ( state ) 3 Write your own fitness function # Class for generating your own fitness function. Class declaration # class CustomFitness ( fitness_fn , problem_type = 'either' , ** kwargs ) Parameters: fitness_fn ( callable ) \u2013 Function for calculating fitness of a state with the signature fitness_fn(state, **kwargs) problem_type ( string, default: \u2018either\u2019 ) \u2013 Specifies problem type as \u2018discrete\u2019, \u2018continuous\u2019, \u2018tsp\u2019 or \u2018either\u2019 (denoting either discrete or continuous). kwargs ( additional arguments ) \u2013 Additional parameters to be passed to the fitness function. Class method # Evaluate the fitness of a state vector. evaluate ( state ) Parameters : state ( array ) \u2013 State array for evaluation. Returns : fitness ( float ) \u2013 Value of fitness function. Example # >>> import mlrose_ky >>> import numpy as np >>> def cust_fn ( state , c ): return c * np . sum ( state ) >>> kwargs = { 'c' : 10 } >>> fitness = mlrose_ky . CustomFitness ( cust_fn , ** kwargs ) >>> state = np . array ([ 1 , 2 , 3 , 4 , 5 ]) >>> fitness . evaluate ( state ) 150","title":"\ud83c\udfc3\ud83c\udffb Fitness Functions"},{"location":"fitness/#fitness-functions","text":"Classes for defining fitness functions.","title":"Fitness Functions"},{"location":"fitness/#one-max","text":"","title":"One Max"},{"location":"fitness/#formula","text":"Fitness function for One Max optimization problem. Evaluates the fitness of a state vector \\( x = [x_0, x_1, \\dots, x_{n-1}] \\) as: \\[ \\text{Fitness}(x) = \\sum_{i=0}^{n-1} x_i \\]","title":"Formula"},{"location":"fitness/#class-declaration","text":"class OneMax Note The One Max fitness function is suitable for use in either discrete or continuous-state optimization problems.","title":"Class declaration"},{"location":"fitness/#class-method","text":"Evaluate the fitness of a state vector. evaluate ( state ) Parameters : state ( array ) \u2013 State array for evaluation. Returns : fitness ( float ) \u2013 Value of fitness function.","title":"Class method"},{"location":"fitness/#example","text":">>> import mlrose_ky >>> import numpy as np >>> fitness = mlrose_ky.OneMax() >>> state = np.array(\\[0, 1, 0, 1, 1, 1, 1\\]) >>> fitness.evaluate(state) 5","title":"Example"},{"location":"fitness/#flip-flops","text":"","title":"Flip Flops"},{"location":"fitness/#formula_1","text":"Fitness function for Flip Flop optimization problem. Evaluates the fitness of a state vector \\( x \\) as the total number of pairs of consecutive elements of \\( x \\) , \\((x_i\\) and \\(x_{i+1})\\) where \\( x_i \\neq x_{i+1} \\) .","title":"Formula"},{"location":"fitness/#class-declaration_1","text":"class FlipFlop Note The Flip Flop fitness function is suitable for use in discrete-state optimization problems only .","title":"Class declaration"},{"location":"fitness/#class-method_1","text":"Evaluate the fitness of a state vector. evaluate () Parameters : state ( array ) \u2013 State array for evaluation. Returns : fitness ( float ) \u2013 Value of fitness function.","title":"Class method"},{"location":"fitness/#example_1","text":">>> import mlrose_ky >>> import numpy as np >>> fitness = mlrose_ky.FlipFlop() >>> state = np.array([0, 1, 0, 1, 1, 1, 1]) >>> fitness.evaluate(state) 3","title":"Example"},{"location":"fitness/#four-peaks","text":"Fitness function for Four Peaks optimization problem.","title":"Four Peaks"},{"location":"fitness/#formula_2","text":"Evaluates the fitness of an n-dimensional state vector \\( x \\) , given parameter \\( T \\) , as: \\[\\text{Fitness}(x, T) = \\max(\\text{tail}(0, x), \\text{head}(1, x)) + R(x, T)\\] where: \\( \\text{tail}(b, x) \\) is the number of trailing \\( b \\) 's in \\( x \\) ; \\( \\text{head}(b, x) \\) is the number of leading \\( b \\) 's in \\( x \\) ; \\( R(x, T) = n \\) , if \\( \\text{tail}(0, x) > T \\) and \\( \\text{head}(1, x) > T \\) ; and \\( R(x, T) = 0 \\) , otherwise.","title":"Formula"},{"location":"fitness/#class-declaration_2","text":"class FourPeaks ( t_pct = 0.1 ) Note The Four Peaks fitness function is suitable for use in bit-string (discrete-state with max_val = 2 ) optimization problems only . Parameters: t_pct ( float , default: 0.1 ) \u2013 Threshold parameter ( \\( T \\) ) for Four Peaks fitness function, expressed as a percentage of the state space dimension, \\( n \\) (i.e. \\( T = t_{pct} \\times n \\) ).","title":"Class declaration"},{"location":"fitness/#class-method_2","text":"Evaluate the fitness of a state vector. evaluate ( state ) Parameters : state ( array ) \u2013 State array for evaluation. Returns : fitness ( float ) \u2013 Value of fitness function.","title":"Class method"},{"location":"fitness/#example_2","text":">>> import mlrose_ky >>> import numpy as np >>> fitness = mlrose_ky . FourPeaks ( t_pct = 0.15 ) >>> state = np . array ([ 1 , 1 , 1 , 0 , 1 , 0 , 0 , 1 , 0 , 0 , 0 , 0 ]) >>> fitness . evaluate ( state ) 16","title":"Example"},{"location":"fitness/#references","text":"De Bonet, J., C. Isbell, and P. Viola (1997). MIMIC: Finding Optima by Estimating Probability Densities. In Advances in Neural Information Processing Systems (NIPS) 9, pp. 424\u2013430.","title":"References"},{"location":"fitness/#six-peaks","text":"Fitness function for Six Peaks optimization problem.","title":"Six Peaks"},{"location":"fitness/#formula_3","text":"Evaluates the fitness of an n-dimensional state vector \\( x \\) , given parameter \\( T \\) , as: \\[\\text{Fitness}(x, T) = \\max(\\text{tail}(0, x), \\text{head}(1, x)) + R(x, T)\\] where: \\( \\text{tail}(b, x) \\) is the number of trailing \\( b \\) 's in \\( x \\) ; \\( \\text{head}(b, x) \\) is the number of leading \\( b \\) 's in \\( x \\) ; \\( R(x, T) = n \\) , if \\( \\text{tail}(0, x) > T \\) and \\( \\text{head}(1, x) > T \\) or \\( \\text{tail}(1, x) > T \\) and \\( \\text{head}(0, x) > T \\) ; and \\( R(x, T) = 0 \\) , otherwise.","title":"Formula"},{"location":"fitness/#class-declaration_3","text":"class SixPeaks ( t_pct = 0.1 ) Note The Six Peaks fitness function is suitable for use in bit-string (discrete-state with max_val = 2 ) optimization problems only. Parameters: t_pct ( float , default: 0.1 ) \u2013 Threshold parameter ( \\( T \\) ) for Six Peaks fitness function, expressed as a percentage of the state space dimension, \\( n \\) (i.e. \\( T = t_{pct} \\times n \\) ).","title":"Class declaration"},{"location":"fitness/#class-method_3","text":"evaluate ( state ) Parameters : state ( array ) \u2013 State array for evaluation. Returns : fitness ( float ) \u2013 Value of fitness function.","title":"Class method"},{"location":"fitness/#example_3","text":">>> import mlrose_ky >>> import numpy as np >>> fitness = mlrose_ky . SixPeaks ( t_pct = 0.15 ) >>> state = np . array ([ 0 , 0 , 0 , 1 , 0 , 1 , 1 , 0 , 1 , 1 , 1 , 1 ]) >>> fitness . evaluate ( state ) 12","title":"Example"},{"location":"fitness/#references_1","text":"De Bonet, J., C. Isbell, and P. Viola (1997). MIMIC: Finding Optima by Estimating Probability Densities. In Advances in Neural Information Processing Systems (NIPS) 9, pp. 424\u2013430.","title":"References"},{"location":"fitness/#continuous-peaks","text":"Fitness function for Continuous Peaks optimization problem.","title":"Continuous Peaks"},{"location":"fitness/#formula_4","text":"Evaluates the fitness of an n-dimensional state vector \\( x \\) , given parameter \\( T \\) , as: \\[ \\text{Fitness}(x, T) = \\max(\\text{max\\_run}(0, x), \\text{max\\_run}(1, x)) + R(x, T) \\] where: \\( \\text{max\\_run}(b, x) \\) is the length of the maximum run of \\( b \\) 's in \\( x \\) ; \\( R(x, T) = n \\) , if \\( \\text{max\\_run}(0, x) > T \\) and \\( \\text{max\\_run}(1, x) > T \\) ; and \\( R(x, T) = 0 \\) , otherwise.","title":"Formula"},{"location":"fitness/#class-declaration_4","text":"class ContinuousPeaks ( t_pct = 0.1 ) Note The Continuous Peaks fitness function is suitable for use in bit-string (discrete-state with max_val = 2 ) optimization problems only . Parameters: t_pct ( float , default: 0.1 ) \u2013 Threshold parameter ( \\( T \\) ) for Continuous Peaks fitness function, expressed as a percentage of the state space dimension, \\( n \\) (i.e. \\( T = t_{pct} \\times n \\) ).","title":"Class declaration"},{"location":"fitness/#class-method_4","text":"evaluate ( state ) Parameters : state ( array ) \u2013 State array for evaluation. Returns: fitness ( float ) \u2013 Value of fitness function.","title":"Class method"},{"location":"fitness/#example_4","text":">>> import mlrose_ky >>> import numpy as np >>> fitness = mlrose_ky . ContinuousPeaks ( t_pct = 0.15 ) >>> state = np . array ([ 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 ]) >>> fitness . evaluate ( state ) 17","title":"Example"},{"location":"fitness/#knapsack","text":"Fitness function for Knapsack optimization problem.","title":"Knapsack"},{"location":"fitness/#formula_5","text":"Given a set of \\( n \\) items, where item \\( i \\) has known weight \\( w_i \\) and known value \\( v_i \\) , and maximum knapsack capacity \\( W \\) , the Knapsack fitness function evaluates the fitness of a state vector \\( x = [x_0, x_1, \\dots, x_{n-1}] \\) as: \\[ \\text{Fitness}(x) = \\sum_{i=0}^{n-1} v_i x_i, \\quad \\text{if} \\quad \\sum_{i=0}^{n-1} w_i x_i \\leq W, \\quad \\text{and } 0, \\text{ otherwise}, \\] where \\( x_i \\) denotes the number of copies of item \\( i \\) included in the knapsack.","title":"Formula"},{"location":"fitness/#class-declaration_5","text":"class Knapsack ( weights , values , max_weight_pct = 0.35 ) Info The Knapsack fitness function is suitable for use in discrete-state optimization problems only . Parameters: - weights ( list ) \u2013 List of weights for each of the \\( n \\) items. - values ( list ) \u2013 List of values for each of the \\( n \\) items. - max_weight_pct ( float , default: 0.35 ) \u2013 Parameter used to set maximum capacity of knapsack ( \\( W \\) ) as a percentage of the total of the weights list ( \\( W = \\text{max_weight_pct} \\times \\text{total_weight} \\) ).","title":"Class declaration"},{"location":"fitness/#class-method_5","text":"evaluate ( state ) Parameters : state ( array ) \u2013 State array for evaluation. Returns: fitness ( float ) \u2013 Value of fitness function.","title":"Class method"},{"location":"fitness/#example_5","text":">>> import mlrose_ky >>> import numpy as np >>> weights = [ 10 , 5 , 2 , 8 , 15 ] >>> values = [ 1 , 2 , 3 , 4 , 5 ] >>> max_weight_pct = 0.6 >>> fitness = mlrose_ky . Knapsack ( weights , values , max_weight_pct ) >>> state = np . array ([ 1 , 0 , 2 , 1 , 0 ]) >>> fitness . evaluate ( state ) 11","title":"Example"},{"location":"fitness/#travelling-salesman-tsp","text":"Fitness function for Travelling Salesman optimization problem.","title":"Travelling Salesman (TSP)"},{"location":"fitness/#formula_6","text":"Evaluates the fitness of a tour of n nodes, represented by state vector [x], giving the order in which the nodes are visited, as the total distance travelled on the tour (including the distance travelled between the final node in the state vector and the first node in the state vector during the return leg of the tour). Each node must be visited exactly once for a tour to be considered valid.","title":"Formula"},{"location":"fitness/#class-declaration_6","text":"class TravellingSales ( coords = None , distances = None ) Note The TravellingSales fitness function is suitable for use in travelling salesperson (tsp) optimization problems only . It is necessary to specify at least one of coords and distances in initializing a TravellingSales fitness function object. Parameters : * coords ( list of pairs, default: None ) \u2013 Ordered list of the (x, y) coordinates of all nodes (where element i gives the coordinates of node i). This assumes that travel between all pairs of nodes is possible. If this is not the case, then use distances instead. * distances ( list of triples, default: None ) \u2013 List giving the distances, d, between all pairs of nodes, u and v, for which travel is possible, with each list item in the form (u, v, d). Order of the nodes does not matter, so (u, v, d) and (v, u, d) are considered to be the same. If a pair is missing from the list, it is assumed that travel between the two nodes is not possible. This argument is ignored if coords is not None .","title":"Class declaration"},{"location":"fitness/#class-method_6","text":"evaluate ( state ) Parameters : state ( array ) \u2013 State array for evaluation. Returns: fitness ( float ) \u2013 Value of fitness function.","title":"Class method"},{"location":"fitness/#example_6","text":">>> import mlrose_ky >>> import numpy as np >>> coords = [( 0 , 0 ), ( 3 , 0 ), ( 3 , 2 ), ( 2 , 4 ), ( 1 , 3 )] >>> dists = [( 0 , 1 , 3 ), ( 0 , 2 , 5 ), ( 0 , 3 , 1 ), ( 0 , 4 , 7 ), ( 1 , 3 , 6 ), ( 4 , 1 , 9 ), ( 2 , 3 , 8 ), ( 2 , 4 , 2 ), ( 3 , 2 , 8 ), ( 3 , 4 , 4 )] >>> fitness_coords = mlrose_ky . TravellingSales ( coords = coords ) >>> state = np . array ([ 0 , 1 , 4 , 3 , 2 ]) >>> fitness_coords . evaluate ( state ) 13.86138 ... >>> fitness_dists = mlrose_ky . TravellingSales ( distances = dists ) >>> fitness_dists . evaluate ( state ) 29","title":"Example"},{"location":"fitness/#n-queens","text":"Fitness function for N-Queens optimization problem.","title":"N-Queens"},{"location":"fitness/#formula_7","text":"Evaluates the fitness of an n-dimensional state vector \\( x = [x_0, x_1, \\dots, x_{n-1}] \\) , where \\( x_i \\) represents the row position (between 0 and \\( n-1 \\) , inclusive) of the \u2018queen\u2019 in column \\( i \\) , as the number of pairs of attacking queens.","title":"Formula"},{"location":"fitness/#class-declaration_7","text":"class Queens Note The Queens fitness function is suitable for use in discrete-state optimization problem only.","title":"Class declaration"},{"location":"fitness/#class-method_7","text":"evaluate ( state ) Parameters : state ( array ) \u2013 State array for evaluation. Returns: fitness ( float ) \u2013 Value of fitness function.","title":"Class method"},{"location":"fitness/#example_7","text":">>> import mlrose_ky >>> import numpy as np >>> fitness = mlrose_ky . Queens () >>> state = np . array ([ 1 , 4 , 1 , 3 , 5 , 5 , 2 , 7 ]) >>> fitness . evaluate ( state ) 6","title":"Example"},{"location":"fitness/#references_2","text":"Russell, S. and P. Norvig (2010). Artificial Intelligence: A Modern Approach , 3rd edition. Prentice Hall, New Jersey, USA.","title":"References"},{"location":"fitness/#max-k-color","text":"Fitness function for Max-k color optimization problem.","title":"Max K Color"},{"location":"fitness/#formula_8","text":"Fitness function for Max-k color optimization problem. Evaluates the fitness of an n-dimensional state vector \\( x = [x_0, x_1, \\dots, x_{n-1}] \\) , where \\( x_i \\) represents the color of node \\( i \\) , as the number of pairs of adjacent nodes of the same color.","title":"Formula"},{"location":"fitness/#class-declaration_8","text":"class MaxKColor ( edges ) Note The MaxKColor fitness function is suitable for use in discrete-state optimization problems only . Parameters : edges ( list of pairs ) \u2013 List of all pairs of connected nodes. Order does not matter, so (a, b) and (b, a) are considered to be the same.","title":"Class declaration"},{"location":"fitness/#class-method_8","text":"Evaluate the fitness of a state vector. evaluate ( state ) Parameters : state ( array ) \u2013 State array for evaluation. Returns: fitness ( float ) \u2013 Value of fitness function.","title":"Class method"},{"location":"fitness/#example_8","text":">>> import mlrose_ky >>> import numpy as np >>> edges = [( 0 , 1 ), ( 0 , 2 ), ( 0 , 4 ), ( 1 , 3 ), ( 2 , 0 ), ( 2 , 3 ), ( 3 , 4 )] >>> fitness = mlrose_ky . MaxKColor ( edges ) >>> state = np . array ([ 0 , 1 , 0 , 1 , 1 ]) >>> fitness . evaluate ( state ) 3","title":"Example"},{"location":"fitness/#write-your-own-fitness-function","text":"Class for generating your own fitness function.","title":"Write your own fitness function"},{"location":"fitness/#class-declaration_9","text":"class CustomFitness ( fitness_fn , problem_type = 'either' , ** kwargs ) Parameters: fitness_fn ( callable ) \u2013 Function for calculating fitness of a state with the signature fitness_fn(state, **kwargs) problem_type ( string, default: \u2018either\u2019 ) \u2013 Specifies problem type as \u2018discrete\u2019, \u2018continuous\u2019, \u2018tsp\u2019 or \u2018either\u2019 (denoting either discrete or continuous). kwargs ( additional arguments ) \u2013 Additional parameters to be passed to the fitness function.","title":"Class declaration"},{"location":"fitness/#class-method_9","text":"Evaluate the fitness of a state vector. evaluate ( state ) Parameters : state ( array ) \u2013 State array for evaluation. Returns : fitness ( float ) \u2013 Value of fitness function.","title":"Class method"},{"location":"fitness/#example_9","text":">>> import mlrose_ky >>> import numpy as np >>> def cust_fn ( state , c ): return c * np . sum ( state ) >>> kwargs = { 'c' : 10 } >>> fitness = mlrose_ky . CustomFitness ( cust_fn , ** kwargs ) >>> state = np . array ([ 1 , 2 , 3 , 4 , 5 ]) >>> fitness . evaluate ( state ) 150","title":"Example"},{"location":"neural/","text":"Machine Learning Weight Optimization # Classes for defining neural network weight optimization problems. Neural Network # Class for defining neural network classifier weights optimization problem. class NeuralNetwork ( hidden_nodes = None , activation = 'relu' , algorithm = 'random_hill_climb' , max_iters = 100 , bias = True , is_classifier = True , learning_rate = 0.1 , early_stopping = False , clip_max = 1e10 , restarts = 0 , schedule =< mlrose_ky . decay . GeomDecay object > , pop_size = 200 , mutation_prob = 0.1 , max_attempts = 10 , random_state = None , curve = False ) Parameters : hidden_nodes ( list of ints ) \u2013 List giving the number of nodes in each hidden layer. activation ( string, default: \u2018relu\u2019 ) \u2013 Activation function for each of the hidden layers. Must be one of: \u2018identity\u2019, \u2018relu\u2019, \u2018sigmoid\u2019 or \u2018tanh\u2019. algorithm ( string, default: \u2018random_hill_climb\u2019 ) \u2013 Algorithm used to find optimal network weights. Must be one of:\u2019random_hill_climb\u2019, \u2018simulated_annealing\u2019, \u2018genetic_alg\u2019 or \u2018gradient_descent\u2019. max_iters ( int, default: 100 ) \u2013 Maximum number of iterations used to fit the weights. bias ( bool, default: True ) \u2013 Whether to include a bias term. is_classifer ( bool, default: True ) \u2013 Whether the network is for classification or regression. Set True for classification and False for regression. learning_rate ( float, default: 0.1 ) \u2013 Learning rate for gradient descent or step size for randomized optimization algorithms. early_stopping ( bool, default: False ) \u2013 Whether to terminate algorithm early if the loss is not improving. If True , then stop after max_attempts iters with no improvement. clip_max ( float, default: 1e+10 ) \u2013 Used to limit weights to the range [-1*clip_max, clip_max]. restarts ( int, default: 0 ) \u2013 Number of random restarts. Only required if algorithm = 'random_hill_climb' . schedule ( schedule object, default = mlrose_ky.GeomDecay() ) \u2013 Schedule used to determine the value of the temperature parameter. Only required if algorithm = 'simulated_annealing' . pop_size ( int, default: 200 ) \u2013 Size of population. Only required if algorithm = 'genetic_alg' . mutation_prob ( float, default: 0.1 ) \u2013 Probability of a mutation at each element of the state vector during reproduction, expressed as a value between 0 and 1. Only required if algorithm = 'genetic_alg' . max_attempts ( int, default: 10 ) \u2013 Maximum number of attempts to find a better state. Only required if early_stopping = True . random_state ( int, default: None ) \u2013 If random_state is a positive integer, random_state is the seed used by np.random.seed(); otherwise, the random seed is not set. curve ( bool, default: False ) \u2013 If bool is True, fitness_curve containing the fitness at each training iteration is returned. Variables: fitted_weights ( array ) \u2013 Numpy array giving the fitted weights when fit is performed. loss ( float ) \u2013 Value of loss function for fitted weights when fit is performed. predicted_probs ( array ) \u2013 Numpy array giving the predicted probabilities for each class when predict is performed for multi-class classification data; or the predicted probability for class 1 when predict is performed for binary classification data. fitness_curve ( array ) \u2013 Numpy array giving the fitness at each training iteration. Linear Regression # Class for defining linear regression weights optimization problem. Inherits fit and predict methods from NeuralNetwork() class. class LinearRegression ( algorithm = 'random_hill_climb' , max_iters = 100 , bias = True , learning_rate = 0.1 , early_stopping = False , clip_max = 1e10 , restarts = 0 , schedule =< mlrose_ky . decay . GeomDecay object > , pop_size = 200 , mutation_prob = 0.1 , max_attempts = 10 , random_state = None , curve = False ) Parameters : algorithm ( string, default: \u2018random_hill_climb\u2019 ) \u2013 Algorithm used to find optimal network weights. Must be one of:\u2019random_hill_climb\u2019, \u2018simulated_annealing\u2019, \u2018genetic_alg\u2019 or \u2018gradient_descent\u2019. max_iters ( int, default: 100 ) \u2013 Maximum number of iterations used to fit the weights. bias ( bool, default: True ) \u2013 Whether to include a bias term. learning_rate ( float, default: 0.1 ) \u2013 Learning rate for gradient descent or step size for randomized optimization algorithms. early_stopping ( bool, default: False ) \u2013 Whether to terminate algorithm early if the loss is not improving. If True , then stop after max_attempts iters with no improvement. clip_max ( float, default: 1e+10 ) \u2013 Used to limit weights to the range [-1*clip_max, clip_max]. restarts ( int, default: 0 ) \u2013 Number of random restarts. Only required if algorithm = 'random_hill_climb' . schedule ( schedule object, default = mlrose_ky.GeomDecay() ) \u2013 Schedule used to determine the value of the temperature parameter. Only required if algorithm = 'simulated_annealing' . pop_size ( int, default: 200 ) \u2013 Size of population. Only required if algorithm = 'genetic_alg' . mutation_prob ( float, default: 0.1 ) \u2013 Probability of a mutation at each element of the state vector during reproduction, expressed as a value between 0 and 1. Only required if algorithm = 'genetic_alg' . max_attempts ( int, default: 10 ) \u2013 Maximum number of attempts to find a better state. Only required if early_stopping = True . random_state ( int, default: None ) \u2013 If random_state is a positive integer, random_state is the seed used by np.random.seed(); otherwise, the random seed is not set. curve ( bool, default: False ) \u2013 If bool is true, curve containing the fitness at each training iteration is returned. Variables : fitted_weights ( array ) \u2013 Numpy array giving the fitted weights when fit is performed. loss ( float ) \u2013 Value of loss function for fitted weights when fit is performed. fitness_curve ( array ) \u2013 Numpy array giving the fitness at each training iteration. Logistic Regression # Class for defining logistic regression weights optimization problem. Inherits fit and predict methods from NeuralNetwork() class. class LogisticRegression ( algorithm = 'random_hill_climb' , max_iters = 100 , bias = True , learning_rate = 0.1 , early_stopping = False , clip_max = 1e10 , restarts = 0 , schedule =< mlrose_ky . decay . GeomDecay object > , pop_size = 200 , mutation_prob = 0.1 , max_attempts = 10 , random_state = None , curve = False ) Parameters : algorithm ( string, default: \u2018random_hill_climb\u2019 ) \u2013 Algorithm used to find optimal network weights. Must be one of:\u2019random_hill_climb\u2019, \u2018simulated_annealing\u2019, \u2018genetic_alg\u2019 or \u2018gradient_descent\u2019. max_iters ( int, default: 100 ) \u2013 Maximum number of iterations used to fit the weights. bias ( bool, default: True ) \u2013 Whether to include a bias term. learning_rate ( float, default: 0.1 ) \u2013 Learning rate for gradient descent or step size for randomized optimization algorithms. early_stopping ( bool, default: False ) \u2013 Whether to terminate algorithm early if the loss is not improving. If True , then stop after max_attempts iters with no improvement. clip_max ( float, default: 1e+10 ) \u2013 Used to limit weights to the range [-1*clip_max, clip_max]. restarts ( int, default: 0 ) \u2013 Number of random restarts. Only required if algorithm = 'random_hill_climb' . schedule ( schedule object, default = mlrose_ky.GeomDecay() ) \u2013 Schedule used to determine the value of the temperature parameter. Only required if algorithm = 'simulated_annealing' . pop_size ( int, default: 200 ) \u2013 Size of population. Only required if algorithm = 'genetic_alg' . mutation_prob ( float, default: 0.1 ) \u2013 Probability of a mutation at each element of the state vector during reproduction, expressed as a value between 0 and 1. Only required if algorithm = 'genetic_alg' . max_attempts ( int, default: 10 ) \u2013 Maximum number of attempts to find a better state. Only required if early_stopping = True . random_state ( int, default: None ) \u2013 If random_state is a positive integer, random_state is the seed used by np.random.seed(); otherwise, the random seed is not set. curve ( bool, default: False ) \u2013 If bool is true, curve containing the fitness at each training iteration is returned. Variables : fitted_weights ( array ) \u2013 Numpy array giving the fitted weights when fit is performed. loss ( float ) \u2013 Value of loss function for fitted weights when fit is performed. fitness_curve ( array ) \u2013 Numpy array giving the fitness at each training iteration.","title":"\ud83d\udcc9 Backpropagation using ROs"},{"location":"neural/#machine-learning-weight-optimization","text":"Classes for defining neural network weight optimization problems.","title":"Machine Learning Weight Optimization"},{"location":"neural/#neural-network","text":"Class for defining neural network classifier weights optimization problem. class NeuralNetwork ( hidden_nodes = None , activation = 'relu' , algorithm = 'random_hill_climb' , max_iters = 100 , bias = True , is_classifier = True , learning_rate = 0.1 , early_stopping = False , clip_max = 1e10 , restarts = 0 , schedule =< mlrose_ky . decay . GeomDecay object > , pop_size = 200 , mutation_prob = 0.1 , max_attempts = 10 , random_state = None , curve = False ) Parameters : hidden_nodes ( list of ints ) \u2013 List giving the number of nodes in each hidden layer. activation ( string, default: \u2018relu\u2019 ) \u2013 Activation function for each of the hidden layers. Must be one of: \u2018identity\u2019, \u2018relu\u2019, \u2018sigmoid\u2019 or \u2018tanh\u2019. algorithm ( string, default: \u2018random_hill_climb\u2019 ) \u2013 Algorithm used to find optimal network weights. Must be one of:\u2019random_hill_climb\u2019, \u2018simulated_annealing\u2019, \u2018genetic_alg\u2019 or \u2018gradient_descent\u2019. max_iters ( int, default: 100 ) \u2013 Maximum number of iterations used to fit the weights. bias ( bool, default: True ) \u2013 Whether to include a bias term. is_classifer ( bool, default: True ) \u2013 Whether the network is for classification or regression. Set True for classification and False for regression. learning_rate ( float, default: 0.1 ) \u2013 Learning rate for gradient descent or step size for randomized optimization algorithms. early_stopping ( bool, default: False ) \u2013 Whether to terminate algorithm early if the loss is not improving. If True , then stop after max_attempts iters with no improvement. clip_max ( float, default: 1e+10 ) \u2013 Used to limit weights to the range [-1*clip_max, clip_max]. restarts ( int, default: 0 ) \u2013 Number of random restarts. Only required if algorithm = 'random_hill_climb' . schedule ( schedule object, default = mlrose_ky.GeomDecay() ) \u2013 Schedule used to determine the value of the temperature parameter. Only required if algorithm = 'simulated_annealing' . pop_size ( int, default: 200 ) \u2013 Size of population. Only required if algorithm = 'genetic_alg' . mutation_prob ( float, default: 0.1 ) \u2013 Probability of a mutation at each element of the state vector during reproduction, expressed as a value between 0 and 1. Only required if algorithm = 'genetic_alg' . max_attempts ( int, default: 10 ) \u2013 Maximum number of attempts to find a better state. Only required if early_stopping = True . random_state ( int, default: None ) \u2013 If random_state is a positive integer, random_state is the seed used by np.random.seed(); otherwise, the random seed is not set. curve ( bool, default: False ) \u2013 If bool is True, fitness_curve containing the fitness at each training iteration is returned. Variables: fitted_weights ( array ) \u2013 Numpy array giving the fitted weights when fit is performed. loss ( float ) \u2013 Value of loss function for fitted weights when fit is performed. predicted_probs ( array ) \u2013 Numpy array giving the predicted probabilities for each class when predict is performed for multi-class classification data; or the predicted probability for class 1 when predict is performed for binary classification data. fitness_curve ( array ) \u2013 Numpy array giving the fitness at each training iteration.","title":"Neural Network"},{"location":"neural/#linear-regression","text":"Class for defining linear regression weights optimization problem. Inherits fit and predict methods from NeuralNetwork() class. class LinearRegression ( algorithm = 'random_hill_climb' , max_iters = 100 , bias = True , learning_rate = 0.1 , early_stopping = False , clip_max = 1e10 , restarts = 0 , schedule =< mlrose_ky . decay . GeomDecay object > , pop_size = 200 , mutation_prob = 0.1 , max_attempts = 10 , random_state = None , curve = False ) Parameters : algorithm ( string, default: \u2018random_hill_climb\u2019 ) \u2013 Algorithm used to find optimal network weights. Must be one of:\u2019random_hill_climb\u2019, \u2018simulated_annealing\u2019, \u2018genetic_alg\u2019 or \u2018gradient_descent\u2019. max_iters ( int, default: 100 ) \u2013 Maximum number of iterations used to fit the weights. bias ( bool, default: True ) \u2013 Whether to include a bias term. learning_rate ( float, default: 0.1 ) \u2013 Learning rate for gradient descent or step size for randomized optimization algorithms. early_stopping ( bool, default: False ) \u2013 Whether to terminate algorithm early if the loss is not improving. If True , then stop after max_attempts iters with no improvement. clip_max ( float, default: 1e+10 ) \u2013 Used to limit weights to the range [-1*clip_max, clip_max]. restarts ( int, default: 0 ) \u2013 Number of random restarts. Only required if algorithm = 'random_hill_climb' . schedule ( schedule object, default = mlrose_ky.GeomDecay() ) \u2013 Schedule used to determine the value of the temperature parameter. Only required if algorithm = 'simulated_annealing' . pop_size ( int, default: 200 ) \u2013 Size of population. Only required if algorithm = 'genetic_alg' . mutation_prob ( float, default: 0.1 ) \u2013 Probability of a mutation at each element of the state vector during reproduction, expressed as a value between 0 and 1. Only required if algorithm = 'genetic_alg' . max_attempts ( int, default: 10 ) \u2013 Maximum number of attempts to find a better state. Only required if early_stopping = True . random_state ( int, default: None ) \u2013 If random_state is a positive integer, random_state is the seed used by np.random.seed(); otherwise, the random seed is not set. curve ( bool, default: False ) \u2013 If bool is true, curve containing the fitness at each training iteration is returned. Variables : fitted_weights ( array ) \u2013 Numpy array giving the fitted weights when fit is performed. loss ( float ) \u2013 Value of loss function for fitted weights when fit is performed. fitness_curve ( array ) \u2013 Numpy array giving the fitness at each training iteration.","title":"Linear Regression"},{"location":"neural/#logistic-regression","text":"Class for defining logistic regression weights optimization problem. Inherits fit and predict methods from NeuralNetwork() class. class LogisticRegression ( algorithm = 'random_hill_climb' , max_iters = 100 , bias = True , learning_rate = 0.1 , early_stopping = False , clip_max = 1e10 , restarts = 0 , schedule =< mlrose_ky . decay . GeomDecay object > , pop_size = 200 , mutation_prob = 0.1 , max_attempts = 10 , random_state = None , curve = False ) Parameters : algorithm ( string, default: \u2018random_hill_climb\u2019 ) \u2013 Algorithm used to find optimal network weights. Must be one of:\u2019random_hill_climb\u2019, \u2018simulated_annealing\u2019, \u2018genetic_alg\u2019 or \u2018gradient_descent\u2019. max_iters ( int, default: 100 ) \u2013 Maximum number of iterations used to fit the weights. bias ( bool, default: True ) \u2013 Whether to include a bias term. learning_rate ( float, default: 0.1 ) \u2013 Learning rate for gradient descent or step size for randomized optimization algorithms. early_stopping ( bool, default: False ) \u2013 Whether to terminate algorithm early if the loss is not improving. If True , then stop after max_attempts iters with no improvement. clip_max ( float, default: 1e+10 ) \u2013 Used to limit weights to the range [-1*clip_max, clip_max]. restarts ( int, default: 0 ) \u2013 Number of random restarts. Only required if algorithm = 'random_hill_climb' . schedule ( schedule object, default = mlrose_ky.GeomDecay() ) \u2013 Schedule used to determine the value of the temperature parameter. Only required if algorithm = 'simulated_annealing' . pop_size ( int, default: 200 ) \u2013 Size of population. Only required if algorithm = 'genetic_alg' . mutation_prob ( float, default: 0.1 ) \u2013 Probability of a mutation at each element of the state vector during reproduction, expressed as a value between 0 and 1. Only required if algorithm = 'genetic_alg' . max_attempts ( int, default: 10 ) \u2013 Maximum number of attempts to find a better state. Only required if early_stopping = True . random_state ( int, default: None ) \u2013 If random_state is a positive integer, random_state is the seed used by np.random.seed(); otherwise, the random seed is not set. curve ( bool, default: False ) \u2013 If bool is true, curve containing the fitness at each training iteration is returned. Variables : fitted_weights ( array ) \u2013 Numpy array giving the fitted weights when fit is performed. loss ( float ) \u2013 Value of loss function for fitted weights when fit is performed. fitness_curve ( array ) \u2013 Numpy array giving the fitness at each training iteration.","title":"Logistic Regression"},{"location":"nn_class_with_sklearn/","text":"Using the NeuralNetwork Class with sklearn methods # by Nikhil Kapila mlrose-ky offers the flexibility to use the NeuralNetwork() class with the sklearn methods to find weights with the lowest loss (highest fitness scores) using randomized optimization algorithms. Since NeuralNetwork() is a child class of sklearn's ClassifierMixin, it has access to it's methods and can use sklearn functions too. In this example, we shall find weights using genetic algorithms (GA). Note: If you used torch in A1, then it's best to peruse the pyperch library instead. Otherwise, you'd have to re-do your A1 neural network experiments in sklearn. Import libraries # import mlrose_ky from mlrose_ky.neural.neural_network import NeuralNetwork import pandas as pd import numpy as np import matplotlib.pyplot as plt import sklearn.model_selection from sklearn.model_selection import learning_curve , LearningCurveDisplay , validation_curve , ValidationCurveDisplay from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split from sklearn.model_selection import KFold from sklearn.metrics import log_loss from sklearn.preprocessing import StandardScaler from matplotlib.ticker import ScalarFormatter import warnings from tqdm.auto import tqdm # You can use tqdm to have nice loading bars in your experiments. warnings . filterwarnings ( 'ignore' ) rng = 1 Loading the dataset and some pre-processing # We load the wine dataset, apply bins and perform scaling to reduce fit times. Please use the same pre-processing that you have done as in A1 so the algorithms work on the same target dataset as per A1. data = 'dataset/winequality-red.csv' ds = pd . read_csv ( data , sep = ';' ) ds [ 'quality' ] = pd . cut ( ds [ 'quality' ], bins = [ 1 , 5 , 10 ], labels = [ 'bad' , 'good' ]) # Encode good and bad labels quality_labels = { 'good' : 1 , 'bad' : 0 } ds [ 'quality' ] = ds [ 'quality' ] . replace ( quality_labels ) ds [ 'quality' ] . value_counts () # split dataframe into target and features X = ds . drop ( 'quality' , axis = 1 ) y = ds [ 'quality' ] # split data into training and testing sets X_train1 , X_test1 , y_train , y_test = train_test_split ( X , y , test_size = 0.3 , random_state = rng ) # scaling training data scaler = StandardScaler () X_train = scaler . fit_transform ( X_train1 ) X_test = scaler . transform ( X_test1 ) Instantiating the NeuralNetwork Object # We instantiate the NeuralNetwork() object and define the different pop sizes we want to perform validation on by keeping other parameters constant. nn_ga = NeuralNetwork ( hidden_nodes = [ 10 ], algorithm = 'genetic_alg' , max_iters = 10000 , is_classifier = True , learning_rate = 0.1 , early_stopping = False , max_attempts = 10 , random_state = rng , curve = True ) pop_sizes = [ 50 , 100 , 200 , 500 , 700 , 1000 ] Validation Curves # Here we use scikit-learn's validation_curve() method and ValidationCurveDisplayMethod(). Hear your computer go brrrr if you use n_jobs=-1! ga_train_pop_size , ga_valid_pop_size = validation_curve ( estimator = nn_ga , X = X_train , y = y_train , param_name = \"pop_size\" , param_range = pop_sizes , cv = 5 , scoring = \"accuracy\" , n_jobs =- 1 ) display = ValidationCurveDisplay ( param_name = 'Population Sizes' , param_range = np . array ( pop_sizes ), train_scores = ga_train_pop_size , test_scores = ga_valid_pop_size , score_name = 'Accuracy' ) param = { \"line_kw\" : { \"marker\" : \"o\" }} display . plot ( ** param ) plt . gca () . xaxis . set_major_formatter ( ScalarFormatter ( useMathText = True )) plt . xticks ( ticks = pop_size ) plt . title ( \"GA: Varying population sizes\" ) Learning curve # Once you have identified the best parameter combination using manual tuning, grid search or whatever library. You can make a new classifier to plot the learning curve. Here we identify that a pop_size of 100 and mutation probability of 25% works well so we go ahead and define the new classifier below. Once the new object is instantiated, we use learning_curve() and LearningCurveDisplay() methods to plot the learning curve. nn_ga_final = NeuralNetwork ( hidden_nodes = [ 10 ], algorithm = 'genetic_alg' , max_iters = 10000 , is_classifier = True , learning_rate = 0.1 , early_stopping = False , max_attempts = 10 , random_state = rng , curve = True , pop_size = 100 , mutation_prob = 0.25 ) train_size , train_score , valid_score = learning_curve ( estimator = nn_ga_final , X = X_train , y = y_train , random_state = rng , train_sizes = np . linspace ( .001 , 1 , 15 ), cv = 5 , n_jobs =- 1 ) display = LearningCurveDisplay ( train_sizes = train_size , train_scores = train_score , test_scores = valid_score , score_name = \"Score\" ) param = { \"line_kw\" : { \"marker\" : \"o\" }} display . plot ( ** param ) plt . title ( \"GA: Learning Curve\" ) Loss curve # Once .fit() is called on your final classifier. We can use the .fitness_curve on the object to get our loss curve out. The length is used as a training iteration and the second column gives us the loss values. nn_ga_final . fit ( X_train , y_train ) df = pd . DataFrame ( nn_ga_final . fitness_curve ) df . to_csv ( 'loss_curve_ga.csv' ) plt . plot ( np . arange ( len ( nn_ga_final . fitness_curve )), nn_ga_final . fitness_curve [:, 0 ]) plt . title ( \"Loss Curve for NN using GA\" ) plt . ylabel ( 'Loss' ) plt . xlabel ( 'Training Iteration' ) Computing performance on the test set. # Similar to sklearn, we can use the .predict() method to use our TRAINED GA classifier to make predictions on the test set. These predictions are then checked against original target labels to give us the accuracy. The .score() gives us the accuracy but you can use your own scoring metric based on what was chosen in A1. nn_ga_final . predict ( X_test ) array([[1], [0], [1], [1], [1], [1], [1], [0], [1], [0], [1], [0], [0], [1], [1], [1], [1], [0], [1], [0], [0], [1], [0], [1], [1], [1], [1], [1], [0], [1], [0], [1], [0], [1], [1], [1], [1], [0], [1], [0], [1], [0], [0], [1], [0], [1], [0], [0], [0], [1], [0], [0], [1], [0], [1], [1], [1], [1], [1], [0], [1], [0], [0], [1], [0], [1], [1], [0], [1], [1], [1], [1], [0], [0], [0], [0], [1], [0], [0], [0], [1], [0], [0], [0], [1], [0], [0], [1], [0], [1], [0], [1], [0], [1], [1], [0], [1], [1], [0], [1], [0], [0], [1], [0], [0], [0], [0], [0], [1], [0], [1], [0], [1], [0], [0], [1], [0], [0], [0], [0], [0], [1], [1], [0], [0], [0], [0], [0], [1], [1], [1], [0], [1], [0], [1], [1], [0], [1], [0], [0], [0], [0], [1], [0], [1], [1], [1], [0], [1], [0], [0], [1], [0], [0], [1], [1], [1], [1], [1], [0], [1], [1], [0], [0], [1], [1], [1], [0], [0], [1], [0], [1], [0], [1], [1], [0], [0], [0], [1], [0], [0], [1], [1], [0], [1], [1], [1], [0], [1], [0], [0], [1], [1], [1], [1], [0], [1], [1], [1], [1], [1], [0], [1], [0], [1], [1], [0], [0], [1], [1], [0], [1], [1], [1], [0], [0], [0], [1], [1], [1], [1], [0], [0], [0], [1], [0], [1], [1], [1], [1], [0], [0], [1], [1], [1], [1], [1], [0], [1], [0], [1], [1], [1], [0], [1], [1], [1], [0], [0], [0], [0], [0], [1], [1], [1], [0], [0], [0], [1], [1], [1], [0], [0], [0], [0], [1], [0], [0], [0], [1], [1], [1], [0], [1], [0], [1], [1], [1], [0], [1], [1], [0], [0], [0], [0], [1], [0], [0], [0], [1], [1], [1], [0], [0], [1], [1], [0], [1], [0], [0], [0], [1], [0], [1], [0], [0], [0], [1], [0], [0], [0], [1], [0], [1], [1], [1], [0], [0], [1], [1], [0], [1], [1], [0], [0], [1], [1], [1], [1], [1], [1], [0], [0], [1], [0], [0], [1], [1], [0], [0], [0], [0], [1], [0], [0], [1], [1], [1], [0], [0], [1], [1], [1], [1], [1], [0], [1], [1], [0], [0], [1], [1], [1], [1], [0], [0], [0], [1], [1], [1], [0], [0], [0], [0], [1], [1], [1], [1], [0], [1], [1], [1], [1], [0], [1], [1], [1], [1], [0], [1], [0], [0], [0], [1], [0], [1], [0], [1], [0], [1], [1], [0], [1], [0], [0], [1], [0], [0], [0], [1], [1], [0], [1], [1], [1], [1], [0], [1], [0], [0], [0], [1], [1], [1], [0], [1], [1], [0], [0], [1], [0], [1], [0], [0], [1], [0], [1], [0], [0], [1], [1], [0], [1], [1], [0], [0], [0], [0], [0], [0], [0], [0], [1], [1], [1], [1], [1], [0], [1], [0], [0], [0], [0], [1], [1], [0], [0], [0], [1], [1], [1], [1], [0], [1], [1], [1], [0], [0], [0], [1]]) nn_ga_final . score ( X = X_test , y = y_test ) 0.6854166666666667","title":"4 - How to use NeuralNetwork() with sklearn (EASY)"},{"location":"nn_class_with_sklearn/#using-the-neuralnetwork-class-with-sklearn-methods","text":"by Nikhil Kapila mlrose-ky offers the flexibility to use the NeuralNetwork() class with the sklearn methods to find weights with the lowest loss (highest fitness scores) using randomized optimization algorithms. Since NeuralNetwork() is a child class of sklearn's ClassifierMixin, it has access to it's methods and can use sklearn functions too. In this example, we shall find weights using genetic algorithms (GA). Note: If you used torch in A1, then it's best to peruse the pyperch library instead. Otherwise, you'd have to re-do your A1 neural network experiments in sklearn.","title":"Using the NeuralNetwork Class with sklearn methods"},{"location":"nn_class_with_sklearn/#import-libraries","text":"import mlrose_ky from mlrose_ky.neural.neural_network import NeuralNetwork import pandas as pd import numpy as np import matplotlib.pyplot as plt import sklearn.model_selection from sklearn.model_selection import learning_curve , LearningCurveDisplay , validation_curve , ValidationCurveDisplay from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split from sklearn.model_selection import KFold from sklearn.metrics import log_loss from sklearn.preprocessing import StandardScaler from matplotlib.ticker import ScalarFormatter import warnings from tqdm.auto import tqdm # You can use tqdm to have nice loading bars in your experiments. warnings . filterwarnings ( 'ignore' ) rng = 1","title":"Import libraries"},{"location":"nn_class_with_sklearn/#loading-the-dataset-and-some-pre-processing","text":"We load the wine dataset, apply bins and perform scaling to reduce fit times. Please use the same pre-processing that you have done as in A1 so the algorithms work on the same target dataset as per A1. data = 'dataset/winequality-red.csv' ds = pd . read_csv ( data , sep = ';' ) ds [ 'quality' ] = pd . cut ( ds [ 'quality' ], bins = [ 1 , 5 , 10 ], labels = [ 'bad' , 'good' ]) # Encode good and bad labels quality_labels = { 'good' : 1 , 'bad' : 0 } ds [ 'quality' ] = ds [ 'quality' ] . replace ( quality_labels ) ds [ 'quality' ] . value_counts () # split dataframe into target and features X = ds . drop ( 'quality' , axis = 1 ) y = ds [ 'quality' ] # split data into training and testing sets X_train1 , X_test1 , y_train , y_test = train_test_split ( X , y , test_size = 0.3 , random_state = rng ) # scaling training data scaler = StandardScaler () X_train = scaler . fit_transform ( X_train1 ) X_test = scaler . transform ( X_test1 )","title":"Loading the dataset and some pre-processing"},{"location":"nn_class_with_sklearn/#instantiating-the-neuralnetwork-object","text":"We instantiate the NeuralNetwork() object and define the different pop sizes we want to perform validation on by keeping other parameters constant. nn_ga = NeuralNetwork ( hidden_nodes = [ 10 ], algorithm = 'genetic_alg' , max_iters = 10000 , is_classifier = True , learning_rate = 0.1 , early_stopping = False , max_attempts = 10 , random_state = rng , curve = True ) pop_sizes = [ 50 , 100 , 200 , 500 , 700 , 1000 ]","title":"Instantiating the NeuralNetwork Object"},{"location":"nn_class_with_sklearn/#validation-curves","text":"Here we use scikit-learn's validation_curve() method and ValidationCurveDisplayMethod(). Hear your computer go brrrr if you use n_jobs=-1! ga_train_pop_size , ga_valid_pop_size = validation_curve ( estimator = nn_ga , X = X_train , y = y_train , param_name = \"pop_size\" , param_range = pop_sizes , cv = 5 , scoring = \"accuracy\" , n_jobs =- 1 ) display = ValidationCurveDisplay ( param_name = 'Population Sizes' , param_range = np . array ( pop_sizes ), train_scores = ga_train_pop_size , test_scores = ga_valid_pop_size , score_name = 'Accuracy' ) param = { \"line_kw\" : { \"marker\" : \"o\" }} display . plot ( ** param ) plt . gca () . xaxis . set_major_formatter ( ScalarFormatter ( useMathText = True )) plt . xticks ( ticks = pop_size ) plt . title ( \"GA: Varying population sizes\" )","title":"Validation Curves"},{"location":"nn_class_with_sklearn/#learning-curve","text":"Once you have identified the best parameter combination using manual tuning, grid search or whatever library. You can make a new classifier to plot the learning curve. Here we identify that a pop_size of 100 and mutation probability of 25% works well so we go ahead and define the new classifier below. Once the new object is instantiated, we use learning_curve() and LearningCurveDisplay() methods to plot the learning curve. nn_ga_final = NeuralNetwork ( hidden_nodes = [ 10 ], algorithm = 'genetic_alg' , max_iters = 10000 , is_classifier = True , learning_rate = 0.1 , early_stopping = False , max_attempts = 10 , random_state = rng , curve = True , pop_size = 100 , mutation_prob = 0.25 ) train_size , train_score , valid_score = learning_curve ( estimator = nn_ga_final , X = X_train , y = y_train , random_state = rng , train_sizes = np . linspace ( .001 , 1 , 15 ), cv = 5 , n_jobs =- 1 ) display = LearningCurveDisplay ( train_sizes = train_size , train_scores = train_score , test_scores = valid_score , score_name = \"Score\" ) param = { \"line_kw\" : { \"marker\" : \"o\" }} display . plot ( ** param ) plt . title ( \"GA: Learning Curve\" )","title":"Learning curve"},{"location":"nn_class_with_sklearn/#loss-curve","text":"Once .fit() is called on your final classifier. We can use the .fitness_curve on the object to get our loss curve out. The length is used as a training iteration and the second column gives us the loss values. nn_ga_final . fit ( X_train , y_train ) df = pd . DataFrame ( nn_ga_final . fitness_curve ) df . to_csv ( 'loss_curve_ga.csv' ) plt . plot ( np . arange ( len ( nn_ga_final . fitness_curve )), nn_ga_final . fitness_curve [:, 0 ]) plt . title ( \"Loss Curve for NN using GA\" ) plt . ylabel ( 'Loss' ) plt . xlabel ( 'Training Iteration' )","title":"Loss curve"},{"location":"nn_class_with_sklearn/#computing-performance-on-the-test-set","text":"Similar to sklearn, we can use the .predict() method to use our TRAINED GA classifier to make predictions on the test set. These predictions are then checked against original target labels to give us the accuracy. The .score() gives us the accuracy but you can use your own scoring metric based on what was chosen in A1. nn_ga_final . predict ( X_test ) array([[1], [0], [1], [1], [1], [1], [1], [0], [1], [0], [1], [0], [0], [1], [1], [1], [1], [0], [1], [0], [0], [1], [0], [1], [1], [1], [1], [1], [0], [1], [0], [1], [0], [1], [1], [1], [1], [0], [1], [0], [1], [0], [0], [1], [0], [1], [0], [0], [0], [1], [0], [0], [1], [0], [1], [1], [1], [1], [1], [0], [1], [0], [0], [1], [0], [1], [1], [0], [1], [1], [1], [1], [0], [0], [0], [0], [1], [0], [0], [0], [1], [0], [0], [0], [1], [0], [0], [1], [0], [1], [0], [1], [0], [1], [1], [0], [1], [1], [0], [1], [0], [0], [1], [0], [0], [0], [0], [0], [1], [0], [1], [0], [1], [0], [0], [1], [0], [0], [0], [0], [0], [1], [1], [0], [0], [0], [0], [0], [1], [1], [1], [0], [1], [0], [1], [1], [0], [1], [0], [0], [0], [0], [1], [0], [1], [1], [1], [0], [1], [0], [0], [1], [0], [0], [1], [1], [1], [1], [1], [0], [1], [1], [0], [0], [1], [1], [1], [0], [0], [1], [0], [1], [0], [1], [1], [0], [0], [0], [1], [0], [0], [1], [1], [0], [1], [1], [1], [0], [1], [0], [0], [1], [1], [1], [1], [0], [1], [1], [1], [1], [1], [0], [1], [0], [1], [1], [0], [0], [1], [1], [0], [1], [1], [1], [0], [0], [0], [1], [1], [1], [1], [0], [0], [0], [1], [0], [1], [1], [1], [1], [0], [0], [1], [1], [1], [1], [1], [0], [1], [0], [1], [1], [1], [0], [1], [1], [1], [0], [0], [0], [0], [0], [1], [1], [1], [0], [0], [0], [1], [1], [1], [0], [0], [0], [0], [1], [0], [0], [0], [1], [1], [1], [0], [1], [0], [1], [1], [1], [0], [1], [1], [0], [0], [0], [0], [1], [0], [0], [0], [1], [1], [1], [0], [0], [1], [1], [0], [1], [0], [0], [0], [1], [0], [1], [0], [0], [0], [1], [0], [0], [0], [1], [0], [1], [1], [1], [0], [0], [1], [1], [0], [1], [1], [0], [0], [1], [1], [1], [1], [1], [1], [0], [0], [1], [0], [0], [1], [1], [0], [0], [0], [0], [1], [0], [0], [1], [1], [1], [0], [0], [1], [1], [1], [1], [1], [0], [1], [1], [0], [0], [1], [1], [1], [1], [0], [0], [0], [1], [1], [1], [0], [0], [0], [0], [1], [1], [1], [1], [0], [1], [1], [1], [1], [0], [1], [1], [1], [1], [0], [1], [0], [0], [0], [1], [0], [1], [0], [1], [0], [1], [1], [0], [1], [0], [0], [1], [0], [0], [0], [1], [1], [0], [1], [1], [1], [1], [0], [1], [0], [0], [0], [1], [1], [1], [0], [1], [1], [0], [0], [1], [0], [1], [0], [0], [1], [0], [1], [0], [0], [1], [1], [0], [1], [1], [0], [0], [0], [0], [0], [0], [0], [0], [1], [1], [1], [1], [1], [0], [1], [0], [0], [0], [0], [1], [1], [0], [0], [0], [1], [1], [1], [1], [0], [1], [1], [1], [0], [0], [0], [1]]) nn_ga_final . score ( X = X_test , y = y_test ) 0.6854166666666667","title":"Computing performance on the test set."},{"location":"nn_examples/","text":"mlrose_ky Generator and Neural Network Runner Usage Examples - Andrew Rollings # Overview # These examples will not solve assignment 2 for you, but they will give you some idea on how to use the problem generator and runner classes. Hopefully this will result in slightly fewer \"How do I <insert basic usage here>?\" questions every semester... Also, and in case it hasn't been made clear enough by the TAs, using any of the visualization code in here for your assignment is a bad idea, for two reasons... (1) It provides nothing useful as far as the assignment goes, and (2) the TAs will undoubtedly frown upon it. Visualization is part of the analysis and, for the most part, you're supposed to do that by yourself. Just including images of the before/after state of a problem really isn't useful in terms of what you're supposed to be analyzing. I also strongly recommend against using the synthetic data class used within for your assignment. It's a toy, with no real value for the assignment. There's much better data out there. Import Libraries # from IPython.core.display import display , HTML # for some notebook formatting. import mlrose_ky import numpy as np import pandas as pd import logging import networkx as nx import matplotlib.pyplot as plt import string from sklearn.model_selection import train_test_split from sklearn.preprocessing import MinMaxScaler , OneHotEncoder from sklearn.metrics import accuracy_score from mlrose_ky import SyntheticData , plot_synthetic_dataset from mlrose_ky import SKMLPRunner , SARunner , GARunner , NNGSRunner # switch off the chatter logging . basicConfig ( level = logging . WARNING ) Generating sample data... # This sample data will be used in subsequent examples. sd = SyntheticData ( seed = 123456 ) clean_data , clean_features , clean_classes , _ = sd . get_synthetic_data ( x_dim = 20 , y_dim = 20 ) print ( f \"Features: { clean_features } \" ) print ( f \"Classes: { clean_classes } \" ) cx , cy , cx_tr , cx_ts , cy_tr , cy_ts = sd . setup_synthetic_data_test_train ( clean_data ) plot_synthetic_dataset ( x_train = cx_tr , y_train = cy_tr , x_test = cx_ts , y_test = cy_ts , transparent_bg = False , bg_color = \"black\" ) Features: ['(1) A', '(2) B'] Classes: ['RED', 'BLUE'] This is our standard 20x20 sample set with no noise or errors, with a 70/30 split between training and test data. Each point represents a data sample. Black-outlined points represent training data, and white-outlined points represent test data. A sample of the clean data can be seen below, with the first two columns being the (x,y) position of the data point, and the third column representing the color. HTML ( pd . DataFrame ( columns = [ \"x\" , \"y\" , \"c\" ], data = clean_data )[ 150 : 160 ] . to_html ()) x y c 150 7 10 0 151 7 11 0 152 7 12 0 153 7 13 1 154 7 14 1 155 7 15 1 156 7 16 1 157 7 17 1 158 7 18 1 159 7 19 1 We can generate \"dirty\" data in two ways: by adding an column of random data, or by introducing noisy duplicate data points that either reinforce or contradict existing data points. noisy_data , noisy_features , noisy_classes , _ = sd . get_synthetic_data ( x_dim = 20 , y_dim = 20 , add_noise = 0.025 ) print ( f \"Features: { noisy_features } \" ) print ( f \"Classes: { noisy_classes } \" ) nx , ny , nx_tr , nx_ts , ny_tr , ny_ts = sd . setup_synthetic_data_test_train ( noisy_data ) plot_synthetic_dataset ( x_train = nx_tr , y_train = ny_tr , x_test = nx_ts , y_test = ny_ts , transparent_bg = False , bg_color = \"black\" ) Features: ['(1) A', '(2) B'] Classes: ['RED', 'BLUE'] This is our data with 2.5% added noise. A sample of the noisy data can be seen below, with the first two columns being the (x,y) position of the data point, and the third column representing the color. Note that the sample shown represent some of the noisy data, which is added to the end of the clean data. HTML ( pd . DataFrame ( columns = [ \"x\" , \"y\" , \"c\" ], data = noisy_data )[ - 10 :] . to_html ()) x y c 410 17 17 1 411 15 18 0 412 15 19 0 413 0 6 0 414 10 14 1 415 7 8 1 416 10 19 1 417 10 9 0 418 16 1 0 419 6 0 1 If we had chosen to add the random column of data, then the plots would look that same as the above two, depending on whether we had chosen to additionally add noise or not. However, the features are different, as shown below, and we can see the extra column in the data sample. extra_data , extra_features , extra_classes , _ = sd . get_synthetic_data ( x_dim = 20 , y_dim = 20 , add_redundant_column = True ) print ( f \"Features: { extra_features } \" ) print ( f \"Classes: { extra_classes } \" ) ex , ey , ex_tr , ex_ts , ey_tr , ey_ts = sd . setup_synthetic_data_test_train ( extra_data ) Features: ['(1) A', '(2) B', '(3) R'] Classes: ['RED', 'BLUE'] HTML ( pd . DataFrame ( columns = [ \"x\" , \"y\" , \"r\" , \"c\" ], data = extra_data )[ 150 : 160 ] . to_html ()) x y r c 150 7.0 10.0 0.972641 0.0 151 7.0 11.0 0.726259 0.0 152 7.0 12.0 0.412651 0.0 153 7.0 13.0 0.990003 1.0 154 7.0 14.0 0.535660 1.0 155 7.0 15.0 0.559253 1.0 156 7.0 16.0 0.867020 1.0 157 7.0 17.0 0.019276 1.0 158 7.0 18.0 0.123097 1.0 159 7.0 19.0 0.808300 1.0 Preparing the experiment parameters # # ensure defaults are in grid search default_grid_search_parameters = { \"max_iters\" : [ 5000 ], \"learning_rate_init\" : [ 0.1 , 0.2 , 0.4 , 0.8 ], \"hidden_layer_sizes\" : [[ 4 , 4 , 4 ]], \"activation\" : [ mlrose_ky . neural . activation . relu ], } default_parameters = { \"seed\" : 123456 , \"iteration_list\" : 2 ** np . arange ( 13 ), \"max_attempts\" : 5000 , \"override_ctrl_c_handler\" : False , # required for running in notebook \"n_jobs\" : 5 , \"cv\" : 5 , } Example 1: Running the SKMLPRunner # (a) Clean Data # skmlp_grid_search_parameters = { ** default_grid_search_parameters , \"max_iters\" : [ 5000 ], \"learning_rate_init\" : [ 0.0001 ], \"activation\" : [ mlrose_ky . neural . activation . sigmoid ], } skmlp_default_parameters = { ** default_parameters , \"early_stopping\" : True , \"tol\" : 1e-05 , \"alpha\" : 0.001 , \"solver\" : \"lbfgs\" } cx_skr = SKMLPRunner ( x_train = cx_tr , y_train = cy_tr , x_test = cx_ts , y_test = cx_ts , experiment_name = \"skmlp_clean\" , grid_search_parameters = skmlp_grid_search_parameters , ** skmlp_default_parameters , ) run_stats_df , curves_df , cv_results_df , cx_sr = cx_skr . run () Fitting 5 folds for each of 1 candidates, totalling 5 fits [Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers. [Parallel(n_jobs=5)]: Done 2 out of 5 | elapsed: 2.0s remaining: 3.0s [Parallel(n_jobs=5)]: Done 5 out of 5 | elapsed: 1.2min finished The following plot shows the baseline predictions made for the clean dataset using the SKLearn runner. Note that the background coloring shows the prediction made by the learner for that area. The intensity of the color is indicative of the prediction confidence level. print ( f 'Hidden layer size: { cx_sr . best_params_ [ \"hidden_layer_sizes\" ] } ' ) plot_synthetic_dataset ( x_train = cx_tr , y_train = cy_tr , x_test = cx_ts , y_test = cy_ts , classifier = cx_sr . best_estimator_ . mlp ) Hidden layer size: [4, 4, 4] (b) Noisy Data # nx1_skr = SKMLPRunner ( x_train = nx_tr , y_train = ny_tr , x_test = nx_ts , y_test = nx_ts , experiment_name = \"skmlp_noisy_1\" , grid_search_parameters = skmlp_grid_search_parameters , ** skmlp_default_parameters , ) nx1_run_stats_df , nx1_curves_df , nx1_cv_results_df , nx1_sr = nx1_skr . run () Fitting 5 folds for each of 1 candidates, totalling 5 fits [Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers. [Parallel(n_jobs=5)]: Done 2 out of 5 | elapsed: 0.4s remaining: 0.6s [Parallel(n_jobs=5)]: Done 5 out of 5 | elapsed: 1.5min finished The following plot shows the predictions made for the noisy dataset using the sklearn runner with the same hidden layer size as the best clean set run. print ( f 'Hidden layer size: { nx1_sr . best_params_ [ \"hidden_layer_sizes\" ] } ' ) plot_synthetic_dataset ( x_train = nx_tr , y_train = ny_tr , x_test = nx_ts , y_test = ny_ts , classifier = nx1_sr . best_estimator_ . mlp ) Hidden layer size: [4, 4, 4] noisy_data_grid_search_parameters = { ** skmlp_grid_search_parameters , \"hidden_layer_sizes\" : [[ 4 , 4 , 4 , 4 ]]} nx2_skr = SKMLPRunner ( x_train = nx_tr , y_train = ny_tr , x_test = nx_ts , y_test = nx_ts , experiment_name = \"skmlp_noisy_2\" , grid_search_parameters = noisy_data_grid_search_parameters , ** skmlp_default_parameters , ) nx2_run_stats_df , nx2_curves_df , nx2_cv_results_df , nx2_sr = nx2_skr . run () Fitting 5 folds for each of 1 candidates, totalling 5 fits [Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers. [Parallel(n_jobs=5)]: Done 2 out of 5 | elapsed: 2.0min remaining: 3.0min [Parallel(n_jobs=5)]: Done 5 out of 5 | elapsed: 2.1min finished The following plot shows the predictions made for the noisy dataset using the sklearn runner with the optimal hidden layer size. print ( f 'Hidden layer size: { nx2_sr . best_params_ [ \"hidden_layer_sizes\" ] } ' ) plot_synthetic_dataset ( x_train = nx_tr , y_train = ny_tr , x_test = nx_ts , y_test = ny_ts , classifier = nx2_sr . best_estimator_ . mlp ) Hidden layer size: [4, 4, 4, 4] (c) Extra Data # ex1_skr = SKMLPRunner ( x_train = ex_tr , y_train = ey_tr , x_test = ex_ts , y_test = ex_ts , experiment_name = \"skmlp_extra_1\" , grid_search_parameters = skmlp_grid_search_parameters , ** skmlp_default_parameters , ) ex1_run_stats_df , ex1_curves_df , ex1_cv_results_df , ex1_sr = ex1_skr . run () Fitting 5 folds for each of 1 candidates, totalling 5 fits [Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers. [Parallel(n_jobs=5)]: Done 2 out of 5 | elapsed: 1.8min remaining: 2.6min [Parallel(n_jobs=5)]: Done 5 out of 5 | elapsed: 1.9min finished The following plot shows the baseline predictions made for the extra-column dataset using the sklearn runner with the same hidden layer size as the best clean set run: print ( f 'Hidden layer size: { ex1_sr . best_params_ [ \"hidden_layer_sizes\" ] } ' ) plot_synthetic_dataset ( x_train = ex_tr , y_train = ey_tr , x_test = ex_ts , y_test = ey_ts , classifier = ex1_sr . best_estimator_ . mlp ) Hidden layer size: [4, 4, 4] # override the hidden_layer_sizes grid search. extra_data_grid_search_parameters = { ** skmlp_grid_search_parameters , \"hidden_layer_sizes\" : [[ 6 , 6 , 6 ]]} ex2_skr = SKMLPRunner ( x_train = ex_tr , y_train = ey_tr , x_test = ex_ts , y_test = ex_ts , experiment_name = \"skmlp_extra_2\" , grid_search_parameters = extra_data_grid_search_parameters , ** skmlp_default_parameters , ) ex2_run_stats_df , ex2_curves_df , ex2_cv_results_df , ex2_sr = ex2_skr . run () Fitting 5 folds for each of 1 candidates, totalling 5 fits [Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers. [Parallel(n_jobs=5)]: Done 2 out of 5 | elapsed: 1.3min remaining: 1.9min [Parallel(n_jobs=5)]: Done 5 out of 5 | elapsed: 2.0min finished The following plot shows the baseline predictions made for the extra-column dataset using the sklearn runner with the optimal hidden layer size: print ( f 'Hidden layer size: { ex2_sr . best_params_ [ \"hidden_layer_sizes\" ] } ' ) plot_synthetic_dataset ( x_train = ex_tr , y_train = ey_tr , x_test = ex_ts , y_test = ey_ts , classifier = ex2_sr . best_estimator_ . mlp ) Hidden layer size: [6, 6, 6]","title":"3 - Using Neural Network Runners"},{"location":"nn_examples/#overview","text":"These examples will not solve assignment 2 for you, but they will give you some idea on how to use the problem generator and runner classes. Hopefully this will result in slightly fewer \"How do I <insert basic usage here>?\" questions every semester... Also, and in case it hasn't been made clear enough by the TAs, using any of the visualization code in here for your assignment is a bad idea, for two reasons... (1) It provides nothing useful as far as the assignment goes, and (2) the TAs will undoubtedly frown upon it. Visualization is part of the analysis and, for the most part, you're supposed to do that by yourself. Just including images of the before/after state of a problem really isn't useful in terms of what you're supposed to be analyzing. I also strongly recommend against using the synthetic data class used within for your assignment. It's a toy, with no real value for the assignment. There's much better data out there.","title":"Overview"},{"location":"nn_examples/#import-libraries","text":"from IPython.core.display import display , HTML # for some notebook formatting. import mlrose_ky import numpy as np import pandas as pd import logging import networkx as nx import matplotlib.pyplot as plt import string from sklearn.model_selection import train_test_split from sklearn.preprocessing import MinMaxScaler , OneHotEncoder from sklearn.metrics import accuracy_score from mlrose_ky import SyntheticData , plot_synthetic_dataset from mlrose_ky import SKMLPRunner , SARunner , GARunner , NNGSRunner # switch off the chatter logging . basicConfig ( level = logging . WARNING )","title":"Import Libraries"},{"location":"nn_examples/#generating-sample-data","text":"This sample data will be used in subsequent examples. sd = SyntheticData ( seed = 123456 ) clean_data , clean_features , clean_classes , _ = sd . get_synthetic_data ( x_dim = 20 , y_dim = 20 ) print ( f \"Features: { clean_features } \" ) print ( f \"Classes: { clean_classes } \" ) cx , cy , cx_tr , cx_ts , cy_tr , cy_ts = sd . setup_synthetic_data_test_train ( clean_data ) plot_synthetic_dataset ( x_train = cx_tr , y_train = cy_tr , x_test = cx_ts , y_test = cy_ts , transparent_bg = False , bg_color = \"black\" ) Features: ['(1) A', '(2) B'] Classes: ['RED', 'BLUE'] This is our standard 20x20 sample set with no noise or errors, with a 70/30 split between training and test data. Each point represents a data sample. Black-outlined points represent training data, and white-outlined points represent test data. A sample of the clean data can be seen below, with the first two columns being the (x,y) position of the data point, and the third column representing the color. HTML ( pd . DataFrame ( columns = [ \"x\" , \"y\" , \"c\" ], data = clean_data )[ 150 : 160 ] . to_html ()) x y c 150 7 10 0 151 7 11 0 152 7 12 0 153 7 13 1 154 7 14 1 155 7 15 1 156 7 16 1 157 7 17 1 158 7 18 1 159 7 19 1 We can generate \"dirty\" data in two ways: by adding an column of random data, or by introducing noisy duplicate data points that either reinforce or contradict existing data points. noisy_data , noisy_features , noisy_classes , _ = sd . get_synthetic_data ( x_dim = 20 , y_dim = 20 , add_noise = 0.025 ) print ( f \"Features: { noisy_features } \" ) print ( f \"Classes: { noisy_classes } \" ) nx , ny , nx_tr , nx_ts , ny_tr , ny_ts = sd . setup_synthetic_data_test_train ( noisy_data ) plot_synthetic_dataset ( x_train = nx_tr , y_train = ny_tr , x_test = nx_ts , y_test = ny_ts , transparent_bg = False , bg_color = \"black\" ) Features: ['(1) A', '(2) B'] Classes: ['RED', 'BLUE'] This is our data with 2.5% added noise. A sample of the noisy data can be seen below, with the first two columns being the (x,y) position of the data point, and the third column representing the color. Note that the sample shown represent some of the noisy data, which is added to the end of the clean data. HTML ( pd . DataFrame ( columns = [ \"x\" , \"y\" , \"c\" ], data = noisy_data )[ - 10 :] . to_html ()) x y c 410 17 17 1 411 15 18 0 412 15 19 0 413 0 6 0 414 10 14 1 415 7 8 1 416 10 19 1 417 10 9 0 418 16 1 0 419 6 0 1 If we had chosen to add the random column of data, then the plots would look that same as the above two, depending on whether we had chosen to additionally add noise or not. However, the features are different, as shown below, and we can see the extra column in the data sample. extra_data , extra_features , extra_classes , _ = sd . get_synthetic_data ( x_dim = 20 , y_dim = 20 , add_redundant_column = True ) print ( f \"Features: { extra_features } \" ) print ( f \"Classes: { extra_classes } \" ) ex , ey , ex_tr , ex_ts , ey_tr , ey_ts = sd . setup_synthetic_data_test_train ( extra_data ) Features: ['(1) A', '(2) B', '(3) R'] Classes: ['RED', 'BLUE'] HTML ( pd . DataFrame ( columns = [ \"x\" , \"y\" , \"r\" , \"c\" ], data = extra_data )[ 150 : 160 ] . to_html ()) x y r c 150 7.0 10.0 0.972641 0.0 151 7.0 11.0 0.726259 0.0 152 7.0 12.0 0.412651 0.0 153 7.0 13.0 0.990003 1.0 154 7.0 14.0 0.535660 1.0 155 7.0 15.0 0.559253 1.0 156 7.0 16.0 0.867020 1.0 157 7.0 17.0 0.019276 1.0 158 7.0 18.0 0.123097 1.0 159 7.0 19.0 0.808300 1.0","title":"Generating sample data..."},{"location":"nn_examples/#preparing-the-experiment-parameters","text":"# ensure defaults are in grid search default_grid_search_parameters = { \"max_iters\" : [ 5000 ], \"learning_rate_init\" : [ 0.1 , 0.2 , 0.4 , 0.8 ], \"hidden_layer_sizes\" : [[ 4 , 4 , 4 ]], \"activation\" : [ mlrose_ky . neural . activation . relu ], } default_parameters = { \"seed\" : 123456 , \"iteration_list\" : 2 ** np . arange ( 13 ), \"max_attempts\" : 5000 , \"override_ctrl_c_handler\" : False , # required for running in notebook \"n_jobs\" : 5 , \"cv\" : 5 , }","title":"Preparing the experiment parameters"},{"location":"nn_examples/#example-1-running-the-skmlprunner","text":"","title":"Example 1: Running the SKMLPRunner"},{"location":"nn_examples/#a-clean-data","text":"skmlp_grid_search_parameters = { ** default_grid_search_parameters , \"max_iters\" : [ 5000 ], \"learning_rate_init\" : [ 0.0001 ], \"activation\" : [ mlrose_ky . neural . activation . sigmoid ], } skmlp_default_parameters = { ** default_parameters , \"early_stopping\" : True , \"tol\" : 1e-05 , \"alpha\" : 0.001 , \"solver\" : \"lbfgs\" } cx_skr = SKMLPRunner ( x_train = cx_tr , y_train = cy_tr , x_test = cx_ts , y_test = cx_ts , experiment_name = \"skmlp_clean\" , grid_search_parameters = skmlp_grid_search_parameters , ** skmlp_default_parameters , ) run_stats_df , curves_df , cv_results_df , cx_sr = cx_skr . run () Fitting 5 folds for each of 1 candidates, totalling 5 fits [Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers. [Parallel(n_jobs=5)]: Done 2 out of 5 | elapsed: 2.0s remaining: 3.0s [Parallel(n_jobs=5)]: Done 5 out of 5 | elapsed: 1.2min finished The following plot shows the baseline predictions made for the clean dataset using the SKLearn runner. Note that the background coloring shows the prediction made by the learner for that area. The intensity of the color is indicative of the prediction confidence level. print ( f 'Hidden layer size: { cx_sr . best_params_ [ \"hidden_layer_sizes\" ] } ' ) plot_synthetic_dataset ( x_train = cx_tr , y_train = cy_tr , x_test = cx_ts , y_test = cy_ts , classifier = cx_sr . best_estimator_ . mlp ) Hidden layer size: [4, 4, 4]","title":"(a) Clean Data"},{"location":"nn_examples/#b-noisy-data","text":"nx1_skr = SKMLPRunner ( x_train = nx_tr , y_train = ny_tr , x_test = nx_ts , y_test = nx_ts , experiment_name = \"skmlp_noisy_1\" , grid_search_parameters = skmlp_grid_search_parameters , ** skmlp_default_parameters , ) nx1_run_stats_df , nx1_curves_df , nx1_cv_results_df , nx1_sr = nx1_skr . run () Fitting 5 folds for each of 1 candidates, totalling 5 fits [Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers. [Parallel(n_jobs=5)]: Done 2 out of 5 | elapsed: 0.4s remaining: 0.6s [Parallel(n_jobs=5)]: Done 5 out of 5 | elapsed: 1.5min finished The following plot shows the predictions made for the noisy dataset using the sklearn runner with the same hidden layer size as the best clean set run. print ( f 'Hidden layer size: { nx1_sr . best_params_ [ \"hidden_layer_sizes\" ] } ' ) plot_synthetic_dataset ( x_train = nx_tr , y_train = ny_tr , x_test = nx_ts , y_test = ny_ts , classifier = nx1_sr . best_estimator_ . mlp ) Hidden layer size: [4, 4, 4] noisy_data_grid_search_parameters = { ** skmlp_grid_search_parameters , \"hidden_layer_sizes\" : [[ 4 , 4 , 4 , 4 ]]} nx2_skr = SKMLPRunner ( x_train = nx_tr , y_train = ny_tr , x_test = nx_ts , y_test = nx_ts , experiment_name = \"skmlp_noisy_2\" , grid_search_parameters = noisy_data_grid_search_parameters , ** skmlp_default_parameters , ) nx2_run_stats_df , nx2_curves_df , nx2_cv_results_df , nx2_sr = nx2_skr . run () Fitting 5 folds for each of 1 candidates, totalling 5 fits [Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers. [Parallel(n_jobs=5)]: Done 2 out of 5 | elapsed: 2.0min remaining: 3.0min [Parallel(n_jobs=5)]: Done 5 out of 5 | elapsed: 2.1min finished The following plot shows the predictions made for the noisy dataset using the sklearn runner with the optimal hidden layer size. print ( f 'Hidden layer size: { nx2_sr . best_params_ [ \"hidden_layer_sizes\" ] } ' ) plot_synthetic_dataset ( x_train = nx_tr , y_train = ny_tr , x_test = nx_ts , y_test = ny_ts , classifier = nx2_sr . best_estimator_ . mlp ) Hidden layer size: [4, 4, 4, 4]","title":"(b) Noisy Data"},{"location":"nn_examples/#c-extra-data","text":"ex1_skr = SKMLPRunner ( x_train = ex_tr , y_train = ey_tr , x_test = ex_ts , y_test = ex_ts , experiment_name = \"skmlp_extra_1\" , grid_search_parameters = skmlp_grid_search_parameters , ** skmlp_default_parameters , ) ex1_run_stats_df , ex1_curves_df , ex1_cv_results_df , ex1_sr = ex1_skr . run () Fitting 5 folds for each of 1 candidates, totalling 5 fits [Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers. [Parallel(n_jobs=5)]: Done 2 out of 5 | elapsed: 1.8min remaining: 2.6min [Parallel(n_jobs=5)]: Done 5 out of 5 | elapsed: 1.9min finished The following plot shows the baseline predictions made for the extra-column dataset using the sklearn runner with the same hidden layer size as the best clean set run: print ( f 'Hidden layer size: { ex1_sr . best_params_ [ \"hidden_layer_sizes\" ] } ' ) plot_synthetic_dataset ( x_train = ex_tr , y_train = ey_tr , x_test = ex_ts , y_test = ey_ts , classifier = ex1_sr . best_estimator_ . mlp ) Hidden layer size: [4, 4, 4] # override the hidden_layer_sizes grid search. extra_data_grid_search_parameters = { ** skmlp_grid_search_parameters , \"hidden_layer_sizes\" : [[ 6 , 6 , 6 ]]} ex2_skr = SKMLPRunner ( x_train = ex_tr , y_train = ey_tr , x_test = ex_ts , y_test = ex_ts , experiment_name = \"skmlp_extra_2\" , grid_search_parameters = extra_data_grid_search_parameters , ** skmlp_default_parameters , ) ex2_run_stats_df , ex2_curves_df , ex2_cv_results_df , ex2_sr = ex2_skr . run () Fitting 5 folds for each of 1 candidates, totalling 5 fits [Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers. [Parallel(n_jobs=5)]: Done 2 out of 5 | elapsed: 1.3min remaining: 1.9min [Parallel(n_jobs=5)]: Done 5 out of 5 | elapsed: 2.0min finished The following plot shows the baseline predictions made for the extra-column dataset using the sklearn runner with the optimal hidden layer size: print ( f 'Hidden layer size: { ex2_sr . best_params_ [ \"hidden_layer_sizes\" ] } ' ) plot_synthetic_dataset ( x_train = ex_tr , y_train = ey_tr , x_test = ex_ts , y_test = ey_ts , classifier = ex2_sr . best_estimator_ . mlp ) Hidden layer size: [6, 6, 6]","title":"(c) Extra Data"},{"location":"opt_probs/","text":"Optimization Problem Types # Classes for defining optimization problem objects. Discrete Optimization Problem # Class for defining discrete-state optimization problems. class class DiscreteOpt ( length , fitness_fn , maximize=True , max_val=2 ) Parameters : length ( int ) \u2013 Number of elements in state vector. fitness_fn ( fitness function object ) \u2013 Object to implement fitness function for optimization. maximize ( bool, default: True ) \u2013 Whether to maximize the fitness function. Set False for minimization problem. max_val ( int, default: 2 ) \u2013 Number of unique values that each element in the state vector can take. Assumes values are integers in the range 0 to (max_val - 1), inclusive. Continuous Optimization Problem # Class for defining continuous-state optimization problems. class class ContinuousOpt ( length , fitness_fn , maximize=True , min_val=0 , max_val=1 , step=0.1 ) Parameters : length ( int ) \u2013 Number of elements in state vector. fitness_fn ( fitness function object ) \u2013 Object to implement fitness function for optimization. maximize ( bool, default: True ) \u2013 Whether to maximize the fitness function. Set False for minimization problem. min_val ( float, default: 0 ) \u2013 Minimum value that each element of the state vector can take. max_val ( float, default: 1 ) \u2013 Maximum value that each element of the state vector can take. step ( float, default: 0.1 ) \u2013 Step size used in determining neighbors of current state. Travelling Salesperson Optimization Problem # Class for defining travelling salesperson optimization problems. class class TSPOpt ( length , fitness_fn=None , maximize=False , coords=None , distances=None ) Parameters : length ( int ) \u2013 Number of elements in state vector. Must equal number of nodes in the tour. fitness_fn ( fitness function object, default: None ) \u2013 Object to implement fitness function for optimization. If None , then TravellingSales(coords=coords, distances=distances) is used by default. maximize ( bool, default: False ) \u2013 Whether to maximize the fitness function. Set False for minimization problem. coords ( list of pairs, default: None ) \u2013 Ordered list of the (x, y) co-ordinates of all nodes. This assumes that travel between all pairs of nodes is possible. If this is not the case, then use distances instead. This argument is ignored if fitness_fn is not None . distances ( list of triples, default: None ) \u2013 List giving the distances, d, between all pairs of nodes, u and v, for which travel is possible, with each list item in the form (u, v, d). Order of the nodes does not matter, so (u, v, d) and (v, u, d) are considered to be the same. If a pair is missing from the list, it is assumed that travel between the two nodes is not possible. This argument is ignored if fitness_fn or coords is not None .","title":"\ud83e\udd14 Optimization Problems"},{"location":"opt_probs/#optimization-problem-types","text":"Classes for defining optimization problem objects.","title":"Optimization Problem Types"},{"location":"opt_probs/#discrete-optimization-problem","text":"Class for defining discrete-state optimization problems. class class DiscreteOpt ( length , fitness_fn , maximize=True , max_val=2 ) Parameters : length ( int ) \u2013 Number of elements in state vector. fitness_fn ( fitness function object ) \u2013 Object to implement fitness function for optimization. maximize ( bool, default: True ) \u2013 Whether to maximize the fitness function. Set False for minimization problem. max_val ( int, default: 2 ) \u2013 Number of unique values that each element in the state vector can take. Assumes values are integers in the range 0 to (max_val - 1), inclusive.","title":"Discrete Optimization Problem"},{"location":"opt_probs/#continuous-optimization-problem","text":"Class for defining continuous-state optimization problems. class class ContinuousOpt ( length , fitness_fn , maximize=True , min_val=0 , max_val=1 , step=0.1 ) Parameters : length ( int ) \u2013 Number of elements in state vector. fitness_fn ( fitness function object ) \u2013 Object to implement fitness function for optimization. maximize ( bool, default: True ) \u2013 Whether to maximize the fitness function. Set False for minimization problem. min_val ( float, default: 0 ) \u2013 Minimum value that each element of the state vector can take. max_val ( float, default: 1 ) \u2013 Maximum value that each element of the state vector can take. step ( float, default: 0.1 ) \u2013 Step size used in determining neighbors of current state.","title":"Continuous Optimization Problem"},{"location":"opt_probs/#travelling-salesperson-optimization-problem","text":"Class for defining travelling salesperson optimization problems. class class TSPOpt ( length , fitness_fn=None , maximize=False , coords=None , distances=None ) Parameters : length ( int ) \u2013 Number of elements in state vector. Must equal number of nodes in the tour. fitness_fn ( fitness function object, default: None ) \u2013 Object to implement fitness function for optimization. If None , then TravellingSales(coords=coords, distances=distances) is used by default. maximize ( bool, default: False ) \u2013 Whether to maximize the fitness function. Set False for minimization problem. coords ( list of pairs, default: None ) \u2013 Ordered list of the (x, y) co-ordinates of all nodes. This assumes that travel between all pairs of nodes is possible. If this is not the case, then use distances instead. This argument is ignored if fitness_fn is not None . distances ( list of triples, default: None ) \u2013 List giving the distances, d, between all pairs of nodes, u and v, for which travel is possible, with each list item in the form (u, v, d). Order of the nodes does not matter, so (u, v, d) and (v, u, d) are considered to be the same. If a pair is missing from the list, it is assumed that travel between the two nodes is not possible. This argument is ignored if fitness_fn or coords is not None .","title":"Travelling Salesperson Optimization Problem"},{"location":"problem_examples/","text":"mlrose_ky Generator and Runner Usage Examples - Andrew Rollings # Modified by Kyle Nakamura Overview # These examples will not solve assignment 2 for you, but they will give you some idea on how to use the problem generator and runner classes. Hopefully this will result in slightly fewer \"How do I \\<insert basic usage here>\" questions every semester... Also, and in case it hasn't been made clear enough by the TAs, using any of the visualizations from this tutorial for your report is a bad idea for two reasons: 1. It provides nothing useful as far as the assignment goes, and 2. The TAs will undoubtedly frown upon it. Visualization is part of the analysis and, for the most part, you're supposed to do that by yourself. Just including images of the before/after state of a problem really isn't useful in terms of what you're supposed to be analyzing. Import Libraries # % pip install chess IPython Requirement already satisfied: chess in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (1.10.0) Requirement already satisfied: IPython in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (8.25.0) Requirement already satisfied: decorator in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (from IPython) (5.1.1) Requirement already satisfied: jedi>=0.16 in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (from IPython) (0.18.1) Requirement already satisfied: matplotlib-inline in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (from IPython) (0.1.6) Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (from IPython) (3.0.47) Requirement already satisfied: pygments>=2.4.0 in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (from IPython) (2.18.0) Requirement already satisfied: stack-data in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (from IPython) (0.2.0) Requirement already satisfied: traitlets>=5.13.0 in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (from IPython) (5.14.3) Requirement already satisfied: typing-extensions>=4.6 in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (from IPython) (4.12.2) Requirement already satisfied: pexpect>4.3 in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (from IPython) (4.9.0) Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (from jedi>=0.16->IPython) (0.8.3) Requirement already satisfied: ptyprocess>=0.5 in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (from pexpect>4.3->IPython) (0.7.0) Requirement already satisfied: wcwidth in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->IPython) (0.2.13) Requirement already satisfied: executing in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (from stack-data->IPython) (0.8.3) Requirement already satisfied: asttokens in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (from stack-data->IPython) (2.0.5) Requirement already satisfied: pure-eval in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (from stack-data->IPython) (0.2.2) Requirement already satisfied: six in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (from asttokens->stack-data->IPython) (1.16.0) Note: you may need to restart the kernel to use updated packages. from IPython.display import HTML import numpy as np import logging import networkx as nx import matplotlib.pyplot as plt import string from ast import literal_eval import chess from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import MinMaxScaler , OneHotEncoder from sklearn.metrics import accuracy_score import mlrose_ky as mlrose from mlrose_ky.generators import QueensGenerator , MaxKColorGenerator , TSPGenerator from mlrose_ky.runners import SARunner , GARunner , NNGSRunner # Hide warnings from libraries logging . basicConfig ( level = logging . WARNING ) Example 1: Solving the 8-Queens problem using the SA algorithm # Initializing and viewing the problem # First, we'll use the QueensGenerator to create an instance of the 8-Queens problem. # Generate a new 8-Queens optimization problem using a fixed seed problem = QueensGenerator . generate ( seed = 123456 , size = 8 ) The initial, un-optimized state can be seen below, both as a list and as a chess board. # View the initial state as a list state = problem . get_state () print ( 'Initial state:' , state ) # View the initial state as a chess board board_layout = \"/\" . join ([ \"\" . join (([ str ( s )] if s > 0 else []) + [ \"Q\" ] + ([ str (( 7 - s ))] if s < 7 else [])) for s in state ]) chess . Board ( board_layout ) # You may need to \"trust\" this notebook for the board visualization to work Initial state: [2 3 3 2 7 0 1 1] Solving 8-Queens using a Runner (i.e., grid search) # Runners are used to execute \"grid search\" experiments on optimization problems. We'll use the Simulated Annealing Runner (SARunner) to perform a grid search on the 8-Queens problem and then extract the optimal SA hyperparameters. Here is a brief explanation of the SARunner parameters used in the example below: - max_attempts : A list of maximum attempts to try improving the fitness score before terminating a run - temperature_list : A list of temperatures to try when initializing the SA algorithm's decay function (e.g., GeomDecay(init_temp=1.0) ) - decay_list : A list of decay schedules to try as the SA algorithm's decay function (e.g., GeomDecay , ExpDecay , etc.) - iteration_list : A list of iterations to snapshot the state of the algorithm at (only determines the rows that the Runner will output) Disclaimer: the values used here are just toy values picked specifically for this example. You will have to choose your own range of values for your experiments. I strongly recommend you don't just copy these, or you will find that the grading is unlikely to go the way you would like. # Create an SA Runner instance to solve the problem sa = SARunner ( problem = problem , experiment_name = \"queens_8_sa\" , seed = 123456 , output_directory = None , # Note: specify an output directory (str) to have these results saved to disk max_attempts = 100 , temperature_list = [ 0.1 , 0.5 , 0.75 , 1.0 , 2.0 , 5.0 ], decay_list = [ mlrose . GeomDecay ], iteration_list = 2 ** np . arange ( 11 ), # list of 11 integers from 2^0 to 2^11 ) # Run the SA Runner and retrieve its results df_run_stats , df_run_curves = sa . run () # Calculate some simple stats about the experiment temperatures_per_run = max ( 1 , len ( sa . temperature_list )) decays_per_run = max ( 1 , len ( sa . decay_list )) iters_per_run = len ( sa . iteration_list ) + 1 total_runs = temperatures_per_run * decays_per_run print ( f \"The experiment executed { total_runs } runs, each with { iters_per_run } snapshots at different iterations.\" ) print ( f \"In total, the output dataframe should contain { total_runs * iters_per_run } rows.\" ) The experiment executed 6 runs, each with 12 snapshots at different iterations. In total, the output dataframe should contain 72 rows. The df_run_stats dataframe contains snapshots of the state of the algorithm at the iterations specified in the iteration_list . Since iterations_list contains 11 numbers, and iteration 0 is always included in the results, each run will take up 12 rows of the dataframe. The first 12 rows (i.e., results from the first run) are shown below: HTML ( df_run_stats [[ \"Iteration\" , \"Fitness\" , \"FEvals\" , \"Time\" , \"State\" ]][: iters_per_run ] . to_html ( index = False )) Iteration Fitness FEvals Time State 0 11.0 0 0.000856 [1, 2, 2, 1, 0, 3, 7, 3] 1 9.0 2 0.002331 [1, 2, 2, 0, 0, 3, 7, 3] 2 8.0 4 0.002864 [1, 2, 2, 0, 0, 3, 7, 5] 4 8.0 7 0.003399 [1, 2, 2, 5, 0, 3, 7, 5] 8 5.0 13 0.004127 [1, 2, 7, 5, 0, 3, 5, 5] 16 4.0 24 0.005042 [1, 2, 7, 5, 3, 0, 5, 5] 32 4.0 47 0.006737 [1, 5, 7, 5, 0, 0, 3, 4] 64 1.0 86 0.009472 [1, 5, 2, 6, 3, 0, 7, 4] 128 1.0 155 0.014421 [1, 5, 2, 6, 3, 0, 4, 7] 256 1.0 295 0.025960 [1, 7, 2, 6, 3, 5, 0, 4] 512 0.0 461 0.044061 [1, 5, 0, 6, 3, 7, 2, 4] 1024 0.0 461 0.044061 [1, 5, 0, 6, 3, 7, 2, 4] Some information was intentionally excluded from the previous output. Let's now preview the entirety of the first run: HTML ( df_run_stats [: iters_per_run ] . to_html ( index = False )) Iteration Fitness FEvals Time State schedule_type schedule_init_temp schedule_decay schedule_min_temp schedule_current_value Temperature max_iters 0 11.0 0 0.000856 [1, 2, 2, 1, 0, 3, 7, 3] geometric 0.1 0.99 0.001 0.099999 0.1 1024 1 9.0 2 0.002331 [1, 2, 2, 0, 0, 3, 7, 3] geometric 0.1 0.99 0.001 0.099998 0.1 1024 2 8.0 4 0.002864 [1, 2, 2, 0, 0, 3, 7, 5] geometric 0.1 0.99 0.001 0.099997 0.1 1024 4 8.0 7 0.003399 [1, 2, 2, 5, 0, 3, 7, 5] geometric 0.1 0.99 0.001 0.099997 0.1 1024 8 5.0 13 0.004127 [1, 2, 7, 5, 0, 3, 5, 5] geometric 0.1 0.99 0.001 0.099996 0.1 1024 16 4.0 24 0.005042 [1, 2, 7, 5, 3, 0, 5, 5] geometric 0.1 0.99 0.001 0.099995 0.1 1024 32 4.0 47 0.006737 [1, 5, 7, 5, 0, 0, 3, 4] geometric 0.1 0.99 0.001 0.099993 0.1 1024 64 1.0 86 0.009472 [1, 5, 2, 6, 3, 0, 7, 4] geometric 0.1 0.99 0.001 0.099990 0.1 1024 128 1.0 155 0.014421 [1, 5, 2, 6, 3, 0, 4, 7] geometric 0.1 0.99 0.001 0.099986 0.1 1024 256 1.0 295 0.025960 [1, 7, 2, 6, 3, 5, 0, 4] geometric 0.1 0.99 0.001 0.099974 0.1 1024 512 0.0 461 0.044061 [1, 5, 0, 6, 3, 7, 2, 4] geometric 0.1 0.99 0.001 0.099956 0.1 1024 1024 0.0 461 0.044061 [1, 5, 0, 6, 3, 7, 2, 4] geometric 0.1 0.99 0.001 0.099956 0.1 1024 What does all this information tell us about the first run of our experiment? Iteration shows the index of the snapshot, with 12 snapshots per run in this example. Fitness shows the fitness score of the state at the corresponding iteration, where 0.0 is optimal for minimization problems. FEvals shows the number of fitness function evaluations performed by the algorithm at the corresponding iteration. Time shows the time elapsed (calculated using time.perf_counter() ) up to the corresponding iteration. State shows the state of the algorithm at the corresponding iteration (see mlrose_ky.fitness.queens for more details). Temperature shows the decay function (and its parameters) that was used for this run. We will have 6 unique values in this column, one for each run. max_iters shows the maximum number of iterations allowed for the algorithm to run. It defaults to max(iteration_list) for all Runners, which is 1024 in this case. To pick out the most performant run from the dataframe, we need to find the row with the best fitness. Since Queens is a minimization problem, we're looking for the row with minimal fitness (i.e., zero). It's likely that multiple runs will achieve the same fitness, so we need to find the run that achieved the best Fitness in the fewest FEvals ( Note: we could make this selection using Iterations or Time if we so desired. ) best_fitness = df_run_stats [ \"Fitness\" ] . min () # Should be 0.0 in this case # Get all runs with the best fitness value best_runs = df_run_stats [ df_run_stats [ \"Fitness\" ] == best_fitness ] best_runs .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Iteration Fitness FEvals Time State schedule_type schedule_init_temp schedule_decay schedule_min_temp schedule_current_value Temperature max_iters 10 512 0.0 461 0.044061 [1, 5, 0, 6, 3, 7, 2, 4] geometric 0.1 0.99 0.001 0.099956 0.1 1024 11 1024 0.0 461 0.044061 [1, 5, 0, 6, 3, 7, 2, 4] geometric 0.1 0.99 0.001 0.099956 0.1 1024 22 512 0.0 461 0.040817 [1, 5, 0, 6, 3, 7, 2, 4] geometric 0.5 0.99 0.001 0.499795 0.5 1024 23 1024 0.0 461 0.040817 [1, 5, 0, 6, 3, 7, 2, 4] geometric 0.5 0.99 0.001 0.499795 0.5 1024 58 512 0.0 427 0.044455 [7, 1, 3, 0, 6, 4, 2, 5] geometric 2.0 0.99 0.001 1.999107 2.0 1024 59 1024 0.0 427 0.044455 [7, 1, 3, 0, 6, 4, 2, 5] geometric 2.0 0.99 0.001 1.999107 2.0 1024 70 512 0.0 583 0.061001 [6, 0, 2, 7, 5, 3, 1, 4] geometric 5.0 0.99 0.001 4.996936 5.0 1024 71 1024 0.0 583 0.061001 [6, 0, 2, 7, 5, 3, 1, 4] geometric 5.0 0.99 0.001 4.996936 5.0 1024 This gives us our candidates for the best run. The best run will be the one that achieved the best fitness in the fewest evaluations. minimum_evaluations = best_runs [ \"FEvals\" ] . min () # Should be 461 in this case # Extract the best run with the minimum number of evaluations best_run = best_runs [ best_runs [ \"FEvals\" ] == minimum_evaluations ] The best run using these criteria is as follows: print ( best_run . iloc [ 0 ]) Iteration 512 Fitness 0.0 FEvals 427 Time 0.044455 State [7, 1, 3, 0, 6, 4, 2, 5] schedule_type geometric schedule_init_temp 2.0 schedule_decay 0.99 schedule_min_temp 0.001 schedule_current_value 1.999107 Temperature 2.0 max_iters 1024 Name: 58, dtype: object Which has the following identifying state information: best_temperature_param = best_run [ \"Temperature\" ] . iloc [ 0 ] . init_temp best_temperature_param 2.0 To map this result back to the original output of the Runner, we are looking for all rows in df_run_stats where the temperature is equal to 2. run_stats_best_run = df_run_stats [ df_run_stats [ \"schedule_init_temp\" ] == best_temperature_param ] run_stats_best_run [[ \"Iteration\" , \"Fitness\" , \"FEvals\" , \"Time\" , \"State\" ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Iteration Fitness FEvals Time State 48 0 11.0 0 0.000136 [1, 2, 2, 1, 0, 3, 7, 3] 49 1 9.0 2 0.001460 [1, 2, 2, 0, 0, 3, 7, 3] 50 2 8.0 4 0.002754 [1, 2, 2, 0, 0, 3, 7, 5] 51 4 8.0 7 0.004162 [1, 2, 2, 5, 0, 3, 7, 5] 52 8 7.0 14 0.005876 [1, 2, 2, 5, 0, 3, 5, 5] 53 16 6.0 27 0.007869 [3, 2, 3, 5, 0, 1, 5, 5] 54 32 4.0 57 0.011020 [3, 5, 6, 5, 5, 0, 4, 7] 55 64 5.0 114 0.015731 [2, 0, 3, 6, 1, 2, 1, 7] 56 128 3.0 205 0.022950 [2, 0, 6, 3, 5, 0, 4, 3] 57 256 2.0 358 0.036341 [7, 1, 3, 6, 6, 4, 0, 5] 58 512 0.0 427 0.044455 [7, 1, 3, 0, 6, 4, 2, 5] 59 1024 0.0 427 0.044455 [7, 1, 3, 0, 6, 4, 2, 5] And the best state associated with this is: best_state = run_stats_best_run [[ \"schedule_current_value\" , \"schedule_init_temp\" , \"schedule_min_temp\" ]] . tail ( 1 ) best_state .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } schedule_current_value schedule_init_temp schedule_min_temp 59 1.999107 2.0 0.001 The final state is as follows: state = literal_eval ( run_stats_best_run [ \"State\" ] . tail ( 1 ) . values [ 0 ]) print ( state ) board_layout = \"/\" . join ([ \"\" . join (([ str ( s )] if s > 0 else []) + [ \"Q\" ] + ([ str (( 7 - s ))] if s < 7 else [])) for s in state ]) board = chess . Board ( board_layout ) board [7, 1, 3, 0, 6, 4, 2, 5] Example 2: Generating and Running Max K Color using the GA algorithm # # Generate a new Max K problem using a fixed seed. problem = MaxKColorGenerator () . generate ( seed = 123456 , number_of_nodes = 10 , max_connections_per_node = 3 , max_colors = 3 ) The input graph generated for the problem looks like this: nx . draw ( problem . source_graph , pos = nx . spring_layout ( problem . source_graph , seed = 3 )) plt . show () # create a runner class and solve the problem ga = GARunner ( problem = problem , experiment_name = \"max_k_ga\" , output_directory = None , # note: specify an output directory to have results saved to disk seed = 123456 , iteration_list = 2 ** np . arange ( 11 ), population_sizes = [ 10 , 20 , 50 ], mutation_rates = [ 0.1 , 0.2 , 0.5 ], ) # the two data frames will contain the results df_run_stats , df_run_curves = ga . run () The preceding code will run the GA algorithm nine times for at most 1024 iterations per run. Each run is a permutation from the list of population_sizes and mutation_rates . Note that the initial state parameters here are just toy values picked specifically for this example. You will have to choose your own range of values for your assignment. I strongly recommend you don't just copy these, or you will find that the grading is unlikely to go the way you would like. Really. I mean it... A mutation rate of 0.5 is little better than a pure random search. The output in the df_run_stats dataframe contains snapshots of the state of the algorithm at the iterations specified in the iteration_list passed into the runner class. The first row (corresponding to the first run of this algorithm) are as follows: df_run_stats [[ \"Iteration\" , \"Fitness\" , \"FEvals\" , \"Time\" , \"State\" ]][ 0 : 1 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Iteration Fitness FEvals Time State 0 0 3.0 10 0.000702 [1, 2, 2, 1, 0, 0, 0, 0, 2, 2] The state information is excluded from the previous output. A sample of this is below: state_sample = df_run_stats [[ \"Population Size\" , \"Mutation Rate\" ]][: 1 ] state_sample .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Population Size Mutation Rate 0 10 0.1 So, to pick out the most performant run from the dataframe, you need to find the row with the best fitness. As Max-K-Color is a minimization problem, you'd pick the row with the minimum fitness. However, I'm going to look in the run_curves (which stores minimal basic information every iteration) to find out which input state achieved the best fitness in the fewest fitness evaluations. best_fitness = df_run_curves [ \"Fitness\" ] . min () best_runs = df_run_curves [ df_run_curves [ \"Fitness\" ] == best_fitness ] best_runs .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Iteration Time Fitness FEvals Population Size Mutation Rate max_iters 4 4 0.003967 0.0 57.0 10 0.1 1024 9 4 0.003967 0.0 57.0 10 0.2 1024 18 8 0.001445 0.0 101.0 10 0.5 1024 24 5 0.000106 0.0 127.0 20 0.1 1024 27 2 0.003304 0.0 64.0 20 0.2 1024 31 3 0.003797 0.0 85.0 20 0.5 1024 41 9 0.001615 0.0 511.0 50 0.1 1024 45 3 0.003797 0.0 205.0 50 0.2 1024 50 4 0.003967 0.0 256.0 50 0.5 1024 This gives us nine candidates for the best run. We are going to pick the one with that reached the best fitness value in the fewest number of evaluations. (We could also have chosen to use Iterations as our criteria.) minimum_evaluations = best_runs [ \"FEvals\" ] . min () best_run = best_runs [ best_runs [ \"FEvals\" ] == minimum_evaluations ] The best runs using these criteria is as follows: best_run .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Iteration Time Fitness FEvals Population Size Mutation Rate max_iters 4 4 0.003967 0.0 57.0 10 0.1 1024 9 4 0.003967 0.0 57.0 10 0.2 1024 We will arbitrarily pick the first row for this example, which has the following identifying state information: best_mr = best_run [ \"Mutation Rate\" ] . iloc ()[ 0 ] best_pop_size = best_run [ \"Population Size\" ] . iloc ()[ 0 ] print ( f \"Best Mutation Rate: { best_mr } , best Population Size: { best_pop_size } \" ) Best Mutation Rate: 0.1, best Population Size: 10 To map this back to the run_stats we look at the configuration data included in the curve data. The curve data includes at least the minimum identifying information to determine which run each row came from. In this case, the values we are looking for are the Mutation Rate and Population Size . So, we are looking for all rows in df_run_stats where the mutation rate and population size are equal to our best values. run_stats_best_run = df_run_stats [( df_run_stats [ \"Mutation Rate\" ] == best_mr ) & ( df_run_stats [ \"Population Size\" ] == best_pop_size )] run_stats_best_run [[ \"Iteration\" , \"Fitness\" , \"FEvals\" , \"Time\" ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Iteration Fitness FEvals Time 0 0 3.0 10 0.000702 1 1 3.0 21 0.002124 2 2 2.0 33 0.003304 3 4 0.0 57 0.003967 4 8 0.0 57 0.003967 5 16 0.0 57 0.003967 6 32 0.0 57 0.003967 7 64 0.0 57 0.003967 8 128 0.0 57 0.003967 9 256 0.0 57 0.003967 10 512 0.0 57 0.003967 11 1024 0.0 57 0.003967 And the best state associated with this is: best_state = run_stats_best_run [[ \"State\" ]] . tail ( 1 ) best_state .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } State 11 [0, 1, 2, 0, 0, 2, 0, 1, 2, 1] For the following node ordering: print ([ n for n in problem . source_graph . nodes ]) [0, 2, 8, 1, 3, 4, 6, 7, 9, 5] Reordering the state by ascending node number gives the following: color_indexes = literal_eval ( run_stats_best_run [ \"State\" ] . tail ( 1 ) . values [ 0 ]) ordered_state = [ color_indexes [ n ] for n in problem . source_graph . nodes ] print ( ordered_state ) [0, 2, 2, 1, 0, 0, 0, 1, 1, 2] Which results in a graph looking like this: colors = [ \"lightcoral\" , \"lightgreen\" , \"yellow\" ] node_color_map = [ colors [ s ] for s in ordered_state ] nx . draw ( problem . source_graph , pos = nx . spring_layout ( problem . source_graph , seed = 3 ), with_labels = True , node_color = node_color_map ) plt . show () Example 3: Generating and Running TSP using the GA algorithm # # Generate a new TSP problem using a fixed seed. problem = TSPGenerator () . generate ( seed = 123456 , number_of_cities = 20 ) The input graph generated for the problem looks like this: fig , ax = plt . subplots ( 1 ) # Prepare 2 plots ax . set_yticklabels ([]) ax . set_xticklabels ([]) for i , ( x , y ) in enumerate ( problem . coords ): ax . scatter ( x , y , s = 200 , c = \"cornflowerblue\" ) # plot A node_labels = { k : str ( v ) for k , v in enumerate ( string . ascii_uppercase ) if k < len ( problem . source_graph . nodes )} for i in node_labels . keys (): x , y = problem . coords [ i ] plt . text ( x , y , node_labels [ i ], ha = \"center\" , va = \"center\" , c = \"white\" , fontweight = \"bold\" , bbox = dict ( boxstyle = f \"circle,pad=0.15\" , fc = \"cornflowerblue\" ), ) plt . tight_layout () plt . show () # create a runner class and solve the problem ga = GARunner ( problem = problem , experiment_name = \"tsp_ga\" , output_directory = None , # note: specify an output directory to have results saved to disk seed = 123456 , iteration_list = 2 ** np . arange ( 11 ), population_sizes = [ 10 , 20 ], mutation_rates = [ 0.1 , 0.25 , 0.5 ], ) # the two data frames will contain the results df_run_stats , df_run_curves = ga . run () The preceding code will run the GA algorithm nine times for at most 1024 iterations per run. Each run is a permutation from the list of population_sizes and mutation_rates . Note that the initial state parameters here are just toy values picked specifically for this example. You will have to choose your own range of values for your assignment. I strongly recommend you don't just copy these, or you will find that the grading is unlikely to go the way you would like. Really. I mean it... A mutation rate of 0.5 is little better than a pure random search. The output in the df_run_stats dataframe contains snapshots of the state of the algorithm at the iterations specified in the iteration_list passed into the runner class. The first row (corresponding to the first run of this algorithm) are as follows: df_run_stats [[ \"Iteration\" , \"Fitness\" , \"FEvals\" , \"Time\" , \"State\" ]][: 1 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Iteration Fitness FEvals Time State 0 0 2722.031402 10 0.00062 [19, 13, 12, 9, 5, 6, 2, 3, 18, 8, 4, 7, 0, 14... The state information is excluded from the previous output. A sample of this is below: state_sample = df_run_stats [[ \"Population Size\" , \"Mutation Rate\" ]][: 1 ] state_sample .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Population Size Mutation Rate 0 10 0.1 So, to pick out the most performant run from the dataframe, you need to find the row with the best fitness. As TSP is a minimization problem, you'd pick the row with the minimum fitness. However, I'm going to look in the run_curves (which stores minimal basic information every iteration) to find out which input state achieved the best fitness in the fewest fitness evaluations. best_fitness = df_run_curves [ \"Fitness\" ] . min () best_runs = df_run_curves [ df_run_curves [ \"Fitness\" ] == best_fitness ] best_runs [: 10 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Iteration Time Fitness FEvals Population Size Mutation Rate max_iters 3709 707 0.296142 941.582778 14903.0 20 0.1 1024 3710 708 0.296608 941.582778 14924.0 20 0.1 1024 3711 709 0.297102 941.582778 14945.0 20 0.1 1024 3712 710 0.297565 941.582778 14966.0 20 0.1 1024 3713 711 0.298021 941.582778 14987.0 20 0.1 1024 3714 712 0.298481 941.582778 15008.0 20 0.1 1024 3715 713 0.298940 941.582778 15029.0 20 0.1 1024 3716 714 0.299380 941.582778 15050.0 20 0.1 1024 3717 715 0.299820 941.582778 15071.0 20 0.1 1024 3718 716 0.300268 941.582778 15092.0 20 0.1 1024 This gives us nine candidates for the best run. We are going to pick the one with that reached the best fitness value in the fewest number of evaluations. (We could also have chosen to use Iterations as our criteria.) minimum_evaluations = best_runs [ \"FEvals\" ] . min () best_run = best_runs [ best_runs [ \"FEvals\" ] == minimum_evaluations ] The best runs using these criteria is as follows: best_run .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Iteration Time Fitness FEvals Population Size Mutation Rate max_iters 3709 707 0.296142 941.582778 14903.0 20 0.1 1024 This has the following identifying state information: best_mr = best_run [ \"Mutation Rate\" ] . iloc ()[ 0 ] best_pop_size = best_run [ \"Population Size\" ] . iloc ()[ 0 ] print ( f \"Best Mutation Rate: { best_mr } , best Population Size: { best_pop_size } \" ) Best Mutation Rate: 0.1, best Population Size: 20 To map this back to the run_stats we look at the configuration data included in the curve data. The curve data includes at least the minimum identifying information to determine which run each row came from. In this case, the values we are looking for are the Mutation Rate and Population Size . So, we are looking for all rows in df_run_stats where the mutation rate and population size are equal to our best values. run_stats_best_run = df_run_stats [( df_run_stats [ \"Mutation Rate\" ] == best_mr ) & ( df_run_stats [ \"Population Size\" ] == best_pop_size )] run_stats_best_run [[ \"Iteration\" , \"Fitness\" , \"FEvals\" , \"Time\" ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Iteration Fitness FEvals Time 36 0 2722.031402 20 0.000232 37 1 2141.868537 42 0.002583 38 2 2141.868537 63 0.004905 39 4 2141.868537 105 0.007919 40 8 2044.319536 190 0.012401 41 16 1807.055513 360 0.019740 42 32 1785.714484 697 0.032535 43 64 1479.922718 1376 0.059770 44 128 1151.614761 2731 0.114027 45 256 1081.456468 5421 0.211686 46 512 970.009048 10803 0.410439 47 1024 941.582778 21560 0.831667 And the best state associated with this is: best_state = run_stats_best_run [[ \"State\" ]] . tail ( 1 ) best_state .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } State 47 [9, 0, 19, 17, 14, 15, 16, 8, 12, 5, 1, 6, 18,... Which results in a graph looking like this: ordered_state = literal_eval ( run_stats_best_run [ \"State\" ] . tail ( 1 ) . values [ 0 ]) edge_labels = {( ordered_state [ i ], ordered_state [( i + 1 ) % len ( ordered_state )]): f \" { str ( i + 1 ) } \u279c\" for i in range ( len ( ordered_state ))} fig , ax = plt . subplots ( 1 ) # Prepare 2 plots ax . set_yticklabels ([]) ax . set_xticklabels ([]) for i , ( x , y ) in enumerate ( problem . coords ): ax . scatter ( x , y , s = 1 , c = \"green\" if i == 5 else \"cornflowerblue\" ) # plot A for i in range ( len ( ordered_state )): start_node = ordered_state [ i ] end_node = ordered_state [( i + 1 ) % len ( ordered_state )] start_pos = problem . coords [ start_node ] end_pos = problem . coords [ end_node ] ax . annotate ( \"\" , xy = start_pos , xycoords = \"data\" , xytext = end_pos , textcoords = \"data\" , c = \"red\" , arrowprops = dict ( arrowstyle = \"->\" , ec = \"red\" , connectionstyle = \"arc3\" ), ) node_labels = { k : str ( v ) for k , v in enumerate ( string . ascii_uppercase ) if k < len ( problem . source_graph . nodes )} for i in node_labels . keys (): x , y = problem . coords [ i ] plt . text ( x , y , node_labels [ i ], ha = \"center\" , va = \"center\" , c = \"white\" , fontweight = \"bold\" , bbox = dict ( boxstyle = f \"circle,pad=0.15\" , fc = \"green\" if i == ordered_state [ 0 ] else \"cornflowerblue\" ), ) plt . tight_layout () plt . show () And, to verify that the route is correct (or at least, the shortest one found): all_edge_lengths = {( x , y ): d for x , y , d in problem . distances } all_edge_lengths . update ({( y , x ): d for x , y , d in problem . distances }) route_length = sum ([ all_edge_lengths [ k ] for k in edge_labels . keys ()]) print ( f \"route_length: ( { round ( route_length , 6 ) } ) equal to best_fitness: ( { round ( best_fitness , 6 ) } )\" ) route_length: (941.582778) equal to best_fitness: (941.582778) Example 4: Using the NNGSRunner with the RHC algorithm # # Load and Split data into training and test sets data = load_iris () X_train , X_test , y_train , y_test = train_test_split ( data . data , data . target , test_size = 0.3 , random_state = 123456 ) # Normalize feature data scaler = MinMaxScaler () X_train_scaled = scaler . fit_transform ( X_train ) X_test_scaled = scaler . transform ( X_test ) # One hot encode target values one_hot = OneHotEncoder () y_train_hot = np . asarray ( one_hot . fit_transform ( y_train . reshape ( - 1 , 1 )) . todense ()) y_test_hot = np . asarray ( one_hot . transform ( y_test . reshape ( - 1 , 1 )) . todense ()) grid_search_parameters = { 'max_iters' : [ 1000 ], # nn params 'learning_rate' : [ 1e-2 ], # nn params 'activation' : [ mlrose . relu ], # nn params 'restarts' : [ 1 ], # rhc params } nnr = NNGSRunner ( x_train = X_train_scaled , y_train = y_train_hot , x_test = X_test_scaled , y_test = y_test_hot , experiment_name = 'nn_test_rhc' , algorithm = mlrose . algorithms . rhc . random_hill_climb , grid_search_parameters = grid_search_parameters , iteration_list = [ 1 , 10 , 50 , 100 , 250 , 500 , 1000 ], hidden_layer_sizes = [[ 2 ]], clip_max = 5 , n_jobs = 5 , seed = 123456 ) run_stats_df , curves_df , cv_results_df , grid_search_cv = nnr . run () Fitting 5 folds for each of 1 candidates, totalling 5 fits The runner returns the run_stats and curves corresponding to best hyperparameter combination, as well as the cross validation results and the underlying GridSearchCV object used in the run. y_test_pred = grid_search_cv . predict ( X_test_scaled ) y_test_accuracy = accuracy_score ( np . asarray ( y_test_hot ), y_test_pred ) y_test_accuracy 0.3333333333333333 y_train_pred = grid_search_cv . predict ( X_train_scaled ) y_train_accuracy = accuracy_score ( y_train_hot , y_train_pred ) y_train_accuracy 0.3333333333333333 Run stats dataframe run_stats_df [[ \"current_restart\" , \"Iteration\" , \"Fitness\" , \"FEvals\" , \"Time\" , \"learning_rate\" ]][: 14 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } current_restart Iteration Fitness FEvals Time learning_rate 0 0 0 1.306448 1 0.001905 0.01 1 0 1 1.290536 3 0.004327 0.01 2 0 10 1.246087 14 0.009143 0.01 3 0 50 1.115004 67 0.027937 0.01 4 0 100 1.044654 127 0.048129 0.01 5 0 250 0.877647 308 0.107263 0.01 6 0 500 0.739120 614 0.212110 0.01 7 0 1000 0.732424 1175 0.433252 0.01 8 1 0 1.306448 1175 0.436525 0.01 9 1 1 1.306448 1176 0.438441 0.01 10 1 10 1.247648 1188 0.444550 0.01 11 1 50 1.148455 1239 0.465959 0.01 12 1 100 1.105441 1298 0.491722 0.01 13 1 250 0.931072 1490 0.570107 0.01 curves dataframe curves_df [[ \"current_restart\" , \"Iteration\" , \"Fitness\" , \"FEvals\" , \"Time\" , \"learning_rate\" ]][: 20 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } current_restart Iteration Fitness FEvals Time learning_rate 0 0 0 1.290536 1.0 0.001905 0.01 1 0 1 1.290536 3.0 0.004327 0.01 2 0 2 1.290536 4.0 0.005450 0.01 3 0 3 1.290536 5.0 0.005843 0.01 4 0 4 1.263664 7.0 0.006530 0.01 5 0 5 1.263664 8.0 0.006857 0.01 6 0 6 1.246087 10.0 0.007742 0.01 7 0 7 1.246087 11.0 0.008119 0.01 8 0 8 1.246087 12.0 0.008459 0.01 9 0 9 1.246087 13.0 0.008787 0.01 10 0 10 1.246087 14.0 0.009143 0.01 11 0 11 1.246087 15.0 0.010364 0.01 12 0 12 1.246087 16.0 0.010792 0.01 13 0 13 1.246087 17.0 0.011168 0.01 14 0 14 1.246087 18.0 0.011561 0.01 15 0 15 1.246087 19.0 0.011916 0.01 16 0 16 1.246087 20.0 0.012291 0.01 17 0 17 1.232142 22.0 0.013562 0.01 18 0 18 1.216927 24.0 0.014337 0.01 19 0 19 1.216927 25.0 0.014832 0.01 cv results dataframe cv_results_df [ [ \"mean_test_score\" , \"rank_test_score\" , \"mean_train_score\" , \"param_activation\" , \"param_hidden_layer_sizes\" , \"param_learning_rate\" , \"param_max_iters\" , \"param_restarts\" , ] ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean_test_score rank_test_score mean_train_score param_activation param_hidden_layer_sizes param_learning_rate param_max_iters param_restarts 0 0.333333 1 0.333333 relu [2] 0.01 1000 1","title":"1 - Runners in code"},{"location":"problem_examples/#overview","text":"These examples will not solve assignment 2 for you, but they will give you some idea on how to use the problem generator and runner classes. Hopefully this will result in slightly fewer \"How do I \\<insert basic usage here>\" questions every semester... Also, and in case it hasn't been made clear enough by the TAs, using any of the visualizations from this tutorial for your report is a bad idea for two reasons: 1. It provides nothing useful as far as the assignment goes, and 2. The TAs will undoubtedly frown upon it. Visualization is part of the analysis and, for the most part, you're supposed to do that by yourself. Just including images of the before/after state of a problem really isn't useful in terms of what you're supposed to be analyzing.","title":"Overview"},{"location":"problem_examples/#import-libraries","text":"% pip install chess IPython Requirement already satisfied: chess in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (1.10.0) Requirement already satisfied: IPython in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (8.25.0) Requirement already satisfied: decorator in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (from IPython) (5.1.1) Requirement already satisfied: jedi>=0.16 in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (from IPython) (0.18.1) Requirement already satisfied: matplotlib-inline in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (from IPython) (0.1.6) Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (from IPython) (3.0.47) Requirement already satisfied: pygments>=2.4.0 in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (from IPython) (2.18.0) Requirement already satisfied: stack-data in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (from IPython) (0.2.0) Requirement already satisfied: traitlets>=5.13.0 in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (from IPython) (5.14.3) Requirement already satisfied: typing-extensions>=4.6 in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (from IPython) (4.12.2) Requirement already satisfied: pexpect>4.3 in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (from IPython) (4.9.0) Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (from jedi>=0.16->IPython) (0.8.3) Requirement already satisfied: ptyprocess>=0.5 in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (from pexpect>4.3->IPython) (0.7.0) Requirement already satisfied: wcwidth in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->IPython) (0.2.13) Requirement already satisfied: executing in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (from stack-data->IPython) (0.8.3) Requirement already satisfied: asttokens in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (from stack-data->IPython) (2.0.5) Requirement already satisfied: pure-eval in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (from stack-data->IPython) (0.2.2) Requirement already satisfied: six in /Users/kylenakamura/anaconda3/envs/machine-learning/lib/python3.11/site-packages (from asttokens->stack-data->IPython) (1.16.0) Note: you may need to restart the kernel to use updated packages. from IPython.display import HTML import numpy as np import logging import networkx as nx import matplotlib.pyplot as plt import string from ast import literal_eval import chess from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import MinMaxScaler , OneHotEncoder from sklearn.metrics import accuracy_score import mlrose_ky as mlrose from mlrose_ky.generators import QueensGenerator , MaxKColorGenerator , TSPGenerator from mlrose_ky.runners import SARunner , GARunner , NNGSRunner # Hide warnings from libraries logging . basicConfig ( level = logging . WARNING )","title":"Import Libraries"},{"location":"problem_examples/#example-1-solving-the-8-queens-problem-using-the-sa-algorithm","text":"","title":"Example 1: Solving the 8-Queens problem using the SA algorithm"},{"location":"problem_examples/#initializing-and-viewing-the-problem","text":"First, we'll use the QueensGenerator to create an instance of the 8-Queens problem. # Generate a new 8-Queens optimization problem using a fixed seed problem = QueensGenerator . generate ( seed = 123456 , size = 8 ) The initial, un-optimized state can be seen below, both as a list and as a chess board. # View the initial state as a list state = problem . get_state () print ( 'Initial state:' , state ) # View the initial state as a chess board board_layout = \"/\" . join ([ \"\" . join (([ str ( s )] if s > 0 else []) + [ \"Q\" ] + ([ str (( 7 - s ))] if s < 7 else [])) for s in state ]) chess . Board ( board_layout ) # You may need to \"trust\" this notebook for the board visualization to work Initial state: [2 3 3 2 7 0 1 1]","title":"Initializing and viewing the problem"},{"location":"problem_examples/#solving-8-queens-using-a-runner-ie-grid-search","text":"Runners are used to execute \"grid search\" experiments on optimization problems. We'll use the Simulated Annealing Runner (SARunner) to perform a grid search on the 8-Queens problem and then extract the optimal SA hyperparameters. Here is a brief explanation of the SARunner parameters used in the example below: - max_attempts : A list of maximum attempts to try improving the fitness score before terminating a run - temperature_list : A list of temperatures to try when initializing the SA algorithm's decay function (e.g., GeomDecay(init_temp=1.0) ) - decay_list : A list of decay schedules to try as the SA algorithm's decay function (e.g., GeomDecay , ExpDecay , etc.) - iteration_list : A list of iterations to snapshot the state of the algorithm at (only determines the rows that the Runner will output) Disclaimer: the values used here are just toy values picked specifically for this example. You will have to choose your own range of values for your experiments. I strongly recommend you don't just copy these, or you will find that the grading is unlikely to go the way you would like. # Create an SA Runner instance to solve the problem sa = SARunner ( problem = problem , experiment_name = \"queens_8_sa\" , seed = 123456 , output_directory = None , # Note: specify an output directory (str) to have these results saved to disk max_attempts = 100 , temperature_list = [ 0.1 , 0.5 , 0.75 , 1.0 , 2.0 , 5.0 ], decay_list = [ mlrose . GeomDecay ], iteration_list = 2 ** np . arange ( 11 ), # list of 11 integers from 2^0 to 2^11 ) # Run the SA Runner and retrieve its results df_run_stats , df_run_curves = sa . run () # Calculate some simple stats about the experiment temperatures_per_run = max ( 1 , len ( sa . temperature_list )) decays_per_run = max ( 1 , len ( sa . decay_list )) iters_per_run = len ( sa . iteration_list ) + 1 total_runs = temperatures_per_run * decays_per_run print ( f \"The experiment executed { total_runs } runs, each with { iters_per_run } snapshots at different iterations.\" ) print ( f \"In total, the output dataframe should contain { total_runs * iters_per_run } rows.\" ) The experiment executed 6 runs, each with 12 snapshots at different iterations. In total, the output dataframe should contain 72 rows. The df_run_stats dataframe contains snapshots of the state of the algorithm at the iterations specified in the iteration_list . Since iterations_list contains 11 numbers, and iteration 0 is always included in the results, each run will take up 12 rows of the dataframe. The first 12 rows (i.e., results from the first run) are shown below: HTML ( df_run_stats [[ \"Iteration\" , \"Fitness\" , \"FEvals\" , \"Time\" , \"State\" ]][: iters_per_run ] . to_html ( index = False )) Iteration Fitness FEvals Time State 0 11.0 0 0.000856 [1, 2, 2, 1, 0, 3, 7, 3] 1 9.0 2 0.002331 [1, 2, 2, 0, 0, 3, 7, 3] 2 8.0 4 0.002864 [1, 2, 2, 0, 0, 3, 7, 5] 4 8.0 7 0.003399 [1, 2, 2, 5, 0, 3, 7, 5] 8 5.0 13 0.004127 [1, 2, 7, 5, 0, 3, 5, 5] 16 4.0 24 0.005042 [1, 2, 7, 5, 3, 0, 5, 5] 32 4.0 47 0.006737 [1, 5, 7, 5, 0, 0, 3, 4] 64 1.0 86 0.009472 [1, 5, 2, 6, 3, 0, 7, 4] 128 1.0 155 0.014421 [1, 5, 2, 6, 3, 0, 4, 7] 256 1.0 295 0.025960 [1, 7, 2, 6, 3, 5, 0, 4] 512 0.0 461 0.044061 [1, 5, 0, 6, 3, 7, 2, 4] 1024 0.0 461 0.044061 [1, 5, 0, 6, 3, 7, 2, 4] Some information was intentionally excluded from the previous output. Let's now preview the entirety of the first run: HTML ( df_run_stats [: iters_per_run ] . to_html ( index = False )) Iteration Fitness FEvals Time State schedule_type schedule_init_temp schedule_decay schedule_min_temp schedule_current_value Temperature max_iters 0 11.0 0 0.000856 [1, 2, 2, 1, 0, 3, 7, 3] geometric 0.1 0.99 0.001 0.099999 0.1 1024 1 9.0 2 0.002331 [1, 2, 2, 0, 0, 3, 7, 3] geometric 0.1 0.99 0.001 0.099998 0.1 1024 2 8.0 4 0.002864 [1, 2, 2, 0, 0, 3, 7, 5] geometric 0.1 0.99 0.001 0.099997 0.1 1024 4 8.0 7 0.003399 [1, 2, 2, 5, 0, 3, 7, 5] geometric 0.1 0.99 0.001 0.099997 0.1 1024 8 5.0 13 0.004127 [1, 2, 7, 5, 0, 3, 5, 5] geometric 0.1 0.99 0.001 0.099996 0.1 1024 16 4.0 24 0.005042 [1, 2, 7, 5, 3, 0, 5, 5] geometric 0.1 0.99 0.001 0.099995 0.1 1024 32 4.0 47 0.006737 [1, 5, 7, 5, 0, 0, 3, 4] geometric 0.1 0.99 0.001 0.099993 0.1 1024 64 1.0 86 0.009472 [1, 5, 2, 6, 3, 0, 7, 4] geometric 0.1 0.99 0.001 0.099990 0.1 1024 128 1.0 155 0.014421 [1, 5, 2, 6, 3, 0, 4, 7] geometric 0.1 0.99 0.001 0.099986 0.1 1024 256 1.0 295 0.025960 [1, 7, 2, 6, 3, 5, 0, 4] geometric 0.1 0.99 0.001 0.099974 0.1 1024 512 0.0 461 0.044061 [1, 5, 0, 6, 3, 7, 2, 4] geometric 0.1 0.99 0.001 0.099956 0.1 1024 1024 0.0 461 0.044061 [1, 5, 0, 6, 3, 7, 2, 4] geometric 0.1 0.99 0.001 0.099956 0.1 1024 What does all this information tell us about the first run of our experiment? Iteration shows the index of the snapshot, with 12 snapshots per run in this example. Fitness shows the fitness score of the state at the corresponding iteration, where 0.0 is optimal for minimization problems. FEvals shows the number of fitness function evaluations performed by the algorithm at the corresponding iteration. Time shows the time elapsed (calculated using time.perf_counter() ) up to the corresponding iteration. State shows the state of the algorithm at the corresponding iteration (see mlrose_ky.fitness.queens for more details). Temperature shows the decay function (and its parameters) that was used for this run. We will have 6 unique values in this column, one for each run. max_iters shows the maximum number of iterations allowed for the algorithm to run. It defaults to max(iteration_list) for all Runners, which is 1024 in this case. To pick out the most performant run from the dataframe, we need to find the row with the best fitness. Since Queens is a minimization problem, we're looking for the row with minimal fitness (i.e., zero). It's likely that multiple runs will achieve the same fitness, so we need to find the run that achieved the best Fitness in the fewest FEvals ( Note: we could make this selection using Iterations or Time if we so desired. ) best_fitness = df_run_stats [ \"Fitness\" ] . min () # Should be 0.0 in this case # Get all runs with the best fitness value best_runs = df_run_stats [ df_run_stats [ \"Fitness\" ] == best_fitness ] best_runs .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Iteration Fitness FEvals Time State schedule_type schedule_init_temp schedule_decay schedule_min_temp schedule_current_value Temperature max_iters 10 512 0.0 461 0.044061 [1, 5, 0, 6, 3, 7, 2, 4] geometric 0.1 0.99 0.001 0.099956 0.1 1024 11 1024 0.0 461 0.044061 [1, 5, 0, 6, 3, 7, 2, 4] geometric 0.1 0.99 0.001 0.099956 0.1 1024 22 512 0.0 461 0.040817 [1, 5, 0, 6, 3, 7, 2, 4] geometric 0.5 0.99 0.001 0.499795 0.5 1024 23 1024 0.0 461 0.040817 [1, 5, 0, 6, 3, 7, 2, 4] geometric 0.5 0.99 0.001 0.499795 0.5 1024 58 512 0.0 427 0.044455 [7, 1, 3, 0, 6, 4, 2, 5] geometric 2.0 0.99 0.001 1.999107 2.0 1024 59 1024 0.0 427 0.044455 [7, 1, 3, 0, 6, 4, 2, 5] geometric 2.0 0.99 0.001 1.999107 2.0 1024 70 512 0.0 583 0.061001 [6, 0, 2, 7, 5, 3, 1, 4] geometric 5.0 0.99 0.001 4.996936 5.0 1024 71 1024 0.0 583 0.061001 [6, 0, 2, 7, 5, 3, 1, 4] geometric 5.0 0.99 0.001 4.996936 5.0 1024 This gives us our candidates for the best run. The best run will be the one that achieved the best fitness in the fewest evaluations. minimum_evaluations = best_runs [ \"FEvals\" ] . min () # Should be 461 in this case # Extract the best run with the minimum number of evaluations best_run = best_runs [ best_runs [ \"FEvals\" ] == minimum_evaluations ] The best run using these criteria is as follows: print ( best_run . iloc [ 0 ]) Iteration 512 Fitness 0.0 FEvals 427 Time 0.044455 State [7, 1, 3, 0, 6, 4, 2, 5] schedule_type geometric schedule_init_temp 2.0 schedule_decay 0.99 schedule_min_temp 0.001 schedule_current_value 1.999107 Temperature 2.0 max_iters 1024 Name: 58, dtype: object Which has the following identifying state information: best_temperature_param = best_run [ \"Temperature\" ] . iloc [ 0 ] . init_temp best_temperature_param 2.0 To map this result back to the original output of the Runner, we are looking for all rows in df_run_stats where the temperature is equal to 2. run_stats_best_run = df_run_stats [ df_run_stats [ \"schedule_init_temp\" ] == best_temperature_param ] run_stats_best_run [[ \"Iteration\" , \"Fitness\" , \"FEvals\" , \"Time\" , \"State\" ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Iteration Fitness FEvals Time State 48 0 11.0 0 0.000136 [1, 2, 2, 1, 0, 3, 7, 3] 49 1 9.0 2 0.001460 [1, 2, 2, 0, 0, 3, 7, 3] 50 2 8.0 4 0.002754 [1, 2, 2, 0, 0, 3, 7, 5] 51 4 8.0 7 0.004162 [1, 2, 2, 5, 0, 3, 7, 5] 52 8 7.0 14 0.005876 [1, 2, 2, 5, 0, 3, 5, 5] 53 16 6.0 27 0.007869 [3, 2, 3, 5, 0, 1, 5, 5] 54 32 4.0 57 0.011020 [3, 5, 6, 5, 5, 0, 4, 7] 55 64 5.0 114 0.015731 [2, 0, 3, 6, 1, 2, 1, 7] 56 128 3.0 205 0.022950 [2, 0, 6, 3, 5, 0, 4, 3] 57 256 2.0 358 0.036341 [7, 1, 3, 6, 6, 4, 0, 5] 58 512 0.0 427 0.044455 [7, 1, 3, 0, 6, 4, 2, 5] 59 1024 0.0 427 0.044455 [7, 1, 3, 0, 6, 4, 2, 5] And the best state associated with this is: best_state = run_stats_best_run [[ \"schedule_current_value\" , \"schedule_init_temp\" , \"schedule_min_temp\" ]] . tail ( 1 ) best_state .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } schedule_current_value schedule_init_temp schedule_min_temp 59 1.999107 2.0 0.001 The final state is as follows: state = literal_eval ( run_stats_best_run [ \"State\" ] . tail ( 1 ) . values [ 0 ]) print ( state ) board_layout = \"/\" . join ([ \"\" . join (([ str ( s )] if s > 0 else []) + [ \"Q\" ] + ([ str (( 7 - s ))] if s < 7 else [])) for s in state ]) board = chess . Board ( board_layout ) board [7, 1, 3, 0, 6, 4, 2, 5]","title":"Solving 8-Queens using a Runner (i.e., grid search)"},{"location":"problem_examples/#example-2-generating-and-running-max-k-color-using-the-ga-algorithm","text":"# Generate a new Max K problem using a fixed seed. problem = MaxKColorGenerator () . generate ( seed = 123456 , number_of_nodes = 10 , max_connections_per_node = 3 , max_colors = 3 ) The input graph generated for the problem looks like this: nx . draw ( problem . source_graph , pos = nx . spring_layout ( problem . source_graph , seed = 3 )) plt . show () # create a runner class and solve the problem ga = GARunner ( problem = problem , experiment_name = \"max_k_ga\" , output_directory = None , # note: specify an output directory to have results saved to disk seed = 123456 , iteration_list = 2 ** np . arange ( 11 ), population_sizes = [ 10 , 20 , 50 ], mutation_rates = [ 0.1 , 0.2 , 0.5 ], ) # the two data frames will contain the results df_run_stats , df_run_curves = ga . run () The preceding code will run the GA algorithm nine times for at most 1024 iterations per run. Each run is a permutation from the list of population_sizes and mutation_rates . Note that the initial state parameters here are just toy values picked specifically for this example. You will have to choose your own range of values for your assignment. I strongly recommend you don't just copy these, or you will find that the grading is unlikely to go the way you would like. Really. I mean it... A mutation rate of 0.5 is little better than a pure random search. The output in the df_run_stats dataframe contains snapshots of the state of the algorithm at the iterations specified in the iteration_list passed into the runner class. The first row (corresponding to the first run of this algorithm) are as follows: df_run_stats [[ \"Iteration\" , \"Fitness\" , \"FEvals\" , \"Time\" , \"State\" ]][ 0 : 1 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Iteration Fitness FEvals Time State 0 0 3.0 10 0.000702 [1, 2, 2, 1, 0, 0, 0, 0, 2, 2] The state information is excluded from the previous output. A sample of this is below: state_sample = df_run_stats [[ \"Population Size\" , \"Mutation Rate\" ]][: 1 ] state_sample .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Population Size Mutation Rate 0 10 0.1 So, to pick out the most performant run from the dataframe, you need to find the row with the best fitness. As Max-K-Color is a minimization problem, you'd pick the row with the minimum fitness. However, I'm going to look in the run_curves (which stores minimal basic information every iteration) to find out which input state achieved the best fitness in the fewest fitness evaluations. best_fitness = df_run_curves [ \"Fitness\" ] . min () best_runs = df_run_curves [ df_run_curves [ \"Fitness\" ] == best_fitness ] best_runs .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Iteration Time Fitness FEvals Population Size Mutation Rate max_iters 4 4 0.003967 0.0 57.0 10 0.1 1024 9 4 0.003967 0.0 57.0 10 0.2 1024 18 8 0.001445 0.0 101.0 10 0.5 1024 24 5 0.000106 0.0 127.0 20 0.1 1024 27 2 0.003304 0.0 64.0 20 0.2 1024 31 3 0.003797 0.0 85.0 20 0.5 1024 41 9 0.001615 0.0 511.0 50 0.1 1024 45 3 0.003797 0.0 205.0 50 0.2 1024 50 4 0.003967 0.0 256.0 50 0.5 1024 This gives us nine candidates for the best run. We are going to pick the one with that reached the best fitness value in the fewest number of evaluations. (We could also have chosen to use Iterations as our criteria.) minimum_evaluations = best_runs [ \"FEvals\" ] . min () best_run = best_runs [ best_runs [ \"FEvals\" ] == minimum_evaluations ] The best runs using these criteria is as follows: best_run .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Iteration Time Fitness FEvals Population Size Mutation Rate max_iters 4 4 0.003967 0.0 57.0 10 0.1 1024 9 4 0.003967 0.0 57.0 10 0.2 1024 We will arbitrarily pick the first row for this example, which has the following identifying state information: best_mr = best_run [ \"Mutation Rate\" ] . iloc ()[ 0 ] best_pop_size = best_run [ \"Population Size\" ] . iloc ()[ 0 ] print ( f \"Best Mutation Rate: { best_mr } , best Population Size: { best_pop_size } \" ) Best Mutation Rate: 0.1, best Population Size: 10 To map this back to the run_stats we look at the configuration data included in the curve data. The curve data includes at least the minimum identifying information to determine which run each row came from. In this case, the values we are looking for are the Mutation Rate and Population Size . So, we are looking for all rows in df_run_stats where the mutation rate and population size are equal to our best values. run_stats_best_run = df_run_stats [( df_run_stats [ \"Mutation Rate\" ] == best_mr ) & ( df_run_stats [ \"Population Size\" ] == best_pop_size )] run_stats_best_run [[ \"Iteration\" , \"Fitness\" , \"FEvals\" , \"Time\" ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Iteration Fitness FEvals Time 0 0 3.0 10 0.000702 1 1 3.0 21 0.002124 2 2 2.0 33 0.003304 3 4 0.0 57 0.003967 4 8 0.0 57 0.003967 5 16 0.0 57 0.003967 6 32 0.0 57 0.003967 7 64 0.0 57 0.003967 8 128 0.0 57 0.003967 9 256 0.0 57 0.003967 10 512 0.0 57 0.003967 11 1024 0.0 57 0.003967 And the best state associated with this is: best_state = run_stats_best_run [[ \"State\" ]] . tail ( 1 ) best_state .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } State 11 [0, 1, 2, 0, 0, 2, 0, 1, 2, 1] For the following node ordering: print ([ n for n in problem . source_graph . nodes ]) [0, 2, 8, 1, 3, 4, 6, 7, 9, 5] Reordering the state by ascending node number gives the following: color_indexes = literal_eval ( run_stats_best_run [ \"State\" ] . tail ( 1 ) . values [ 0 ]) ordered_state = [ color_indexes [ n ] for n in problem . source_graph . nodes ] print ( ordered_state ) [0, 2, 2, 1, 0, 0, 0, 1, 1, 2] Which results in a graph looking like this: colors = [ \"lightcoral\" , \"lightgreen\" , \"yellow\" ] node_color_map = [ colors [ s ] for s in ordered_state ] nx . draw ( problem . source_graph , pos = nx . spring_layout ( problem . source_graph , seed = 3 ), with_labels = True , node_color = node_color_map ) plt . show ()","title":"Example 2: Generating and Running Max K Color using the GA algorithm"},{"location":"problem_examples/#example-3-generating-and-running-tsp-using-the-ga-algorithm","text":"# Generate a new TSP problem using a fixed seed. problem = TSPGenerator () . generate ( seed = 123456 , number_of_cities = 20 ) The input graph generated for the problem looks like this: fig , ax = plt . subplots ( 1 ) # Prepare 2 plots ax . set_yticklabels ([]) ax . set_xticklabels ([]) for i , ( x , y ) in enumerate ( problem . coords ): ax . scatter ( x , y , s = 200 , c = \"cornflowerblue\" ) # plot A node_labels = { k : str ( v ) for k , v in enumerate ( string . ascii_uppercase ) if k < len ( problem . source_graph . nodes )} for i in node_labels . keys (): x , y = problem . coords [ i ] plt . text ( x , y , node_labels [ i ], ha = \"center\" , va = \"center\" , c = \"white\" , fontweight = \"bold\" , bbox = dict ( boxstyle = f \"circle,pad=0.15\" , fc = \"cornflowerblue\" ), ) plt . tight_layout () plt . show () # create a runner class and solve the problem ga = GARunner ( problem = problem , experiment_name = \"tsp_ga\" , output_directory = None , # note: specify an output directory to have results saved to disk seed = 123456 , iteration_list = 2 ** np . arange ( 11 ), population_sizes = [ 10 , 20 ], mutation_rates = [ 0.1 , 0.25 , 0.5 ], ) # the two data frames will contain the results df_run_stats , df_run_curves = ga . run () The preceding code will run the GA algorithm nine times for at most 1024 iterations per run. Each run is a permutation from the list of population_sizes and mutation_rates . Note that the initial state parameters here are just toy values picked specifically for this example. You will have to choose your own range of values for your assignment. I strongly recommend you don't just copy these, or you will find that the grading is unlikely to go the way you would like. Really. I mean it... A mutation rate of 0.5 is little better than a pure random search. The output in the df_run_stats dataframe contains snapshots of the state of the algorithm at the iterations specified in the iteration_list passed into the runner class. The first row (corresponding to the first run of this algorithm) are as follows: df_run_stats [[ \"Iteration\" , \"Fitness\" , \"FEvals\" , \"Time\" , \"State\" ]][: 1 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Iteration Fitness FEvals Time State 0 0 2722.031402 10 0.00062 [19, 13, 12, 9, 5, 6, 2, 3, 18, 8, 4, 7, 0, 14... The state information is excluded from the previous output. A sample of this is below: state_sample = df_run_stats [[ \"Population Size\" , \"Mutation Rate\" ]][: 1 ] state_sample .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Population Size Mutation Rate 0 10 0.1 So, to pick out the most performant run from the dataframe, you need to find the row with the best fitness. As TSP is a minimization problem, you'd pick the row with the minimum fitness. However, I'm going to look in the run_curves (which stores minimal basic information every iteration) to find out which input state achieved the best fitness in the fewest fitness evaluations. best_fitness = df_run_curves [ \"Fitness\" ] . min () best_runs = df_run_curves [ df_run_curves [ \"Fitness\" ] == best_fitness ] best_runs [: 10 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Iteration Time Fitness FEvals Population Size Mutation Rate max_iters 3709 707 0.296142 941.582778 14903.0 20 0.1 1024 3710 708 0.296608 941.582778 14924.0 20 0.1 1024 3711 709 0.297102 941.582778 14945.0 20 0.1 1024 3712 710 0.297565 941.582778 14966.0 20 0.1 1024 3713 711 0.298021 941.582778 14987.0 20 0.1 1024 3714 712 0.298481 941.582778 15008.0 20 0.1 1024 3715 713 0.298940 941.582778 15029.0 20 0.1 1024 3716 714 0.299380 941.582778 15050.0 20 0.1 1024 3717 715 0.299820 941.582778 15071.0 20 0.1 1024 3718 716 0.300268 941.582778 15092.0 20 0.1 1024 This gives us nine candidates for the best run. We are going to pick the one with that reached the best fitness value in the fewest number of evaluations. (We could also have chosen to use Iterations as our criteria.) minimum_evaluations = best_runs [ \"FEvals\" ] . min () best_run = best_runs [ best_runs [ \"FEvals\" ] == minimum_evaluations ] The best runs using these criteria is as follows: best_run .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Iteration Time Fitness FEvals Population Size Mutation Rate max_iters 3709 707 0.296142 941.582778 14903.0 20 0.1 1024 This has the following identifying state information: best_mr = best_run [ \"Mutation Rate\" ] . iloc ()[ 0 ] best_pop_size = best_run [ \"Population Size\" ] . iloc ()[ 0 ] print ( f \"Best Mutation Rate: { best_mr } , best Population Size: { best_pop_size } \" ) Best Mutation Rate: 0.1, best Population Size: 20 To map this back to the run_stats we look at the configuration data included in the curve data. The curve data includes at least the minimum identifying information to determine which run each row came from. In this case, the values we are looking for are the Mutation Rate and Population Size . So, we are looking for all rows in df_run_stats where the mutation rate and population size are equal to our best values. run_stats_best_run = df_run_stats [( df_run_stats [ \"Mutation Rate\" ] == best_mr ) & ( df_run_stats [ \"Population Size\" ] == best_pop_size )] run_stats_best_run [[ \"Iteration\" , \"Fitness\" , \"FEvals\" , \"Time\" ]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Iteration Fitness FEvals Time 36 0 2722.031402 20 0.000232 37 1 2141.868537 42 0.002583 38 2 2141.868537 63 0.004905 39 4 2141.868537 105 0.007919 40 8 2044.319536 190 0.012401 41 16 1807.055513 360 0.019740 42 32 1785.714484 697 0.032535 43 64 1479.922718 1376 0.059770 44 128 1151.614761 2731 0.114027 45 256 1081.456468 5421 0.211686 46 512 970.009048 10803 0.410439 47 1024 941.582778 21560 0.831667 And the best state associated with this is: best_state = run_stats_best_run [[ \"State\" ]] . tail ( 1 ) best_state .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } State 47 [9, 0, 19, 17, 14, 15, 16, 8, 12, 5, 1, 6, 18,... Which results in a graph looking like this: ordered_state = literal_eval ( run_stats_best_run [ \"State\" ] . tail ( 1 ) . values [ 0 ]) edge_labels = {( ordered_state [ i ], ordered_state [( i + 1 ) % len ( ordered_state )]): f \" { str ( i + 1 ) } \u279c\" for i in range ( len ( ordered_state ))} fig , ax = plt . subplots ( 1 ) # Prepare 2 plots ax . set_yticklabels ([]) ax . set_xticklabels ([]) for i , ( x , y ) in enumerate ( problem . coords ): ax . scatter ( x , y , s = 1 , c = \"green\" if i == 5 else \"cornflowerblue\" ) # plot A for i in range ( len ( ordered_state )): start_node = ordered_state [ i ] end_node = ordered_state [( i + 1 ) % len ( ordered_state )] start_pos = problem . coords [ start_node ] end_pos = problem . coords [ end_node ] ax . annotate ( \"\" , xy = start_pos , xycoords = \"data\" , xytext = end_pos , textcoords = \"data\" , c = \"red\" , arrowprops = dict ( arrowstyle = \"->\" , ec = \"red\" , connectionstyle = \"arc3\" ), ) node_labels = { k : str ( v ) for k , v in enumerate ( string . ascii_uppercase ) if k < len ( problem . source_graph . nodes )} for i in node_labels . keys (): x , y = problem . coords [ i ] plt . text ( x , y , node_labels [ i ], ha = \"center\" , va = \"center\" , c = \"white\" , fontweight = \"bold\" , bbox = dict ( boxstyle = f \"circle,pad=0.15\" , fc = \"green\" if i == ordered_state [ 0 ] else \"cornflowerblue\" ), ) plt . tight_layout () plt . show () And, to verify that the route is correct (or at least, the shortest one found): all_edge_lengths = {( x , y ): d for x , y , d in problem . distances } all_edge_lengths . update ({( y , x ): d for x , y , d in problem . distances }) route_length = sum ([ all_edge_lengths [ k ] for k in edge_labels . keys ()]) print ( f \"route_length: ( { round ( route_length , 6 ) } ) equal to best_fitness: ( { round ( best_fitness , 6 ) } )\" ) route_length: (941.582778) equal to best_fitness: (941.582778)","title":"Example 3: Generating and Running TSP using the GA algorithm"},{"location":"problem_examples/#example-4-using-the-nngsrunner-with-the-rhc-algorithm","text":"# Load and Split data into training and test sets data = load_iris () X_train , X_test , y_train , y_test = train_test_split ( data . data , data . target , test_size = 0.3 , random_state = 123456 ) # Normalize feature data scaler = MinMaxScaler () X_train_scaled = scaler . fit_transform ( X_train ) X_test_scaled = scaler . transform ( X_test ) # One hot encode target values one_hot = OneHotEncoder () y_train_hot = np . asarray ( one_hot . fit_transform ( y_train . reshape ( - 1 , 1 )) . todense ()) y_test_hot = np . asarray ( one_hot . transform ( y_test . reshape ( - 1 , 1 )) . todense ()) grid_search_parameters = { 'max_iters' : [ 1000 ], # nn params 'learning_rate' : [ 1e-2 ], # nn params 'activation' : [ mlrose . relu ], # nn params 'restarts' : [ 1 ], # rhc params } nnr = NNGSRunner ( x_train = X_train_scaled , y_train = y_train_hot , x_test = X_test_scaled , y_test = y_test_hot , experiment_name = 'nn_test_rhc' , algorithm = mlrose . algorithms . rhc . random_hill_climb , grid_search_parameters = grid_search_parameters , iteration_list = [ 1 , 10 , 50 , 100 , 250 , 500 , 1000 ], hidden_layer_sizes = [[ 2 ]], clip_max = 5 , n_jobs = 5 , seed = 123456 ) run_stats_df , curves_df , cv_results_df , grid_search_cv = nnr . run () Fitting 5 folds for each of 1 candidates, totalling 5 fits The runner returns the run_stats and curves corresponding to best hyperparameter combination, as well as the cross validation results and the underlying GridSearchCV object used in the run. y_test_pred = grid_search_cv . predict ( X_test_scaled ) y_test_accuracy = accuracy_score ( np . asarray ( y_test_hot ), y_test_pred ) y_test_accuracy 0.3333333333333333 y_train_pred = grid_search_cv . predict ( X_train_scaled ) y_train_accuracy = accuracy_score ( y_train_hot , y_train_pred ) y_train_accuracy 0.3333333333333333 Run stats dataframe run_stats_df [[ \"current_restart\" , \"Iteration\" , \"Fitness\" , \"FEvals\" , \"Time\" , \"learning_rate\" ]][: 14 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } current_restart Iteration Fitness FEvals Time learning_rate 0 0 0 1.306448 1 0.001905 0.01 1 0 1 1.290536 3 0.004327 0.01 2 0 10 1.246087 14 0.009143 0.01 3 0 50 1.115004 67 0.027937 0.01 4 0 100 1.044654 127 0.048129 0.01 5 0 250 0.877647 308 0.107263 0.01 6 0 500 0.739120 614 0.212110 0.01 7 0 1000 0.732424 1175 0.433252 0.01 8 1 0 1.306448 1175 0.436525 0.01 9 1 1 1.306448 1176 0.438441 0.01 10 1 10 1.247648 1188 0.444550 0.01 11 1 50 1.148455 1239 0.465959 0.01 12 1 100 1.105441 1298 0.491722 0.01 13 1 250 0.931072 1490 0.570107 0.01 curves dataframe curves_df [[ \"current_restart\" , \"Iteration\" , \"Fitness\" , \"FEvals\" , \"Time\" , \"learning_rate\" ]][: 20 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } current_restart Iteration Fitness FEvals Time learning_rate 0 0 0 1.290536 1.0 0.001905 0.01 1 0 1 1.290536 3.0 0.004327 0.01 2 0 2 1.290536 4.0 0.005450 0.01 3 0 3 1.290536 5.0 0.005843 0.01 4 0 4 1.263664 7.0 0.006530 0.01 5 0 5 1.263664 8.0 0.006857 0.01 6 0 6 1.246087 10.0 0.007742 0.01 7 0 7 1.246087 11.0 0.008119 0.01 8 0 8 1.246087 12.0 0.008459 0.01 9 0 9 1.246087 13.0 0.008787 0.01 10 0 10 1.246087 14.0 0.009143 0.01 11 0 11 1.246087 15.0 0.010364 0.01 12 0 12 1.246087 16.0 0.010792 0.01 13 0 13 1.246087 17.0 0.011168 0.01 14 0 14 1.246087 18.0 0.011561 0.01 15 0 15 1.246087 19.0 0.011916 0.01 16 0 16 1.246087 20.0 0.012291 0.01 17 0 17 1.232142 22.0 0.013562 0.01 18 0 18 1.216927 24.0 0.014337 0.01 19 0 19 1.216927 25.0 0.014832 0.01 cv results dataframe cv_results_df [ [ \"mean_test_score\" , \"rank_test_score\" , \"mean_train_score\" , \"param_activation\" , \"param_hidden_layer_sizes\" , \"param_learning_rate\" , \"param_max_iters\" , \"param_restarts\" , ] ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean_test_score rank_test_score mean_train_score param_activation param_hidden_layer_sizes param_learning_rate param_max_iters param_restarts 0 0.333333 1 0.333333 relu [2] 0.01 1000 1","title":"Example 4: Using the NNGSRunner with the RHC algorithm"},{"location":"runners/","text":"Tutorial - How to use Runners? # Recommendation It is highly recommended that you use the Runners class for the second assignment on randomized optimizations. An example with RHC Runner # The below example illustrates how to initialize an RHCRunner object. import mlrose_ky # initialize the following variables problem = # An opt prob object from [[opt_probs]] seed = # a seed so experiments are reproducible iteration_list = 2 ** np . arange ( 10 ) max_attempts = 5000 restart_list = [ 25 , 75 , 100 ] rhc = mlrose_ky . RHCRunner ( problem = problem , experiment_name = \"exp_name\" , seed = seed iteration_list = iteration_list max_attempts = max_attempts , restart_list = restart_list ) The .run() method can be called on the rhc to return 2 dataframes that contain the results. df_run_stats , df_run_curves = rhc . run () The df_run_stats dataframe records the information based on the parameters in the restart list, i.e. 25 restarts, 75 restarts and 100 restarts. For the other runners, the df_run_stats follows the iteration list instead, i.e. df_run_stats has information based on list mentioned in iteration_list However, we use df_run_curves for plotting that has all the information, i.e. all restarts from 0 to 100. Recommendation Since the experiments take a lot of time to run, it is highly recommended that you save the df_run_curves output using the pandas.to_csv() method.> This saved df_run_curves returns a nice dataframe that can be used with plotting functions (which is a WIP in mlrose-ky ). The output has the following items: - Fitness score - FEvals (Fitness Evaluations) - Time (in seconds) - Other algorithm variables depending on Runner used, eg: Population Size and Keep Percent for MIMIC. Recommendation Simulated Annealing does not track decay types so it is best to record different decay types into df_run_curves before saving the runner. Use runners to make your own custom wrapper # The best way to use runners would be to wrap and call the runners in your own function. One example workflow is below. Pass the following input to the function. Generate a problem using one of the problem generators . Pass in the seed , iteration_list , max_attempts , and restart_list Other parameters based on algorithm used, e.g. temperature and decay for Simulated Annealing, etc. Number of for loops to run the experiment on, i.e. to plot mean and average. Run the runners and record the output, record any other necessary information you feel could be useful for plotting into df_run_curves . Save df_run_curves as a csv file. Use information from df_run_curves to make the necessary plots. Feel free to innovate based on your requirements, the workflow above is just a guideline.","title":"4 - How to use Runners for Optimization Problems?"},{"location":"runners/#tutorial-how-to-use-runners","text":"Recommendation It is highly recommended that you use the Runners class for the second assignment on randomized optimizations.","title":"Tutorial - How to use Runners?"},{"location":"runners/#an-example-with-rhc-runner","text":"The below example illustrates how to initialize an RHCRunner object. import mlrose_ky # initialize the following variables problem = # An opt prob object from [[opt_probs]] seed = # a seed so experiments are reproducible iteration_list = 2 ** np . arange ( 10 ) max_attempts = 5000 restart_list = [ 25 , 75 , 100 ] rhc = mlrose_ky . RHCRunner ( problem = problem , experiment_name = \"exp_name\" , seed = seed iteration_list = iteration_list max_attempts = max_attempts , restart_list = restart_list ) The .run() method can be called on the rhc to return 2 dataframes that contain the results. df_run_stats , df_run_curves = rhc . run () The df_run_stats dataframe records the information based on the parameters in the restart list, i.e. 25 restarts, 75 restarts and 100 restarts. For the other runners, the df_run_stats follows the iteration list instead, i.e. df_run_stats has information based on list mentioned in iteration_list However, we use df_run_curves for plotting that has all the information, i.e. all restarts from 0 to 100. Recommendation Since the experiments take a lot of time to run, it is highly recommended that you save the df_run_curves output using the pandas.to_csv() method.> This saved df_run_curves returns a nice dataframe that can be used with plotting functions (which is a WIP in mlrose-ky ). The output has the following items: - Fitness score - FEvals (Fitness Evaluations) - Time (in seconds) - Other algorithm variables depending on Runner used, eg: Population Size and Keep Percent for MIMIC. Recommendation Simulated Annealing does not track decay types so it is best to record different decay types into df_run_curves before saving the runner.","title":"An example with RHC Runner"},{"location":"runners/#use-runners-to-make-your-own-custom-wrapper","text":"The best way to use runners would be to wrap and call the runners in your own function. One example workflow is below. Pass the following input to the function. Generate a problem using one of the problem generators . Pass in the seed , iteration_list , max_attempts , and restart_list Other parameters based on algorithm used, e.g. temperature and decay for Simulated Annealing, etc. Number of for loops to run the experiment on, i.e. to plot mean and average. Run the runners and record the output, record any other necessary information you feel could be useful for plotting into df_run_curves . Save df_run_curves as a csv file. Use information from df_run_curves to make the necessary plots. Feel free to innovate based on your requirements, the workflow above is just a guideline.","title":"Use runners to make your own custom wrapper"},{"location":"tutorial1/","text":"Tutorial - Getting Started # mlrose-ky provides functionality for implementing some of the most popular randomization and search algorithms, and applying them to a range of different optimization problem domains. In this tutorial, we will discuss what is meant by an optimization problem and step through an example of how mlrose-ky can be used to solve them. It is assumed that you have already installed mlrose-ky on your computer. If not, you can do so using the instructions provided here . What is an Optimization Problem? # An optimization problem is defined by Russell and Norvig (2010) as a problem in which \u201cthe aim is to find the best state according to an objective function.\u201d What is meant by a \u201cstate\u201d depends on the context of the problem. Some examples of states are: the weights used in a machine learning model, such as a neural network; the placement of chess pieces on a chess board; the order that cities are visited in a tour of all cities on a map of a country; the colors selected to color the countries in a map of the world. What is important, for our purposes, is that the state can be represented numerically, ideally as a one-dimensional array (or vector) of values. What is meant by \u201cbest\u201d is defined by a mathematical formula or function (known as an objective function, fitness function, cost function or loss function), which we want to either maximize or minimize. The function accepts a state array as an input and returns a \u201cfitness\u201d value as an output. The output fitness values allow us to compare the inputted state to other states we might be considering. In this context, the elements of the state array can be thought of as the variables (or parameters) of the function. Therefore, an optimization problem can be simply thought of as a mathematical function that we would like to maximize/minimize by selecting the optimal values for each of its parameters. Example The five-dimensional One-Max optimization problem involves finding the value of state vector \\(x = [x_{0}, x_{1}, x_{2}, x_{3}, x_{4}]\\) which maximizes \\(Fitness(x) = x_{0} + x_{1} + x_{2} + x_{3} + x_{4}\\) . If each of the elements of \\(x\\) can only take the values 0 or 1, then the solution to this problem is \\(x = [1, 1, 1, 1, 1]\\) . When \\(x\\) is set equal to this optimal value, \\(Fitness(x) = 5\\) , the maximum value it can take. Why use Randomized Optimization? # For the One-Max example given above, even if the solution was not immediately obvious, it would be possible to calculate the fitness value for all possible state vectors, \\(x\\) , and then select the best of those vectors. However, for more complicated problems, this cannot always be done within a reasonable period of time. Randomized optimization overcomes this issue. Randomized optimization algorithms typically start at an initial \u201cbest\u201d state vector (or population of multiple state vectors) and then randomly generate a new state vector (often a neighbor of the current \u201cbest\u201d state). If the new state is better than the current \u201cbest\u201d state, then the new vector becomes the new \u201cbest\u201d state vector. This process is repeated until it is no longer possible to find a better state vector than the current \u201cbest\u201d state, or if a better state vector cannot be found within a pre-specified number of attempts. There is no guarantee a randomized optimization algorithm will find the optimal solution to a given optimization problem (for example, it is possible that the algorithm may find a local maximum of the fitness function, instead of the global maximum). However, if a sufficiently large number of attempts are made to find a better state at each step of the algorithm, then the algorithm will return a \u201cgood\u201d solution to the problem. There is a trade-off between the time spent searching for the optimal solution to an optimization problem and the quality of the solution ultimately found. Solving Optimization Problems with mlrose-ky # Solving an optimization problem using mlrose-ky involves three simple steps: Define a fitness function object. Define an optimization problem object. Select and run a randomized optimization algorithm. To illustrate each of these steps, in the next few sections we will work through the example of the 8-Queens optimization problem, described below: Example: 8-Queens In chess, the queen is the most powerful piece on the board. It can attack any piece in the same row, column or diagonal. In the 8-Queens problem, you are given a chessboard with eight queens (and no other pieces) and the aim is to place the queens on the board so that none of them can attack each other (Russell and Norvig (2010). Clearly, in an optimal solution to this problem, there will be exactly one queen in each column. So, we only need to determine the row position of each queen, and we can define the state vector for this problem as \\(x = [x_{0}, x_{1}, x_{2}, x_{3}, x_{4}, x_{5}, x_{6}, x_{7}]\\) , where \\(x_{i}\\) denotes the row position of the queen in column i (for i = 0, 1, \u2026, 7). The chessboard pictured below could, therefore, be described by the state vector \\(x = [6, 1, 7, 5, 0, 2, 3, 4]\\) , where the bottom left corner of the chessboard is assumed to be in column 0 and row 0. This is not an optimal solution to the 8-Queens problem, since the three queens in columns 5, 6 and 7 are attacking each other diagonally, as are the queens in columns 2 and 6. Before starting with this example, you will need to import the mlrose-ky and Numpy Python packages. import mlrose_ky import numpy as np Define a Fitness Function Object # The first step in solving any optimization problem is to define the fitness function. This is the function we would ultimately like to maximize or minimize, and which can be used to evaluate the fitness of a given state vector, \\(x\\) . In the context of the 8-Queens problem, our goal is to find a state vector for which no pairs of attacking queens exist. Therefore, we could define our fitness function as evaluating the number of pairs of attacking queens for a given state and try to minimize this function. mlrose-ky includes pre-defined fitness function classes for a range of common optimization problems, including the N-Queens family of problems (of which 8-Queens is a member). A list of the pre-defined fitness functions can be found here . The pre-defined Queens() class includes an implementation of the (8-)Queens fitness function described above. We can initialize a fitness function object for this class, as follows: fitness = mlrose_ky . Queens () Alternatively, we could look at the 8-Queens problem as one where the aim is to find a state vector for which all pairs of queens do not attack each other. In this context, we could define our fitness function as evaluating the number of pairs of non-attacking queens for a given state and try to maximize this function. This definition of the 8-Queens fitness function is different from that used by mlrose\u2019s pre-defined Queens() class, so to use it, we will need to create a custom fitness function. This can be done by first defining a fitness function with a signature of the form fitness_fn(state, **kwargs) , and then using mlrose\u2019s CustomFitness() class to create a fitness function object, as follows: # Define alternative N-Queens fitness function for maximization problem def queens_max ( state ): # Initialize counter fitness_cnt = 0 # For all pairs of queens for i in range ( len ( state ) - 1 ): for j in range ( i + 1 , len ( state )): # Check for horizontal, diagonal-up and diagonal-down attacks if ( state [ j ] != state [ i ]) \\ and ( state [ j ] != state [ i ] + ( j - i )) \\ and ( state [ j ] != state [ i ] - ( j - i )): # If no attacks, then increment counter fitness_cnt += 1 return fitness_cnt # Initialize custom fitness function object fitness_cust = mlrose - ky . CustomFitness ( queens_max ) Define an Optimization Problem Object # Once we have created a fitness function object, we can use it as an input into an optimization problem object. In mlrose-ky, optimization problem objects are used to contain all of the important information about the optimization problem we are trying to solve. mlrose-ky provides classes for defining three types of optimization problem objects: DiscreteOpt() : This is used to describe discrete-state optimization problems. A discrete-state optimization problem is one where each element of the state vector can only take on a discrete set of values. In mlrose-ky, these values are assumed to be integers in the range 0 to (max_val - 1), where max_val is defined at initialization. ContinuousOpt() : This is used to describe continuous-state optimization problems. Continuous-state optimization problems are similar to discrete-state optimization problems, except that each value in the state vector can take any value in the continuous range between min_val and max_val, as specified at initialization. TSPOpt() : This is used to describe travelling salesperson (or tour) optimization problems. Travelling salesperson optimization problems differ from the previous two problem types in that, we know the elements of the optimal state vector are the integers 0 to (n - 1), where n is the length of the state vector, and our goal is to find the optimal ordering of those integers. We provide a worked example of this problem type here , so will not discuss it further for now. The 8-Queens problem is an example of a discrete-state optimization problem, since each of the elements of the state vector must take on an integer value in the range 0 to 7. To initialize a discrete-state optimization problem object, it is necessary to specify the problem length (i.e. the length of the state vector, which is 8 in this case); max_val, as defined above (also 8); the fitness function object created in the previous step; and whether the problem is a maximization or minimization problem. For this example, we will use the first of the two fitness function objects defined above, so we want to solve a minimization problem. problem = mlrose_ky . DiscreteOpt ( length = 8 , fitness_fn = fitness , maximize = False , max_val = 8 ) However, had we chosen to use the second (custom) fitness function object, we would be dealing with a maximization problem, so, in the above code, we would have to set the maximize parameter to True instead of False (in addition to changing the value of the fitness_fn parameter). Select and Run a Randomized Optimization Algorithm # Now that we have defined an optimization problem object, we are ready to solve our optimization problem. mlrose-ky includes implementations of the (random-restart) hill climbing, randomized hill climbing (also known as stochastic hill climbing), simulated annealing, genetic algorithm and MIMIC (Mutual-Information-Maximizing Input Clustering) randomized optimization algorithms (references to each of these algorithms can be found here ). For discrete-state and travelling salesperson optimization problems, we can choose any of these algorithms. However, continuous-state problems are not supported in the case of MIMIC. For our example, suppose we wish to use simulated annealing. To implement this algorithm, in addition to defining an optimization problem object, we must also define a schedule object (to specify how the simulated annealing temperature parameter changes over time); the number of attempts the algorithm should make to find a \u201cbetter\u201d state at each step (max_attempts); and the maximum number of iterations the algorithm should run for overall (max_iters). We can also specify the starting state for the algorithm, if desired (init_state). To specify the schedule object, mlrose-ky includes pre-defined decay schedule classes for geometric, arithmetic and expontential decay, as well as a class for defining your own decay schedule in a manner similar to the way in which we created a customized fitness function object. These classes are defined here . Suppose we wish to use an exponential decay schedule (with default parameter settings); make at most 10 attempts to find a \u201cbetter\u201d state at each algorithm step; limit ourselves to at most 1000 iterations of the algorithm; and start at an initial state of \\(x = [0, 1, 2, 3, 4, 5, 6, 7]\\) . This can be done using the following code. The algorithm returns the best state it can find, given the parameter values it has been provided, as well as the fitness value for that state. # Define decay schedule schedule = mlrose_ky . ExpDecay () # Define initial state init_state = np . array ([ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 ]) # Solve problem using simulated annealing best_state , best_fitness = mlrose_ky . simulated_annealing ( problem , schedule = schedule , max_attempts = 10 , max_iters = 1000 , init_state = init_state , random_state = 1 ) print ( best_state ) [ 6 4 7 3 6 2 5 1 ] print ( best_fitness ) 2.0 Running this code gives us a good solution to the 8-Queens problem, but not the optimal solution. The solution found by the algorithm, is pictured below: The solution state has a fitness value of 2, indicating there are still two pairs of attacking queens on the chessboard (the queens in columns 0 and 3; and the two queens in row 6). Ideally, we would like our solution to have a fitness value of 0. We can try to improve on our solution by tuning the parameters of our algorithm. Any of the algorithm\u2019s parameters can be tuned. However, in this case, let\u2019s focus on tuning the max_attempts parameter only, and increase it from 10 to 100. # Solve problem using simulated annealing best_state , best_fitness , _ = mlrose_ky . simulated_annealing ( problem , schedule = schedule , max_attempts = 100 , max_iters = 1000 , init_state = init_state , random_state = 1 ) print ( best_state ) [ 4 1 3 5 7 2 0 6 ] print ( best_fitness ) 0.0 This time when we run our code, we get a solution with a fitness value of 0, indicating that none of the queens on the chessboard are attacking each other. This can be verified below: Summary # In this tutorial we defined what is meant by an optimization problem and went through a simple example of how mlrose-ky can be used to solve them. This is all you need to solve the majority of optimization problems. However, there is one type of problem we have only briefly touched upon so far: the travelling salesperson optimization problem. In the next tutorial we will go through an example of how mlrose-ky can be used to solve this problem type. References # Brownlee, J (2011). Clever Algorithms: Nature-Inspired Programming Recipes . http://www.cleveralgorithms.com . De Bonet, J., C. Isbell, and P. Viola (1997). MIMIC: Finding Optima by Estimating Probability Densities. In Advances in Neural Information Processing Systems (NIPS) 9, pp. 424\u2013430. Russell, S. and P. Norvig (2010). Artificial Intelligence: A Modern Approach , 3rd edition. Prentice Hall, New Jersey, USA.","title":"1 - Getting Started"},{"location":"tutorial1/#tutorial-getting-started","text":"mlrose-ky provides functionality for implementing some of the most popular randomization and search algorithms, and applying them to a range of different optimization problem domains. In this tutorial, we will discuss what is meant by an optimization problem and step through an example of how mlrose-ky can be used to solve them. It is assumed that you have already installed mlrose-ky on your computer. If not, you can do so using the instructions provided here .","title":"Tutorial - Getting Started"},{"location":"tutorial1/#what-is-an-optimization-problem","text":"An optimization problem is defined by Russell and Norvig (2010) as a problem in which \u201cthe aim is to find the best state according to an objective function.\u201d What is meant by a \u201cstate\u201d depends on the context of the problem. Some examples of states are: the weights used in a machine learning model, such as a neural network; the placement of chess pieces on a chess board; the order that cities are visited in a tour of all cities on a map of a country; the colors selected to color the countries in a map of the world. What is important, for our purposes, is that the state can be represented numerically, ideally as a one-dimensional array (or vector) of values. What is meant by \u201cbest\u201d is defined by a mathematical formula or function (known as an objective function, fitness function, cost function or loss function), which we want to either maximize or minimize. The function accepts a state array as an input and returns a \u201cfitness\u201d value as an output. The output fitness values allow us to compare the inputted state to other states we might be considering. In this context, the elements of the state array can be thought of as the variables (or parameters) of the function. Therefore, an optimization problem can be simply thought of as a mathematical function that we would like to maximize/minimize by selecting the optimal values for each of its parameters. Example The five-dimensional One-Max optimization problem involves finding the value of state vector \\(x = [x_{0}, x_{1}, x_{2}, x_{3}, x_{4}]\\) which maximizes \\(Fitness(x) = x_{0} + x_{1} + x_{2} + x_{3} + x_{4}\\) . If each of the elements of \\(x\\) can only take the values 0 or 1, then the solution to this problem is \\(x = [1, 1, 1, 1, 1]\\) . When \\(x\\) is set equal to this optimal value, \\(Fitness(x) = 5\\) , the maximum value it can take.","title":"What is an Optimization Problem?"},{"location":"tutorial1/#why-use-randomized-optimization","text":"For the One-Max example given above, even if the solution was not immediately obvious, it would be possible to calculate the fitness value for all possible state vectors, \\(x\\) , and then select the best of those vectors. However, for more complicated problems, this cannot always be done within a reasonable period of time. Randomized optimization overcomes this issue. Randomized optimization algorithms typically start at an initial \u201cbest\u201d state vector (or population of multiple state vectors) and then randomly generate a new state vector (often a neighbor of the current \u201cbest\u201d state). If the new state is better than the current \u201cbest\u201d state, then the new vector becomes the new \u201cbest\u201d state vector. This process is repeated until it is no longer possible to find a better state vector than the current \u201cbest\u201d state, or if a better state vector cannot be found within a pre-specified number of attempts. There is no guarantee a randomized optimization algorithm will find the optimal solution to a given optimization problem (for example, it is possible that the algorithm may find a local maximum of the fitness function, instead of the global maximum). However, if a sufficiently large number of attempts are made to find a better state at each step of the algorithm, then the algorithm will return a \u201cgood\u201d solution to the problem. There is a trade-off between the time spent searching for the optimal solution to an optimization problem and the quality of the solution ultimately found.","title":"Why use Randomized Optimization?"},{"location":"tutorial1/#solving-optimization-problems-with-mlrose-ky","text":"Solving an optimization problem using mlrose-ky involves three simple steps: Define a fitness function object. Define an optimization problem object. Select and run a randomized optimization algorithm. To illustrate each of these steps, in the next few sections we will work through the example of the 8-Queens optimization problem, described below: Example: 8-Queens In chess, the queen is the most powerful piece on the board. It can attack any piece in the same row, column or diagonal. In the 8-Queens problem, you are given a chessboard with eight queens (and no other pieces) and the aim is to place the queens on the board so that none of them can attack each other (Russell and Norvig (2010). Clearly, in an optimal solution to this problem, there will be exactly one queen in each column. So, we only need to determine the row position of each queen, and we can define the state vector for this problem as \\(x = [x_{0}, x_{1}, x_{2}, x_{3}, x_{4}, x_{5}, x_{6}, x_{7}]\\) , where \\(x_{i}\\) denotes the row position of the queen in column i (for i = 0, 1, \u2026, 7). The chessboard pictured below could, therefore, be described by the state vector \\(x = [6, 1, 7, 5, 0, 2, 3, 4]\\) , where the bottom left corner of the chessboard is assumed to be in column 0 and row 0. This is not an optimal solution to the 8-Queens problem, since the three queens in columns 5, 6 and 7 are attacking each other diagonally, as are the queens in columns 2 and 6. Before starting with this example, you will need to import the mlrose-ky and Numpy Python packages. import mlrose_ky import numpy as np","title":"Solving Optimization Problems with mlrose-ky"},{"location":"tutorial1/#define-a-fitness-function-object","text":"The first step in solving any optimization problem is to define the fitness function. This is the function we would ultimately like to maximize or minimize, and which can be used to evaluate the fitness of a given state vector, \\(x\\) . In the context of the 8-Queens problem, our goal is to find a state vector for which no pairs of attacking queens exist. Therefore, we could define our fitness function as evaluating the number of pairs of attacking queens for a given state and try to minimize this function. mlrose-ky includes pre-defined fitness function classes for a range of common optimization problems, including the N-Queens family of problems (of which 8-Queens is a member). A list of the pre-defined fitness functions can be found here . The pre-defined Queens() class includes an implementation of the (8-)Queens fitness function described above. We can initialize a fitness function object for this class, as follows: fitness = mlrose_ky . Queens () Alternatively, we could look at the 8-Queens problem as one where the aim is to find a state vector for which all pairs of queens do not attack each other. In this context, we could define our fitness function as evaluating the number of pairs of non-attacking queens for a given state and try to maximize this function. This definition of the 8-Queens fitness function is different from that used by mlrose\u2019s pre-defined Queens() class, so to use it, we will need to create a custom fitness function. This can be done by first defining a fitness function with a signature of the form fitness_fn(state, **kwargs) , and then using mlrose\u2019s CustomFitness() class to create a fitness function object, as follows: # Define alternative N-Queens fitness function for maximization problem def queens_max ( state ): # Initialize counter fitness_cnt = 0 # For all pairs of queens for i in range ( len ( state ) - 1 ): for j in range ( i + 1 , len ( state )): # Check for horizontal, diagonal-up and diagonal-down attacks if ( state [ j ] != state [ i ]) \\ and ( state [ j ] != state [ i ] + ( j - i )) \\ and ( state [ j ] != state [ i ] - ( j - i )): # If no attacks, then increment counter fitness_cnt += 1 return fitness_cnt # Initialize custom fitness function object fitness_cust = mlrose - ky . CustomFitness ( queens_max )","title":"Define a Fitness Function Object"},{"location":"tutorial1/#define-an-optimization-problem-object","text":"Once we have created a fitness function object, we can use it as an input into an optimization problem object. In mlrose-ky, optimization problem objects are used to contain all of the important information about the optimization problem we are trying to solve. mlrose-ky provides classes for defining three types of optimization problem objects: DiscreteOpt() : This is used to describe discrete-state optimization problems. A discrete-state optimization problem is one where each element of the state vector can only take on a discrete set of values. In mlrose-ky, these values are assumed to be integers in the range 0 to (max_val - 1), where max_val is defined at initialization. ContinuousOpt() : This is used to describe continuous-state optimization problems. Continuous-state optimization problems are similar to discrete-state optimization problems, except that each value in the state vector can take any value in the continuous range between min_val and max_val, as specified at initialization. TSPOpt() : This is used to describe travelling salesperson (or tour) optimization problems. Travelling salesperson optimization problems differ from the previous two problem types in that, we know the elements of the optimal state vector are the integers 0 to (n - 1), where n is the length of the state vector, and our goal is to find the optimal ordering of those integers. We provide a worked example of this problem type here , so will not discuss it further for now. The 8-Queens problem is an example of a discrete-state optimization problem, since each of the elements of the state vector must take on an integer value in the range 0 to 7. To initialize a discrete-state optimization problem object, it is necessary to specify the problem length (i.e. the length of the state vector, which is 8 in this case); max_val, as defined above (also 8); the fitness function object created in the previous step; and whether the problem is a maximization or minimization problem. For this example, we will use the first of the two fitness function objects defined above, so we want to solve a minimization problem. problem = mlrose_ky . DiscreteOpt ( length = 8 , fitness_fn = fitness , maximize = False , max_val = 8 ) However, had we chosen to use the second (custom) fitness function object, we would be dealing with a maximization problem, so, in the above code, we would have to set the maximize parameter to True instead of False (in addition to changing the value of the fitness_fn parameter).","title":"Define an Optimization Problem Object"},{"location":"tutorial1/#select-and-run-a-randomized-optimization-algorithm","text":"Now that we have defined an optimization problem object, we are ready to solve our optimization problem. mlrose-ky includes implementations of the (random-restart) hill climbing, randomized hill climbing (also known as stochastic hill climbing), simulated annealing, genetic algorithm and MIMIC (Mutual-Information-Maximizing Input Clustering) randomized optimization algorithms (references to each of these algorithms can be found here ). For discrete-state and travelling salesperson optimization problems, we can choose any of these algorithms. However, continuous-state problems are not supported in the case of MIMIC. For our example, suppose we wish to use simulated annealing. To implement this algorithm, in addition to defining an optimization problem object, we must also define a schedule object (to specify how the simulated annealing temperature parameter changes over time); the number of attempts the algorithm should make to find a \u201cbetter\u201d state at each step (max_attempts); and the maximum number of iterations the algorithm should run for overall (max_iters). We can also specify the starting state for the algorithm, if desired (init_state). To specify the schedule object, mlrose-ky includes pre-defined decay schedule classes for geometric, arithmetic and expontential decay, as well as a class for defining your own decay schedule in a manner similar to the way in which we created a customized fitness function object. These classes are defined here . Suppose we wish to use an exponential decay schedule (with default parameter settings); make at most 10 attempts to find a \u201cbetter\u201d state at each algorithm step; limit ourselves to at most 1000 iterations of the algorithm; and start at an initial state of \\(x = [0, 1, 2, 3, 4, 5, 6, 7]\\) . This can be done using the following code. The algorithm returns the best state it can find, given the parameter values it has been provided, as well as the fitness value for that state. # Define decay schedule schedule = mlrose_ky . ExpDecay () # Define initial state init_state = np . array ([ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 ]) # Solve problem using simulated annealing best_state , best_fitness = mlrose_ky . simulated_annealing ( problem , schedule = schedule , max_attempts = 10 , max_iters = 1000 , init_state = init_state , random_state = 1 ) print ( best_state ) [ 6 4 7 3 6 2 5 1 ] print ( best_fitness ) 2.0 Running this code gives us a good solution to the 8-Queens problem, but not the optimal solution. The solution found by the algorithm, is pictured below: The solution state has a fitness value of 2, indicating there are still two pairs of attacking queens on the chessboard (the queens in columns 0 and 3; and the two queens in row 6). Ideally, we would like our solution to have a fitness value of 0. We can try to improve on our solution by tuning the parameters of our algorithm. Any of the algorithm\u2019s parameters can be tuned. However, in this case, let\u2019s focus on tuning the max_attempts parameter only, and increase it from 10 to 100. # Solve problem using simulated annealing best_state , best_fitness , _ = mlrose_ky . simulated_annealing ( problem , schedule = schedule , max_attempts = 100 , max_iters = 1000 , init_state = init_state , random_state = 1 ) print ( best_state ) [ 4 1 3 5 7 2 0 6 ] print ( best_fitness ) 0.0 This time when we run our code, we get a solution with a fitness value of 0, indicating that none of the queens on the chessboard are attacking each other. This can be verified below:","title":"Select and Run a Randomized Optimization Algorithm"},{"location":"tutorial1/#summary","text":"In this tutorial we defined what is meant by an optimization problem and went through a simple example of how mlrose-ky can be used to solve them. This is all you need to solve the majority of optimization problems. However, there is one type of problem we have only briefly touched upon so far: the travelling salesperson optimization problem. In the next tutorial we will go through an example of how mlrose-ky can be used to solve this problem type.","title":"Summary"},{"location":"tutorial1/#references","text":"Brownlee, J (2011). Clever Algorithms: Nature-Inspired Programming Recipes . http://www.cleveralgorithms.com . De Bonet, J., C. Isbell, and P. Viola (1997). MIMIC: Finding Optima by Estimating Probability Densities. In Advances in Neural Information Processing Systems (NIPS) 9, pp. 424\u2013430. Russell, S. and P. Norvig (2010). Artificial Intelligence: A Modern Approach , 3rd edition. Prentice Hall, New Jersey, USA.","title":"References"},{"location":"tutorial2/","text":"Tutorial - Travelling Saleperson Problems # What is a Travelling Salesperson Problem? # The travelling salesperson problem (TSP) is a classic optimization problem where the goal is to determine the shortest tour of a collection of n \u201ccities\u201d (i.e. nodes), starting and ending in the same city and visiting all of the other cities exactly once. In such a situation, a solution can be represented by a vector of n integers, each in the range 0 to n-1, specifying the order in which the cities should be visited. TSP is an NP-hard problem, meaning that, for larger values of n, it is not feasible to evaluate every possible problem solution within a reasonable period of time. Consequently, TSPs are well suited to solving using randomized optimization algorithms. Example Consider the following map containing 8 cities, numbered 0 to 7. A salesperson would like to travel to each of these cities, starting and ending in the same city and visiting each of the other cities exactly once. One possible tour of the cities is illustrated below, and could be represented by the solution vector \\(x = [0, 4, 2, 6, 5, 3, 7, 1]\\) (assuming the tour starts and ends at City 0). However, this is not the shortest tour of these cities. The aim of this problem is to find the shortest tour of the 8 cities. Solving TSPs with mlrose-ky # Given the solution to the TSP can be represented by a vector of integers in the range 0 to n-1, we could define a discrete-state optimization problem object and use one of mlrose\u2019s randomized optimization algorithms to solve it, as we did for the 8-Queens problem in the previous tutorial. However, by defining the problem this way, we would end up potentially considering invalid \u201csolutions\u201d, which involve us visiting some cities more than once and some not at all. An alternative is to define an optimization problem object that only allows us to consider valid tours of the n cities as potential solutions. This is a much more efficient approach to solving TSPs and can be implemented in mlrose_ky using the TSPOpt() optimization problem class. In this tutorial, we will use this alternative approach to solve the TSP example given above. The steps required to solve this problem are the same as those used to solve any optimization problem in mlrose-ky. Specificially: Define a fitness function object. Define an optimization problem object. Select and run a randomized optimization algorithm. Before starting with the example, you will need to import the mlrose-ky and Numpy Python packages. import mlrose_ky import numpy as np Define a Fitness Function Object # For the TSP in the example, the goal is to find the shortest tour of the eight cities. As a result, the fitness function should calculate the total length of a given tour. This is the fitness definition used in mlrose\u2019s pre-defined TravellingSales() class. The TSPOpt() optimization problem class assumes, by default, that the TravellingSales() class is used to define the fitness function for a TSP. As a result, if the TravellingSales() class is to be used to define the fitness function object, then this step can be skipped. However, it is also possible to manually define the fitness function object, if so desired. To initialize a fitness function object for the TravellingSales() class, it is necessary to specify either the (x, y) coordinates of all the cities or the distances between each pair of cities for which travel is possible. If the former is specified, then it is assumed that travel between each pair of cities is possible. If we choose to specify the coordinates, then these should be input as an ordered list of pairs (where pair i specifies the coordinates of city i), as follows: # Create list of city coordinates coords_list = [( 1 , 1 ), ( 4 , 2 ), ( 5 , 2 ), ( 6 , 4 ), ( 4 , 4 ), ( 3 , 6 ), ( 1 , 5 ), ( 2 , 3 )] # Initialize fitness function object using coords_list fitness_coords = mlrose_ky . TravellingSales ( coords = coords_list ) Alternatively, if we choose to specity the distances, then these should be input as a list of triples giving the distances, d, between all pairs of cities, u and v, for which travel is possible, with each triple in the form (u, v, d). The order in which the cities is specified does not matter (i.e., the distance between cities 1 and 2 is assumed to be the same as the distance between cities 2 and 1), and so each pair of cities need only be included in the list once. Using the distance approach, the fitness function object can be initialize as follows: Create list of distances between pairs of cities dist_list = [( 0 , 1 , 3.1623 ), ( 0 , 2 , 4.1231 ), ( 0 , 3 , 5.8310 ), ( 0 , 4 , 4.2426 ), \\ ( 0 , 5 , 5.3852 ), ( 0 , 6 , 4.0000 ), ( 0 , 7 , 2.2361 ), ( 1 , 2 , 1.0000 ), \\ ( 1 , 3 , 2.8284 ), ( 1 , 4 , 2.0000 ), ( 1 , 5 , 4.1231 ), ( 1 , 6 , 4.2426 ), \\ ( 1 , 7 , 2.2361 ), ( 2 , 3 , 2.2361 ), ( 2 , 4 , 2.2361 ), ( 2 , 5 , 4.4721 ), \\ ( 2 , 6 , 5.0000 ), ( 2 , 7 , 3.1623 ), ( 3 , 4 , 2.0000 ), ( 3 , 5 , 3.6056 ), \\ ( 3 , 6 , 5.0990 ), ( 3 , 7 , 4.1231 ), ( 4 , 5 , 2.2361 ), ( 4 , 6 , 3.1623 ), \\ ( 4 , 7 , 2.2361 ), ( 5 , 6 , 2.2361 ), ( 5 , 7 , 3.1623 ), ( 6 , 7 , 2.2361 )] # Initialize fitness function object using dist_list fitness_dists = mlrose_ky . TravellingSales ( distances = dist_list ) If both a list of coordinates and a list of distances are specified in initializing the fitness function object, then the distance list will be ignored. Define an Optimization Problem Object # As mentioned previously, the most efficient approach to solving a TSP in mlrose-ky is to define the optimization problem object using the TSPOpt() optimization problem class. If a fitness function has already been manually defined, as demonstrated in the previous step, then the only additional information required to initialize a TSPOpt() object are the length of the problem (i.e. the number of cities to be visited on the tour) and whether our problem is a maximization or a minimization problem. In our example, we want to solve a minimization problem of length 8. If we use the fitness_coords fitness function defined above, we can define an optimization problem object as follows: # Define optimization problem object problem_fit = mlrose_ky . TSPOpt ( length = 8 , fitness_fn = fitness_coords , maximize = False ) Alternatively, if we had not previously defined a fitness function (and we wish to use the TravellingSales() class to define the fitness function), then this can be done as part of the optimization problem object initialization step by specifying either a list of coordinates or a list of distances, instead of a fitness function object, similar to what was done when manually initializing the fitness function object. In the case of our example, if we choose to specify a list of coordinates, in place of a fitness function object, we can initialize our optimization problem object as: # Create list of city coordinates coords_list = [( 1 , 1 ), ( 4 , 2 ), ( 5 , 2 ), ( 6 , 4 ), ( 4 , 4 ), ( 3 , 6 ), ( 1 , 5 ), ( 2 , 3 )] # Define optimization problem object problem_no_fit = mlrose_ky . TSPOpt ( length = 8 , coords = coords_list , maximize = False ) As with manually defining the fitness function object, if both a list of coordinates and a list of distances are specified in initializing the optimization problem object, then the distance list will be ignored. Furthermore, if a fitness function object is specified in addition to a list of coordinates and/or a list of distances, then the list of coordinates/distances will be ignored. Select and Run a Randomized Optimization Algorithm # Once the optimization object is defined, all that is left to do is to select a randomized optimization algorithm and use it to solve our problem. This time, suppose we wish to use the genetic algorithms with the default parameter settings of a population size (pop_size) of 200, a mutation probability (mutation_prob) of 0.1, a maximum of 10 attempts per step (max_attempts) and no limit on the maximum total number of iteration of the algorithm (max_iters). This returns the following solution: # Solve problem using the genetic algorithm best_state , best_fitness = mlrose_ky . genetic_alg ( problem_fit , random_state = 2 ) print ( best_state ) [ 1 3 4 5 6 7 0 2 ] print ( best_fitness ) 18.8958046604 The solution tour found by the algorithm is pictured below and has a total length of 18.896 units. As in the 8-Queens example given in the previous tutorial, this solution can potentially be improved on by tuning the parameters of the optimization algorithm. For example, increasing the maximum number of attempts per step to 100 and increasing the mutation probability to 0.2, yields a tour with a total length of 17.343 units. # Solve problem using the genetic algorithm best_state , best_fitness = mlrose_ky . genetic_alg ( problem_fit , mutation_prob = 0.2 , max_attempts = 100 , random_state = 2 ) print ( best_state ) [ 7 6 5 4 3 2 1 0 ] print ( best_fitness ) 17.3426175477 This solution is illustrated below and can be shown to be the optimal solution to this problem. Summary # In this tutorial we introduced the travelling salesperson problem, and discussed how mlrose-ky can be used to efficiently solve this problem. This is an example of how mlrose-ky caters to solving one very specific type of optimization problem. Another very specific type of optimization problem mlrose-ky caters to solving is the machine learning weight optimization problem. That is, the problem of finding the optimal weights for machine learning models such as neural networks and regression models. We will discuss how mlrose-ky can be used to solve this problem next, in our third and final tutorial.","title":"2 - What is a Travelling Salesman Problem?"},{"location":"tutorial2/#tutorial-travelling-saleperson-problems","text":"","title":"Tutorial - Travelling Saleperson Problems"},{"location":"tutorial2/#what-is-a-travelling-salesperson-problem","text":"The travelling salesperson problem (TSP) is a classic optimization problem where the goal is to determine the shortest tour of a collection of n \u201ccities\u201d (i.e. nodes), starting and ending in the same city and visiting all of the other cities exactly once. In such a situation, a solution can be represented by a vector of n integers, each in the range 0 to n-1, specifying the order in which the cities should be visited. TSP is an NP-hard problem, meaning that, for larger values of n, it is not feasible to evaluate every possible problem solution within a reasonable period of time. Consequently, TSPs are well suited to solving using randomized optimization algorithms. Example Consider the following map containing 8 cities, numbered 0 to 7. A salesperson would like to travel to each of these cities, starting and ending in the same city and visiting each of the other cities exactly once. One possible tour of the cities is illustrated below, and could be represented by the solution vector \\(x = [0, 4, 2, 6, 5, 3, 7, 1]\\) (assuming the tour starts and ends at City 0). However, this is not the shortest tour of these cities. The aim of this problem is to find the shortest tour of the 8 cities.","title":"What is a Travelling Salesperson Problem?"},{"location":"tutorial2/#solving-tsps-with-mlrose-ky","text":"Given the solution to the TSP can be represented by a vector of integers in the range 0 to n-1, we could define a discrete-state optimization problem object and use one of mlrose\u2019s randomized optimization algorithms to solve it, as we did for the 8-Queens problem in the previous tutorial. However, by defining the problem this way, we would end up potentially considering invalid \u201csolutions\u201d, which involve us visiting some cities more than once and some not at all. An alternative is to define an optimization problem object that only allows us to consider valid tours of the n cities as potential solutions. This is a much more efficient approach to solving TSPs and can be implemented in mlrose_ky using the TSPOpt() optimization problem class. In this tutorial, we will use this alternative approach to solve the TSP example given above. The steps required to solve this problem are the same as those used to solve any optimization problem in mlrose-ky. Specificially: Define a fitness function object. Define an optimization problem object. Select and run a randomized optimization algorithm. Before starting with the example, you will need to import the mlrose-ky and Numpy Python packages. import mlrose_ky import numpy as np","title":"Solving TSPs with mlrose-ky"},{"location":"tutorial2/#define-a-fitness-function-object","text":"For the TSP in the example, the goal is to find the shortest tour of the eight cities. As a result, the fitness function should calculate the total length of a given tour. This is the fitness definition used in mlrose\u2019s pre-defined TravellingSales() class. The TSPOpt() optimization problem class assumes, by default, that the TravellingSales() class is used to define the fitness function for a TSP. As a result, if the TravellingSales() class is to be used to define the fitness function object, then this step can be skipped. However, it is also possible to manually define the fitness function object, if so desired. To initialize a fitness function object for the TravellingSales() class, it is necessary to specify either the (x, y) coordinates of all the cities or the distances between each pair of cities for which travel is possible. If the former is specified, then it is assumed that travel between each pair of cities is possible. If we choose to specify the coordinates, then these should be input as an ordered list of pairs (where pair i specifies the coordinates of city i), as follows: # Create list of city coordinates coords_list = [( 1 , 1 ), ( 4 , 2 ), ( 5 , 2 ), ( 6 , 4 ), ( 4 , 4 ), ( 3 , 6 ), ( 1 , 5 ), ( 2 , 3 )] # Initialize fitness function object using coords_list fitness_coords = mlrose_ky . TravellingSales ( coords = coords_list ) Alternatively, if we choose to specity the distances, then these should be input as a list of triples giving the distances, d, between all pairs of cities, u and v, for which travel is possible, with each triple in the form (u, v, d). The order in which the cities is specified does not matter (i.e., the distance between cities 1 and 2 is assumed to be the same as the distance between cities 2 and 1), and so each pair of cities need only be included in the list once. Using the distance approach, the fitness function object can be initialize as follows: Create list of distances between pairs of cities dist_list = [( 0 , 1 , 3.1623 ), ( 0 , 2 , 4.1231 ), ( 0 , 3 , 5.8310 ), ( 0 , 4 , 4.2426 ), \\ ( 0 , 5 , 5.3852 ), ( 0 , 6 , 4.0000 ), ( 0 , 7 , 2.2361 ), ( 1 , 2 , 1.0000 ), \\ ( 1 , 3 , 2.8284 ), ( 1 , 4 , 2.0000 ), ( 1 , 5 , 4.1231 ), ( 1 , 6 , 4.2426 ), \\ ( 1 , 7 , 2.2361 ), ( 2 , 3 , 2.2361 ), ( 2 , 4 , 2.2361 ), ( 2 , 5 , 4.4721 ), \\ ( 2 , 6 , 5.0000 ), ( 2 , 7 , 3.1623 ), ( 3 , 4 , 2.0000 ), ( 3 , 5 , 3.6056 ), \\ ( 3 , 6 , 5.0990 ), ( 3 , 7 , 4.1231 ), ( 4 , 5 , 2.2361 ), ( 4 , 6 , 3.1623 ), \\ ( 4 , 7 , 2.2361 ), ( 5 , 6 , 2.2361 ), ( 5 , 7 , 3.1623 ), ( 6 , 7 , 2.2361 )] # Initialize fitness function object using dist_list fitness_dists = mlrose_ky . TravellingSales ( distances = dist_list ) If both a list of coordinates and a list of distances are specified in initializing the fitness function object, then the distance list will be ignored.","title":"Define a Fitness Function Object"},{"location":"tutorial2/#define-an-optimization-problem-object","text":"As mentioned previously, the most efficient approach to solving a TSP in mlrose-ky is to define the optimization problem object using the TSPOpt() optimization problem class. If a fitness function has already been manually defined, as demonstrated in the previous step, then the only additional information required to initialize a TSPOpt() object are the length of the problem (i.e. the number of cities to be visited on the tour) and whether our problem is a maximization or a minimization problem. In our example, we want to solve a minimization problem of length 8. If we use the fitness_coords fitness function defined above, we can define an optimization problem object as follows: # Define optimization problem object problem_fit = mlrose_ky . TSPOpt ( length = 8 , fitness_fn = fitness_coords , maximize = False ) Alternatively, if we had not previously defined a fitness function (and we wish to use the TravellingSales() class to define the fitness function), then this can be done as part of the optimization problem object initialization step by specifying either a list of coordinates or a list of distances, instead of a fitness function object, similar to what was done when manually initializing the fitness function object. In the case of our example, if we choose to specify a list of coordinates, in place of a fitness function object, we can initialize our optimization problem object as: # Create list of city coordinates coords_list = [( 1 , 1 ), ( 4 , 2 ), ( 5 , 2 ), ( 6 , 4 ), ( 4 , 4 ), ( 3 , 6 ), ( 1 , 5 ), ( 2 , 3 )] # Define optimization problem object problem_no_fit = mlrose_ky . TSPOpt ( length = 8 , coords = coords_list , maximize = False ) As with manually defining the fitness function object, if both a list of coordinates and a list of distances are specified in initializing the optimization problem object, then the distance list will be ignored. Furthermore, if a fitness function object is specified in addition to a list of coordinates and/or a list of distances, then the list of coordinates/distances will be ignored.","title":"Define an Optimization Problem Object"},{"location":"tutorial2/#select-and-run-a-randomized-optimization-algorithm","text":"Once the optimization object is defined, all that is left to do is to select a randomized optimization algorithm and use it to solve our problem. This time, suppose we wish to use the genetic algorithms with the default parameter settings of a population size (pop_size) of 200, a mutation probability (mutation_prob) of 0.1, a maximum of 10 attempts per step (max_attempts) and no limit on the maximum total number of iteration of the algorithm (max_iters). This returns the following solution: # Solve problem using the genetic algorithm best_state , best_fitness = mlrose_ky . genetic_alg ( problem_fit , random_state = 2 ) print ( best_state ) [ 1 3 4 5 6 7 0 2 ] print ( best_fitness ) 18.8958046604 The solution tour found by the algorithm is pictured below and has a total length of 18.896 units. As in the 8-Queens example given in the previous tutorial, this solution can potentially be improved on by tuning the parameters of the optimization algorithm. For example, increasing the maximum number of attempts per step to 100 and increasing the mutation probability to 0.2, yields a tour with a total length of 17.343 units. # Solve problem using the genetic algorithm best_state , best_fitness = mlrose_ky . genetic_alg ( problem_fit , mutation_prob = 0.2 , max_attempts = 100 , random_state = 2 ) print ( best_state ) [ 7 6 5 4 3 2 1 0 ] print ( best_fitness ) 17.3426175477 This solution is illustrated below and can be shown to be the optimal solution to this problem.","title":"Select and Run a Randomized Optimization Algorithm"},{"location":"tutorial2/#summary","text":"In this tutorial we introduced the travelling salesperson problem, and discussed how mlrose-ky can be used to efficiently solve this problem. This is an example of how mlrose-ky caters to solving one very specific type of optimization problem. Another very specific type of optimization problem mlrose-ky caters to solving is the machine learning weight optimization problem. That is, the problem of finding the optimal weights for machine learning models such as neural networks and regression models. We will discuss how mlrose-ky can be used to solve this problem next, in our third and final tutorial.","title":"Summary"},{"location":"tutorial3/","text":"Tutorial - Machine Learning Weight Optimization Problems # What is a Machine Learning Weight Optimization Problem? # For a number of different machine learning models, the process of fitting the model parameters involves finding the parameter values that minimize a pre-specified loss function for a given training dataset. Examples of such models include neural networks, linear regression models and logistic regression models, and the optimal model weights for such models are typically found using methods such as gradient descent. However, the problem of fitting the parameters (or weights) of a machine learning model can also be viewed as a continuous-state optimization problem, where the loss function takes the role of the fitness function, and the goal is to minimize this function. By framing the problem this way, we can use any of the randomized optimization algorithms that are suited to continuous-state optimization problems to fit the model parameters. In this tutorial, we will work through an example of how this can be done with mlrose-ky. Solving Machine Learning Weight Optimization Problems with mlrose-ky # mlrose-ky contains built-in functionality for solving the weight optimization problem for three types of machine learning models: (standard) neural networks, linear regression models and logistic regression models. This is done using the NeuralNetwork() , LinearRegression() and LogisticRegression() classes respectively. Each of these classes includes a fit method, which implements the three steps for solving an optimization problem defined in the previous tutorials, for a given training dataset. However, when fitting a machine learning model, finding the optimal model weights is merely a means to an end. We want to find the optimal model weights so that we can use our fitted model to predict the labels of future observations as accurately as possible, not because we are actually interested in knowing the optimal weight values. As a result, the abovementioned classes also include a predict method, which, if called after the fit method, will predict the labels for a given test dataset using the fitted model. The steps involved in solving a machine learning weight optimization problem with mlrose-ky are typically: Initialize a machine learning weight optimization problem object. Find the optimal model weights for a given training dataset by calling the fit method of the object initialized in step 1. Predict the labels for a test dataset by calling the predict method of the object initialized in step 1. To fit the model weights, the user can choose between using either randomized hill climbing, simulated annealing, the genetic algorithm or gradient descent. In mlrose-ky, the gradient descent algorithm is only available for use in solving the machine learning weight optimization problem and has been included primarily for benchmarking purposes, since this is one of the most common algorithm used in fitting neural networks and regression models. We will now work through an example to illustrate how mlrose-ky can be used to fit a neural network and a regression model to a given dataset. Example: the Iris Dataset # The Iris dataset is a famous multivariate classification dataset first presented in a 1936 research paper by statistician and biologist Ronald Fisher. It contains 150 observations of three classes (species) of iris flowers (50 observations of each class), with each observation providing the sepal length, sepal width, petal length and petal width (i.e. the feature values), as well as the class label (i.e. the target value), of each flower under consideration. The Iris dataset is included with the Python sklearn package. The feature values and label of the first observation in the dataset are shown below, along with the maximum and minimum values of each of the features and the unique label values: import numpy as np from sklearn.datasets import load_iris # Load the Iris dataset data = load_iris () # Get feature values print ( data . data [ 0 ]) [ 5.1 3.5 1.4 0.2 ] # Get feature names print ( data . feature_names ) [ 'sepal length (cm)' , 'sepal width (cm)' , 'petal length (cm)' , 'petal width (cm)' ] # Get target value of first observation print ( data . target [ 0 ]) 0 # Get target name of first observation print ( data . target_names [ data . target [ 0 ]]) setosa # Get minimum feature values print ( np . min ( data . data , axis = 0 )) [ 4.3 2. 1. 0.1 ] # Get maximum feature values print ( np . max ( data . data , axis = 0 )) [ 7.9 4.4 6.9 2.5 ] # Get unique target values print ( np . unique ( data . target )) [ 0 1 2 ] From this we can see that all features in the Iris data set are numeric, albeit with different ranges, and that the class labels have been represented by integers. In the next few sections we will show how mlrose-ky can be used to fit a neural network and a logistic regression model to this dataset, to predict the species of an iris flower given its feature values. Data Pre-Processing # Before we can fit any sort of machine learning model to a dataset, it is necessary to manipulate our data into the form expected by mlrose-ky. Each of the three machine learning models supported by mlrose-ky expect to receive feature data in the form of a numpy array, with one row per observation and numeric features only (any categorical features must be one-hot encoded before passing to the machine learning models). The models also expect to receive the target values as either: a list of numeric values (for regression data); a list of 0-1 indicator values (for binary classification data); or as a numpy array of one-hot encoded labels, with one row per observation (for multi-class classification data). In the case of the Iris dataset, all of our features are numeric, so no one-hot encoding is required. However, it is necessary to one-hot encode the class labels. In keeping with standard machine learning practice, it is also necessary to split the data into training and test subsets, and since the range of the Iris data varies considerably from feature to feature, to standardize the values of our feature variables. These pre-processing steps are implemented below. from sklearn.model_selection import train_test_split from sklearn.preprocessing import MinMaxScaler , OneHotEncoder # Split data into training and test sets X_train , X_test , y_train , y_test = train_test_split ( data . data , data . target , \\ test_size = 0.2 , random_state = 3 ) # Normalize feature data scaler = MinMaxScaler () X_train_scaled = scaler . fit_transform ( X_train ) X_test_scaled = scaler . transform ( X_test ) # One hot encode target values one_hot = OneHotEncoder () y_train_hot = one_hot . fit_transform ( y_train . reshape ( - 1 , 1 )) . todense () y_test_hot = one_hot . transform ( y_test . reshape ( - 1 , 1 )) . todense () Neural Networks # Once the data has been preprocessed, fitting a neural network in mlrose-ky simply involves following the steps listed above. Suppose we wish to fit a neural network classifier to our Iris dataset with one hidden layer containing 2 nodes and a ReLU activation function (mlrose-ky supports the ReLU, identity, sigmoid and tanh activation functions). For this example, we will use the Randomized Hill Climbing algorithm to find the optimal weights, with a maximum of 1000 iterations of the algorithm and 100 attempts to find a better set of weights at each step. We will also include a bias term; use a step size (learning rate) of 0.0001; and limit our weights to being in the range -5 to 5 (to reduce the landscape over which the algorithm must search in order to find the optimal weights). This model is initialized and fitted to our preprocessed data below: # Initialize neural network object and fit object nn_model1 = mlrose_ky . NeuralNetwork ( hidden_nodes = [ 2 ], activation = 'relu' , \\ algorithm = 'random_hill_climb' , max_iters = 1000 , \\ bias = True , is_classifier = True , learning_rate = 0.0001 , \\ early_stopping = True , clip_max = 5 , max_attempts = 100 , \\ random_state = 3 ) nn_model1 . fit ( X_train_scaled , y_train_hot ) Once the model is fitted, we can use it to predict the labels for our training and test datasets and use these prediction to assess the model\u2019s training and test accuracy. from sklearn.metrics import accuracy_score # Predict labels for train set and assess accuracy y_train_pred = nn_model1 . predict ( X_train_scaled ) y_train_accuracy = accuracy_score ( y_train_hot , y_train_pred ) print ( y_train_accuracy ) 0.45 # Predict labels for test set and assess accuracy y_test_pred = nn_model1 . predict ( X_test_scaled ) y_test_accuracy = accuracy_score ( y_test_hot , y_test_pred ) print ( y_test_accuracy ) 0.533333333333 In this case, our model achieves training accuracy of 45% and test accuracy of 53.3%. These accuracy levels are better than if the labels were selected at random, but still leave room for improvement. We can potentially improve on the accuracy of our model by tuning the parameters we set when initializing the neural network object. Suppose we decide to change the optimization algorithm to gradient descent, but leave all other model parameters unchanged. # Initialize neural network object and fit object nn_model2 = mlrose_ky . NeuralNetwork ( hidden_nodes = [ 2 ], activation = 'relu' , \\ algorithm = 'gradient_descent' , max_iters = 1000 , \\ bias = True , is_classifier = True , learning_rate = 0.0001 , \\ early_stopping = True , clip_max = 5 , max_attempts = 100 , \\ random_state = 3 ) nn_model2 . fit ( X_train_scaled , y_train_hot ) # Predict labels for train set and assess accuracy y_train_pred = nn_model2 . predict ( X_train_scaled ) y_train_accuracy = accuracy_score ( y_train_hot , y_train_pred ) print ( y_train_accuracy ) 0.625 # Predict labels for test set and assess accuracy y_test_pred = nn_model2 . predict ( X_test_scaled ) y_test_accuracy = accuracy_score ( y_test_hot , y_test_pred ) print ( y_test_accuracy ) 0.566666666667 This results in a 39% increase in training accuracy to 62.5%, but a much smaller increase in test accuracy to 56.7%. Linear and Logistic Regression Models # Linear and logistic regression models are special cases of neural networks. A linear regression is a regression neural network with no hidden layers and an identity activation fuction, while a logistic regression is a classification neural network with no hidden layers and a sigmoid activation function. As a result, we could fit either of these models to our data using the NeuralNetwork() class with parameters set appropriately. For example, suppose we wished to fit a logistic regression to our Iris data using the randomized hill climbing algorithm and all other parameters set as for the example in the previous section. We could do this by initializing a NeuralNetwork() object like so: lr_nn_model1 = mlrose_ky . NeuralNetwork ( hidden_nodes = [], activation = 'sigmoid' , \\ algorithm = 'random_hill_climb' , max_iters = 1000 , \\ bias = True , is_classifier = True , learning_rate = 0.0001 , \\ early_stopping = True , clip_max = 5 , max_attempts = 100 , \\ random_state = 3 ) However, for convenience, mlrose-ky provides the LinearRegression() and LogisticRegression() wrapper classes, which simplify model initialization. In our Iris dataset example, we can, thus, initialize and fit our logistic regression model as follows: # Initialize logistic regression object and fit object lr_model1 = mlrose_ky . LogisticRegression ( algorithm = 'random_hill_climb' , max_iters = 1000 , \\ bias = True , learning_rate = 0.0001 , \\ early_stopping = True , clip_max = 5 , max_attempts = 100 , \\ random_state = 3 ) lr_model1 . fit ( X_train_scaled , y_train_hot ) # Predict labels for train set and assess accuracy y_train_pred = lr_model1 . predict ( X_train_scaled ) y_train_accuracy = accuracy_score ( y_train_hot , y_train_pred ) print ( y_train_accuracy ) 0.191666666667 # Predict labels for test set and assess accuracy y_test_pred = lr_model1 . predict ( X_test_scaled ) y_test_accuracy = accuracy_score ( y_test_hot , y_test_pred ) print ( y_test_accuracy ) 0.0666666666667 This model achieves 19.2% training accuracy and 6.7% test accuracy, which is worse than if we predicted the labels by selecting values at random. Nevertheless, as in the previous section, we can potentially improve model accuracy by tuning the parameters set at initialization. Suppose we increase our learning rate to 0.01. # Initialize logistic regression object and fit object lr_model2 = mlrose_ky . LogisticRegression ( algorithm = 'random_hill_climb' , max_iters = 1000 , \\ bias = True , learning_rate = 0.01 , \\ early_stopping = True , clip_max = 5 , max_attempts = 100 , \\ random_state = 3 ) lr_model2 . fit ( X_train_scaled , y_train_hot ) # Predict labels for train set and assess accuracy y_train_pred = lr_model2 . predict ( X_train_scaled ) y_train_accuracy = accuracy_score ( y_train_hot , y_train_pred ) print ( y_train_accuracy ) 0.683333333333 # Predict labels for test set and assess accuracy y_test_pred = lr_model2 . predict ( X_test_scaled ) y_test_accuracy = accuracy_score ( y_test_hot , y_test_pred ) print ( y_test_accuracy ) 0.7 This results in signficant improvements to both training and test accuracy, with training accuracy levels now reaching 68.3% and test accuracy levels reaching 70%. Summary # In this tutorial we demonstrated how mlrose-ky can be used to find the optimal weights of three types of machine learning models: neural networks, linear regression models and logistic regression models. Applying randomized optimization algorithms to the machine learning weight optimization problem is most certainly not the most common approach to solving this problem. However, it serves to demonstrate the versatility of the mlrose-ky package and of randomized optimization algorithms in general.","title":"3 - What is a Machine Learning Weight Optimization Problem?"},{"location":"tutorial3/#tutorial-machine-learning-weight-optimization-problems","text":"","title":"Tutorial - Machine Learning Weight Optimization Problems"},{"location":"tutorial3/#what-is-a-machine-learning-weight-optimization-problem","text":"For a number of different machine learning models, the process of fitting the model parameters involves finding the parameter values that minimize a pre-specified loss function for a given training dataset. Examples of such models include neural networks, linear regression models and logistic regression models, and the optimal model weights for such models are typically found using methods such as gradient descent. However, the problem of fitting the parameters (or weights) of a machine learning model can also be viewed as a continuous-state optimization problem, where the loss function takes the role of the fitness function, and the goal is to minimize this function. By framing the problem this way, we can use any of the randomized optimization algorithms that are suited to continuous-state optimization problems to fit the model parameters. In this tutorial, we will work through an example of how this can be done with mlrose-ky.","title":"What is a Machine Learning Weight Optimization Problem?"},{"location":"tutorial3/#solving-machine-learning-weight-optimization-problems-with-mlrose-ky","text":"mlrose-ky contains built-in functionality for solving the weight optimization problem for three types of machine learning models: (standard) neural networks, linear regression models and logistic regression models. This is done using the NeuralNetwork() , LinearRegression() and LogisticRegression() classes respectively. Each of these classes includes a fit method, which implements the three steps for solving an optimization problem defined in the previous tutorials, for a given training dataset. However, when fitting a machine learning model, finding the optimal model weights is merely a means to an end. We want to find the optimal model weights so that we can use our fitted model to predict the labels of future observations as accurately as possible, not because we are actually interested in knowing the optimal weight values. As a result, the abovementioned classes also include a predict method, which, if called after the fit method, will predict the labels for a given test dataset using the fitted model. The steps involved in solving a machine learning weight optimization problem with mlrose-ky are typically: Initialize a machine learning weight optimization problem object. Find the optimal model weights for a given training dataset by calling the fit method of the object initialized in step 1. Predict the labels for a test dataset by calling the predict method of the object initialized in step 1. To fit the model weights, the user can choose between using either randomized hill climbing, simulated annealing, the genetic algorithm or gradient descent. In mlrose-ky, the gradient descent algorithm is only available for use in solving the machine learning weight optimization problem and has been included primarily for benchmarking purposes, since this is one of the most common algorithm used in fitting neural networks and regression models. We will now work through an example to illustrate how mlrose-ky can be used to fit a neural network and a regression model to a given dataset.","title":"Solving Machine Learning Weight Optimization Problems with mlrose-ky"},{"location":"tutorial3/#example-the-iris-dataset","text":"The Iris dataset is a famous multivariate classification dataset first presented in a 1936 research paper by statistician and biologist Ronald Fisher. It contains 150 observations of three classes (species) of iris flowers (50 observations of each class), with each observation providing the sepal length, sepal width, petal length and petal width (i.e. the feature values), as well as the class label (i.e. the target value), of each flower under consideration. The Iris dataset is included with the Python sklearn package. The feature values and label of the first observation in the dataset are shown below, along with the maximum and minimum values of each of the features and the unique label values: import numpy as np from sklearn.datasets import load_iris # Load the Iris dataset data = load_iris () # Get feature values print ( data . data [ 0 ]) [ 5.1 3.5 1.4 0.2 ] # Get feature names print ( data . feature_names ) [ 'sepal length (cm)' , 'sepal width (cm)' , 'petal length (cm)' , 'petal width (cm)' ] # Get target value of first observation print ( data . target [ 0 ]) 0 # Get target name of first observation print ( data . target_names [ data . target [ 0 ]]) setosa # Get minimum feature values print ( np . min ( data . data , axis = 0 )) [ 4.3 2. 1. 0.1 ] # Get maximum feature values print ( np . max ( data . data , axis = 0 )) [ 7.9 4.4 6.9 2.5 ] # Get unique target values print ( np . unique ( data . target )) [ 0 1 2 ] From this we can see that all features in the Iris data set are numeric, albeit with different ranges, and that the class labels have been represented by integers. In the next few sections we will show how mlrose-ky can be used to fit a neural network and a logistic regression model to this dataset, to predict the species of an iris flower given its feature values.","title":"Example: the Iris Dataset"},{"location":"tutorial3/#data-pre-processing","text":"Before we can fit any sort of machine learning model to a dataset, it is necessary to manipulate our data into the form expected by mlrose-ky. Each of the three machine learning models supported by mlrose-ky expect to receive feature data in the form of a numpy array, with one row per observation and numeric features only (any categorical features must be one-hot encoded before passing to the machine learning models). The models also expect to receive the target values as either: a list of numeric values (for regression data); a list of 0-1 indicator values (for binary classification data); or as a numpy array of one-hot encoded labels, with one row per observation (for multi-class classification data). In the case of the Iris dataset, all of our features are numeric, so no one-hot encoding is required. However, it is necessary to one-hot encode the class labels. In keeping with standard machine learning practice, it is also necessary to split the data into training and test subsets, and since the range of the Iris data varies considerably from feature to feature, to standardize the values of our feature variables. These pre-processing steps are implemented below. from sklearn.model_selection import train_test_split from sklearn.preprocessing import MinMaxScaler , OneHotEncoder # Split data into training and test sets X_train , X_test , y_train , y_test = train_test_split ( data . data , data . target , \\ test_size = 0.2 , random_state = 3 ) # Normalize feature data scaler = MinMaxScaler () X_train_scaled = scaler . fit_transform ( X_train ) X_test_scaled = scaler . transform ( X_test ) # One hot encode target values one_hot = OneHotEncoder () y_train_hot = one_hot . fit_transform ( y_train . reshape ( - 1 , 1 )) . todense () y_test_hot = one_hot . transform ( y_test . reshape ( - 1 , 1 )) . todense ()","title":"Data Pre-Processing"},{"location":"tutorial3/#neural-networks","text":"Once the data has been preprocessed, fitting a neural network in mlrose-ky simply involves following the steps listed above. Suppose we wish to fit a neural network classifier to our Iris dataset with one hidden layer containing 2 nodes and a ReLU activation function (mlrose-ky supports the ReLU, identity, sigmoid and tanh activation functions). For this example, we will use the Randomized Hill Climbing algorithm to find the optimal weights, with a maximum of 1000 iterations of the algorithm and 100 attempts to find a better set of weights at each step. We will also include a bias term; use a step size (learning rate) of 0.0001; and limit our weights to being in the range -5 to 5 (to reduce the landscape over which the algorithm must search in order to find the optimal weights). This model is initialized and fitted to our preprocessed data below: # Initialize neural network object and fit object nn_model1 = mlrose_ky . NeuralNetwork ( hidden_nodes = [ 2 ], activation = 'relu' , \\ algorithm = 'random_hill_climb' , max_iters = 1000 , \\ bias = True , is_classifier = True , learning_rate = 0.0001 , \\ early_stopping = True , clip_max = 5 , max_attempts = 100 , \\ random_state = 3 ) nn_model1 . fit ( X_train_scaled , y_train_hot ) Once the model is fitted, we can use it to predict the labels for our training and test datasets and use these prediction to assess the model\u2019s training and test accuracy. from sklearn.metrics import accuracy_score # Predict labels for train set and assess accuracy y_train_pred = nn_model1 . predict ( X_train_scaled ) y_train_accuracy = accuracy_score ( y_train_hot , y_train_pred ) print ( y_train_accuracy ) 0.45 # Predict labels for test set and assess accuracy y_test_pred = nn_model1 . predict ( X_test_scaled ) y_test_accuracy = accuracy_score ( y_test_hot , y_test_pred ) print ( y_test_accuracy ) 0.533333333333 In this case, our model achieves training accuracy of 45% and test accuracy of 53.3%. These accuracy levels are better than if the labels were selected at random, but still leave room for improvement. We can potentially improve on the accuracy of our model by tuning the parameters we set when initializing the neural network object. Suppose we decide to change the optimization algorithm to gradient descent, but leave all other model parameters unchanged. # Initialize neural network object and fit object nn_model2 = mlrose_ky . NeuralNetwork ( hidden_nodes = [ 2 ], activation = 'relu' , \\ algorithm = 'gradient_descent' , max_iters = 1000 , \\ bias = True , is_classifier = True , learning_rate = 0.0001 , \\ early_stopping = True , clip_max = 5 , max_attempts = 100 , \\ random_state = 3 ) nn_model2 . fit ( X_train_scaled , y_train_hot ) # Predict labels for train set and assess accuracy y_train_pred = nn_model2 . predict ( X_train_scaled ) y_train_accuracy = accuracy_score ( y_train_hot , y_train_pred ) print ( y_train_accuracy ) 0.625 # Predict labels for test set and assess accuracy y_test_pred = nn_model2 . predict ( X_test_scaled ) y_test_accuracy = accuracy_score ( y_test_hot , y_test_pred ) print ( y_test_accuracy ) 0.566666666667 This results in a 39% increase in training accuracy to 62.5%, but a much smaller increase in test accuracy to 56.7%.","title":"Neural Networks"},{"location":"tutorial3/#linear-and-logistic-regression-models","text":"Linear and logistic regression models are special cases of neural networks. A linear regression is a regression neural network with no hidden layers and an identity activation fuction, while a logistic regression is a classification neural network with no hidden layers and a sigmoid activation function. As a result, we could fit either of these models to our data using the NeuralNetwork() class with parameters set appropriately. For example, suppose we wished to fit a logistic regression to our Iris data using the randomized hill climbing algorithm and all other parameters set as for the example in the previous section. We could do this by initializing a NeuralNetwork() object like so: lr_nn_model1 = mlrose_ky . NeuralNetwork ( hidden_nodes = [], activation = 'sigmoid' , \\ algorithm = 'random_hill_climb' , max_iters = 1000 , \\ bias = True , is_classifier = True , learning_rate = 0.0001 , \\ early_stopping = True , clip_max = 5 , max_attempts = 100 , \\ random_state = 3 ) However, for convenience, mlrose-ky provides the LinearRegression() and LogisticRegression() wrapper classes, which simplify model initialization. In our Iris dataset example, we can, thus, initialize and fit our logistic regression model as follows: # Initialize logistic regression object and fit object lr_model1 = mlrose_ky . LogisticRegression ( algorithm = 'random_hill_climb' , max_iters = 1000 , \\ bias = True , learning_rate = 0.0001 , \\ early_stopping = True , clip_max = 5 , max_attempts = 100 , \\ random_state = 3 ) lr_model1 . fit ( X_train_scaled , y_train_hot ) # Predict labels for train set and assess accuracy y_train_pred = lr_model1 . predict ( X_train_scaled ) y_train_accuracy = accuracy_score ( y_train_hot , y_train_pred ) print ( y_train_accuracy ) 0.191666666667 # Predict labels for test set and assess accuracy y_test_pred = lr_model1 . predict ( X_test_scaled ) y_test_accuracy = accuracy_score ( y_test_hot , y_test_pred ) print ( y_test_accuracy ) 0.0666666666667 This model achieves 19.2% training accuracy and 6.7% test accuracy, which is worse than if we predicted the labels by selecting values at random. Nevertheless, as in the previous section, we can potentially improve model accuracy by tuning the parameters set at initialization. Suppose we increase our learning rate to 0.01. # Initialize logistic regression object and fit object lr_model2 = mlrose_ky . LogisticRegression ( algorithm = 'random_hill_climb' , max_iters = 1000 , \\ bias = True , learning_rate = 0.01 , \\ early_stopping = True , clip_max = 5 , max_attempts = 100 , \\ random_state = 3 ) lr_model2 . fit ( X_train_scaled , y_train_hot ) # Predict labels for train set and assess accuracy y_train_pred = lr_model2 . predict ( X_train_scaled ) y_train_accuracy = accuracy_score ( y_train_hot , y_train_pred ) print ( y_train_accuracy ) 0.683333333333 # Predict labels for test set and assess accuracy y_test_pred = lr_model2 . predict ( X_test_scaled ) y_test_accuracy = accuracy_score ( y_test_hot , y_test_pred ) print ( y_test_accuracy ) 0.7 This results in signficant improvements to both training and test accuracy, with training accuracy levels now reaching 68.3% and test accuracy levels reaching 70%.","title":"Linear and Logistic Regression Models"},{"location":"tutorial3/#summary","text":"In this tutorial we demonstrated how mlrose-ky can be used to find the optimal weights of three types of machine learning models: neural networks, linear regression models and logistic regression models. Applying randomized optimization algorithms to the machine learning weight optimization problem is most certainly not the most common approach to solving this problem. However, it serves to demonstrate the versatility of the mlrose-ky package and of randomized optimization algorithms in general.","title":"Summary"},{"location":"tutorial_examples/","text":"mlrose-ky Tutorial Examples - Genevieve Hayes, Kyle Nakamura # Overview # mlrose_ky is a Python package for applying some of the most common randomized optimization and search algorithms to a range of different optimization problems, over both discrete- and continuous-valued parameter spaces. This notebook contains the examples used in the mlrose_ky tutorial. Import Libraries # import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import MinMaxScaler , OneHotEncoder from sklearn.metrics import accuracy_score import mlrose_ky as mlrose Example 1: 8-Queens Using Pre-Defined Fitness Function # # Initialize fitness function object using pre-defined class fitness = mlrose . Queens () # Define optimization problem object problem = mlrose . DiscreteOpt ( length = 8 , fitness_fn = fitness , maximize = False , max_val = 8 ) # Define decay schedule schedule = mlrose . ExpDecay () # Solve using simulated annealing - attempt 1 init_state = np . array ([ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 ]) best_state , best_fitness , _ = mlrose . simulated_annealing ( problem , schedule = schedule , max_iters = 1000 , init_state = init_state , random_state = 1 ) print ( \"The best state found is:\" , best_state ) The best state found is: [6 3 7 2 1 5 2 5] print ( \"The fitness at the best state is:\" , best_fitness ) The fitness at the best state is: 3.0 # Solve using simulated annealing - attempt 2 best_state , best_fitness , _ = mlrose . simulated_annealing ( problem , schedule = schedule , max_attempts = 100 , max_iters = 1000 , init_state = init_state , random_state = 1 ) best_state array([4, 2, 0, 6, 1, 7, 5, 3]) best_fitness 0.0 Example 2: 8-Queens Using Custom Fitness Function # # Define alternative N-Queens fitness function for maximization problem def queens_max ( state ): # Initialize counter fitness = 0 # For all pairs of queens for i in range ( len ( state ) - 1 ): for j in range ( i + 1 , len ( state )): # Check for horizontal, diagonal-up and diagonal-down attacks if ( state [ j ] != state [ i ]) and ( state [ j ] != state [ i ] + ( j - i )) and ( state [ j ] != state [ i ] - ( j - i )): # If no attacks, then increment counter fitness += 1 return fitness # Check function is working correctly state = np . array ([ 1 , 4 , 1 , 3 , 5 , 5 , 2 , 7 ]) # The fitness of this state should be 22 queens_max ( state ) 22 # Initialize custom fitness function object fitness_cust = mlrose . CustomFitness ( queens_max ) # Define optimization problem object problem_cust = mlrose . DiscreteOpt ( length = 8 , fitness_fn = fitness_cust , max_val = 8 ) # Solve using simulated annealing - attempt 1 best_state , best_fitness , _ = mlrose . simulated_annealing ( problem_cust , schedule = schedule , max_iters = 1000 , init_state = init_state , random_state = 1 ) best_state array([6, 4, 7, 3, 6, 2, 5, 1]) best_fitness 26.0 # Solve using simulated annealing - attempt 2 best_state , best_fitness , _ = mlrose . simulated_annealing ( problem_cust , schedule = schedule , max_attempts = 100 , max_iters = 1000 , init_state = init_state , random_state = 1 ) best_state array([4, 1, 3, 5, 7, 2, 0, 6]) best_fitness 28.0 Example 3: Travelling Salesperson Using Coordinate-Defined Fitness Function # # Create list of city coordinates coords_list = [( 1 , 1 ), ( 4 , 2 ), ( 5 , 2 ), ( 6 , 4 ), ( 4 , 4 ), ( 3 , 6 ), ( 1 , 5 ), ( 2 , 3 )] # Initialize fitness function object using coords_list fitness_coords = mlrose . TravellingSales ( coords = coords_list ) # Define optimization problem object problem_fit = mlrose . TSPOpt ( length = 8 , fitness_fn = fitness_coords ) # Solve using genetic algorithm - attempt 1 best_state , best_fitness , _ = mlrose . genetic_alg ( problem_fit , random_state = 2 ) best_state array([0, 7, 6, 5, 4, 3, 2, 1]) best_fitness 17.342617547667327 # Solve using genetic algorithm - attempt 2 best_state , best_fitness , _ = mlrose . genetic_alg ( problem_fit , mutation_prob = 0.2 , max_attempts = 100 , random_state = 2 ) best_state array([0, 7, 6, 5, 4, 3, 2, 1]) best_fitness 17.342617547667327 Example 4: Travelling Salesperson Using Distance-Defined Fitness Function # # Create list of distances between pairs of cities dist_list = [ ( 0 , 1 , 3.1623 ), ( 0 , 2 , 4.1231 ), ( 0 , 3 , 5.8310 ), ( 0 , 4 , 4.2426 ), ( 0 , 5 , 5.3852 ), ( 0 , 6 , 4.0000 ), ( 0 , 7 , 2.2361 ), ( 1 , 2 , 1.0000 ), ( 1 , 3 , 2.8284 ), ( 1 , 4 , 2.0000 ), ( 1 , 5 , 4.1231 ), ( 1 , 6 , 4.2426 ), ( 1 , 7 , 2.2361 ), ( 2 , 3 , 2.2361 ), ( 2 , 4 , 2.2361 ), ( 2 , 5 , 4.4721 ), ( 2 , 6 , 5.0000 ), ( 2 , 7 , 3.1623 ), ( 3 , 4 , 2.0000 ), ( 3 , 5 , 3.6056 ), ( 3 , 6 , 5.0990 ), ( 3 , 7 , 4.1231 ), ( 4 , 5 , 2.2361 ), ( 4 , 6 , 3.1623 ), ( 4 , 7 , 2.2361 ), ( 5 , 6 , 2.2361 ), ( 5 , 7 , 3.1623 ), ( 6 , 7 , 2.2361 ), ] # Initialize fitness function object using dist_list fitness_dists = mlrose . TravellingSales ( distances = dist_list ) # Define optimization problem object problem_fit2 = mlrose . TSPOpt ( length = 8 , fitness_fn = fitness_dists ) # Solve using genetic algorithm best_state , best_fitness , _ = mlrose . genetic_alg ( problem_fit2 , mutation_prob = 0.2 , max_attempts = 100 , random_state = 2 ) best_state array([7, 0, 1, 2, 3, 4, 5, 6]) best_fitness 17.3428 Example 5: Travelling Salesperson Defining Fitness Function as Part of Optimization Problem Definition Step # # Create list of city coordinates coords_list = [( 1 , 1 ), ( 4 , 2 ), ( 5 , 2 ), ( 6 , 4 ), ( 4 , 4 ), ( 3 , 6 ), ( 1 , 5 ), ( 2 , 3 )] # Define optimization problem object problem_no_fit = mlrose . TSPOpt ( length = 8 , coords = coords_list ) # Solve using genetic algorithm best_state , best_fitness , _ = mlrose . genetic_alg ( problem_no_fit , mutation_prob = 0.2 , max_attempts = 100 , random_state = 2 ) best_state array([0, 7, 6, 5, 4, 3, 2, 1]) best_fitness 17.342617547667327 Example 6: Fitting a Neural Network to the Iris Dataset # # Load the Iris dataset data = load_iris () # Get feature values of first observation print ( data . data [ 0 ]) [5.1 3.5 1.4 0.2] # Get feature names print ( data . feature_names ) ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] # Get target value of first observation print ( data . target [ 0 ]) 0 # Get target name of first observation print ( data . target_names [ data . target [ 0 ]]) setosa # Get minimum feature values print ( np . min ( data . data , axis = 0 )) [4.3 2. 1. 0.1] # Get maximum feature values print ( np . max ( data . data , axis = 0 )) [7.9 4.4 6.9 2.5] # Get unique target values print ( np . unique ( data . target )) [0 1 2] # Split data into training and test sets X_train , X_test , y_train , y_test = train_test_split ( data . data , data . target , test_size = 0.2 , random_state = 3 ) # Normalize feature data scaler = MinMaxScaler () X_train_scaled = scaler . fit_transform ( X_train ) X_test_scaled = scaler . transform ( X_test ) # One hot encode target values one_hot = OneHotEncoder () y_train_hot = np . asarray ( one_hot . fit_transform ( y_train . reshape ( - 1 , 1 )) . todense ()) y_test_hot = np . asarray ( one_hot . transform ( y_test . reshape ( - 1 , 1 )) . todense ()) # Initialize neural network object and fit object - attempt 1 nn_model1 = mlrose . NeuralNetwork ( hidden_nodes = [ 2 ], max_iters = 1000 , learning_rate = 0.0001 , early_stopping = True , clip_max = 5 , max_attempts = 100 , random_state = 3 ) nn_model1 . fit ( X_train_scaled , y_train_hot ) #sk-container-id-1 { /* Definition of color scheme common for light and dark mode */ --sklearn-color-text: black; --sklearn-color-line: gray; /* Definition of color scheme for unfitted estimators */ --sklearn-color-unfitted-level-0: #fff5e6; --sklearn-color-unfitted-level-1: #f6e4d2; --sklearn-color-unfitted-level-2: #ffe0b3; --sklearn-color-unfitted-level-3: chocolate; /* Definition of color scheme for fitted estimators */ --sklearn-color-fitted-level-0: #f0f8ff; --sklearn-color-fitted-level-1: #d4ebff; --sklearn-color-fitted-level-2: #b3dbfd; --sklearn-color-fitted-level-3: cornflowerblue; /* Specific color for light theme */ --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black))); --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white))); --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black))); --sklearn-color-icon: #696969; @media (prefers-color-scheme: dark) { /* Redefinition of color scheme for dark theme */ --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white))); --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111))); --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white))); --sklearn-color-icon: #878787; } } #sk-container-id-1 { color: var(--sklearn-color-text); } #sk-container-id-1 pre { padding: 0; } #sk-container-id-1 input.sk-hidden--visually { border: 0; clip: rect(1px 1px 1px 1px); clip: rect(1px, 1px, 1px, 1px); height: 1px; margin: -1px; overflow: hidden; padding: 0; position: absolute; width: 1px; } #sk-container-id-1 div.sk-dashed-wrapped { border: 1px dashed var(--sklearn-color-line); margin: 0 0.4em 0.5em 0.4em; box-sizing: border-box; padding-bottom: 0.4em; background-color: var(--sklearn-color-background); } #sk-container-id-1 div.sk-container { /* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */ display: inline-block !important; position: relative; } #sk-container-id-1 div.sk-text-repr-fallback { display: none; } div.sk-parallel-item, div.sk-serial, div.sk-item { /* draw centered vertical line to link estimators */ background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background)); background-size: 2px 100%; background-repeat: no-repeat; background-position: center center; } /* Parallel-specific style estimator block */ #sk-container-id-1 div.sk-parallel-item::after { content: \"\"; width: 100%; border-bottom: 2px solid var(--sklearn-color-text-on-default-background); flex-grow: 1; } #sk-container-id-1 div.sk-parallel { display: flex; align-items: stretch; justify-content: center; background-color: var(--sklearn-color-background); position: relative; } #sk-container-id-1 div.sk-parallel-item { display: flex; flex-direction: column; } #sk-container-id-1 div.sk-parallel-item:first-child::after { align-self: flex-end; width: 50%; } #sk-container-id-1 div.sk-parallel-item:last-child::after { align-self: flex-start; width: 50%; } #sk-container-id-1 div.sk-parallel-item:only-child::after { width: 0; } /* Serial-specific style estimator block */ #sk-container-id-1 div.sk-serial { display: flex; flex-direction: column; align-items: center; background-color: var(--sklearn-color-background); padding-right: 1em; padding-left: 1em; } /* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is clickable and can be expanded/collapsed. - Pipeline and ColumnTransformer use this feature and define the default style - Estimators will overwrite some part of the style using the `sk-estimator` class */ /* Pipeline and ColumnTransformer style (default) */ #sk-container-id-1 div.sk-toggleable { /* Default theme specific background. It is overwritten whether we have a specific estimator or a Pipeline/ColumnTransformer */ background-color: var(--sklearn-color-background); } /* Toggleable label */ #sk-container-id-1 label.sk-toggleable__label { cursor: pointer; display: block; width: 100%; margin-bottom: 0; padding: 0.5em; box-sizing: border-box; text-align: center; } #sk-container-id-1 label.sk-toggleable__label-arrow:before { /* Arrow on the left of the label */ content: \"\u25b8\"; float: left; margin-right: 0.25em; color: var(--sklearn-color-icon); } #sk-container-id-1 label.sk-toggleable__label-arrow:hover:before { color: var(--sklearn-color-text); } /* Toggleable content - dropdown */ #sk-container-id-1 div.sk-toggleable__content { max-height: 0; max-width: 0; overflow: hidden; text-align: left; /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-1 div.sk-toggleable__content.fitted { /* fitted */ background-color: var(--sklearn-color-fitted-level-0); } #sk-container-id-1 div.sk-toggleable__content pre { margin: 0.2em; border-radius: 0.25em; color: var(--sklearn-color-text); /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-1 div.sk-toggleable__content.fitted pre { /* unfitted */ background-color: var(--sklearn-color-fitted-level-0); } #sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content { /* Expand drop-down */ max-height: 200px; max-width: 100%; overflow: auto; } #sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before { content: \"\u25be\"; } /* Pipeline/ColumnTransformer-specific style */ #sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label { color: var(--sklearn-color-text); background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label { background-color: var(--sklearn-color-fitted-level-2); } /* Estimator-specific style */ /* Colorize estimator box */ #sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label { /* fitted */ background-color: var(--sklearn-color-fitted-level-2); } #sk-container-id-1 div.sk-label label.sk-toggleable__label, #sk-container-id-1 div.sk-label label { /* The background is the default theme color */ color: var(--sklearn-color-text-on-default-background); } /* On hover, darken the color of the background */ #sk-container-id-1 div.sk-label:hover label.sk-toggleable__label { color: var(--sklearn-color-text); background-color: var(--sklearn-color-unfitted-level-2); } /* Label box, darken color on hover, fitted */ #sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted { color: var(--sklearn-color-text); background-color: var(--sklearn-color-fitted-level-2); } /* Estimator label */ #sk-container-id-1 div.sk-label label { font-family: monospace; font-weight: bold; display: inline-block; line-height: 1.2em; } #sk-container-id-1 div.sk-label-container { text-align: center; } /* Estimator-specific */ #sk-container-id-1 div.sk-estimator { font-family: monospace; border: 1px dotted var(--sklearn-color-border-box); border-radius: 0.25em; box-sizing: border-box; margin-bottom: 0.5em; /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-1 div.sk-estimator.fitted { /* fitted */ background-color: var(--sklearn-color-fitted-level-0); } /* on hover */ #sk-container-id-1 div.sk-estimator:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-1 div.sk-estimator.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-2); } /* Specification for estimator info (e.g. \"i\" and \"?\") */ /* Common style for \"i\" and \"?\" */ .sk-estimator-doc-link, a:link.sk-estimator-doc-link, a:visited.sk-estimator-doc-link { float: right; font-size: smaller; line-height: 1em; font-family: monospace; background-color: var(--sklearn-color-background); border-radius: 1em; height: 1em; width: 1em; text-decoration: none !important; margin-left: 1ex; /* unfitted */ border: var(--sklearn-color-unfitted-level-1) 1pt solid; color: var(--sklearn-color-unfitted-level-1); } .sk-estimator-doc-link.fitted, a:link.sk-estimator-doc-link.fitted, a:visited.sk-estimator-doc-link.fitted { /* fitted */ border: var(--sklearn-color-fitted-level-1) 1pt solid; color: var(--sklearn-color-fitted-level-1); } /* On hover */ div.sk-estimator:hover .sk-estimator-doc-link:hover, .sk-estimator-doc-link:hover, div.sk-label-container:hover .sk-estimator-doc-link:hover, .sk-estimator-doc-link:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover, .sk-estimator-doc-link.fitted:hover, div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover, .sk-estimator-doc-link.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } /* Span, style for the box shown on hovering the info icon */ .sk-estimator-doc-link span { display: none; z-index: 9999; position: relative; font-weight: normal; right: .2ex; padding: .5ex; margin: .5ex; width: min-content; min-width: 20ex; max-width: 50ex; color: var(--sklearn-color-text); box-shadow: 2pt 2pt 4pt #999; /* unfitted */ background: var(--sklearn-color-unfitted-level-0); border: .5pt solid var(--sklearn-color-unfitted-level-3); } .sk-estimator-doc-link.fitted span { /* fitted */ background: var(--sklearn-color-fitted-level-0); border: var(--sklearn-color-fitted-level-3); } .sk-estimator-doc-link:hover span { display: block; } /* \"?\"-specific style due to the `<a>` HTML tag */ #sk-container-id-1 a.estimator_doc_link { float: right; font-size: 1rem; line-height: 1em; font-family: monospace; background-color: var(--sklearn-color-background); border-radius: 1rem; height: 1rem; width: 1rem; text-decoration: none; /* unfitted */ color: var(--sklearn-color-unfitted-level-1); border: var(--sklearn-color-unfitted-level-1) 1pt solid; } #sk-container-id-1 a.estimator_doc_link.fitted { /* fitted */ border: var(--sklearn-color-fitted-level-1) 1pt solid; color: var(--sklearn-color-fitted-level-1); } /* On hover */ #sk-container-id-1 a.estimator_doc_link:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } #sk-container-id-1 a.estimator_doc_link.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-3); } NeuralNetwork(clip_max=5, early_stopping=True, hidden_nodes=[2], learning_rate=0.0001, max_attempts=100, max_iters=1000, random_state=3) In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. NeuralNetwork i Fitted NeuralNetwork(clip_max=5, early_stopping=True, hidden_nodes=[2], learning_rate=0.0001, max_attempts=100, max_iters=1000, random_state=3) # Predict labels for train set and assess accuracy y_train_pred = nn_model1 . predict ( X_train_scaled ) y_train_accuracy = accuracy_score ( y_train_hot , y_train_pred ) y_train_accuracy 0.45 # Predict labels for test set and assess accuracy y_test_pred = nn_model1 . predict ( X_test_scaled ) y_test_accuracy = accuracy_score ( y_test_hot , y_test_pred ) y_test_accuracy 0.5333333333333333 # Initialize neural network object and fit object - attempt 2 nn_model2 = mlrose . NeuralNetwork ( hidden_nodes = [ 2 ], algorithm = \"gradient_descent\" , max_iters = 1000 , learning_rate = 0.0001 , early_stopping = True , clip_max = 5 , max_attempts = 100 , random_state = 3 ) nn_model2 . fit ( X_train_scaled , y_train_hot ) #sk-container-id-2 { /* Definition of color scheme common for light and dark mode */ --sklearn-color-text: black; --sklearn-color-line: gray; /* Definition of color scheme for unfitted estimators */ --sklearn-color-unfitted-level-0: #fff5e6; --sklearn-color-unfitted-level-1: #f6e4d2; --sklearn-color-unfitted-level-2: #ffe0b3; --sklearn-color-unfitted-level-3: chocolate; /* Definition of color scheme for fitted estimators */ --sklearn-color-fitted-level-0: #f0f8ff; --sklearn-color-fitted-level-1: #d4ebff; --sklearn-color-fitted-level-2: #b3dbfd; --sklearn-color-fitted-level-3: cornflowerblue; /* Specific color for light theme */ --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black))); --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white))); --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black))); --sklearn-color-icon: #696969; @media (prefers-color-scheme: dark) { /* Redefinition of color scheme for dark theme */ --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white))); --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111))); --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white))); --sklearn-color-icon: #878787; } } #sk-container-id-2 { color: var(--sklearn-color-text); } #sk-container-id-2 pre { padding: 0; } #sk-container-id-2 input.sk-hidden--visually { border: 0; clip: rect(1px 1px 1px 1px); clip: rect(1px, 1px, 1px, 1px); height: 1px; margin: -1px; overflow: hidden; padding: 0; position: absolute; width: 1px; } #sk-container-id-2 div.sk-dashed-wrapped { border: 1px dashed var(--sklearn-color-line); margin: 0 0.4em 0.5em 0.4em; box-sizing: border-box; padding-bottom: 0.4em; background-color: var(--sklearn-color-background); } #sk-container-id-2 div.sk-container { /* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */ display: inline-block !important; position: relative; } #sk-container-id-2 div.sk-text-repr-fallback { display: none; } div.sk-parallel-item, div.sk-serial, div.sk-item { /* draw centered vertical line to link estimators */ background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background)); background-size: 2px 100%; background-repeat: no-repeat; background-position: center center; } /* Parallel-specific style estimator block */ #sk-container-id-2 div.sk-parallel-item::after { content: \"\"; width: 100%; border-bottom: 2px solid var(--sklearn-color-text-on-default-background); flex-grow: 1; } #sk-container-id-2 div.sk-parallel { display: flex; align-items: stretch; justify-content: center; background-color: var(--sklearn-color-background); position: relative; } #sk-container-id-2 div.sk-parallel-item { display: flex; flex-direction: column; } #sk-container-id-2 div.sk-parallel-item:first-child::after { align-self: flex-end; width: 50%; } #sk-container-id-2 div.sk-parallel-item:last-child::after { align-self: flex-start; width: 50%; } #sk-container-id-2 div.sk-parallel-item:only-child::after { width: 0; } /* Serial-specific style estimator block */ #sk-container-id-2 div.sk-serial { display: flex; flex-direction: column; align-items: center; background-color: var(--sklearn-color-background); padding-right: 1em; padding-left: 1em; } /* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is clickable and can be expanded/collapsed. - Pipeline and ColumnTransformer use this feature and define the default style - Estimators will overwrite some part of the style using the `sk-estimator` class */ /* Pipeline and ColumnTransformer style (default) */ #sk-container-id-2 div.sk-toggleable { /* Default theme specific background. It is overwritten whether we have a specific estimator or a Pipeline/ColumnTransformer */ background-color: var(--sklearn-color-background); } /* Toggleable label */ #sk-container-id-2 label.sk-toggleable__label { cursor: pointer; display: block; width: 100%; margin-bottom: 0; padding: 0.5em; box-sizing: border-box; text-align: center; } #sk-container-id-2 label.sk-toggleable__label-arrow:before { /* Arrow on the left of the label */ content: \"\u25b8\"; float: left; margin-right: 0.25em; color: var(--sklearn-color-icon); } #sk-container-id-2 label.sk-toggleable__label-arrow:hover:before { color: var(--sklearn-color-text); } /* Toggleable content - dropdown */ #sk-container-id-2 div.sk-toggleable__content { max-height: 0; max-width: 0; overflow: hidden; text-align: left; /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-2 div.sk-toggleable__content.fitted { /* fitted */ background-color: var(--sklearn-color-fitted-level-0); } #sk-container-id-2 div.sk-toggleable__content pre { margin: 0.2em; border-radius: 0.25em; color: var(--sklearn-color-text); /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-2 div.sk-toggleable__content.fitted pre { /* unfitted */ background-color: var(--sklearn-color-fitted-level-0); } #sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content { /* Expand drop-down */ max-height: 200px; max-width: 100%; overflow: auto; } #sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before { content: \"\u25be\"; } /* Pipeline/ColumnTransformer-specific style */ #sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label { color: var(--sklearn-color-text); background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label { background-color: var(--sklearn-color-fitted-level-2); } /* Estimator-specific style */ /* Colorize estimator box */ #sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label { /* fitted */ background-color: var(--sklearn-color-fitted-level-2); } #sk-container-id-2 div.sk-label label.sk-toggleable__label, #sk-container-id-2 div.sk-label label { /* The background is the default theme color */ color: var(--sklearn-color-text-on-default-background); } /* On hover, darken the color of the background */ #sk-container-id-2 div.sk-label:hover label.sk-toggleable__label { color: var(--sklearn-color-text); background-color: var(--sklearn-color-unfitted-level-2); } /* Label box, darken color on hover, fitted */ #sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted { color: var(--sklearn-color-text); background-color: var(--sklearn-color-fitted-level-2); } /* Estimator label */ #sk-container-id-2 div.sk-label label { font-family: monospace; font-weight: bold; display: inline-block; line-height: 1.2em; } #sk-container-id-2 div.sk-label-container { text-align: center; } /* Estimator-specific */ #sk-container-id-2 div.sk-estimator { font-family: monospace; border: 1px dotted var(--sklearn-color-border-box); border-radius: 0.25em; box-sizing: border-box; margin-bottom: 0.5em; /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-2 div.sk-estimator.fitted { /* fitted */ background-color: var(--sklearn-color-fitted-level-0); } /* on hover */ #sk-container-id-2 div.sk-estimator:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-2 div.sk-estimator.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-2); } /* Specification for estimator info (e.g. \"i\" and \"?\") */ /* Common style for \"i\" and \"?\" */ .sk-estimator-doc-link, a:link.sk-estimator-doc-link, a:visited.sk-estimator-doc-link { float: right; font-size: smaller; line-height: 1em; font-family: monospace; background-color: var(--sklearn-color-background); border-radius: 1em; height: 1em; width: 1em; text-decoration: none !important; margin-left: 1ex; /* unfitted */ border: var(--sklearn-color-unfitted-level-1) 1pt solid; color: var(--sklearn-color-unfitted-level-1); } .sk-estimator-doc-link.fitted, a:link.sk-estimator-doc-link.fitted, a:visited.sk-estimator-doc-link.fitted { /* fitted */ border: var(--sklearn-color-fitted-level-1) 1pt solid; color: var(--sklearn-color-fitted-level-1); } /* On hover */ div.sk-estimator:hover .sk-estimator-doc-link:hover, .sk-estimator-doc-link:hover, div.sk-label-container:hover .sk-estimator-doc-link:hover, .sk-estimator-doc-link:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover, .sk-estimator-doc-link.fitted:hover, div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover, .sk-estimator-doc-link.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } /* Span, style for the box shown on hovering the info icon */ .sk-estimator-doc-link span { display: none; z-index: 9999; position: relative; font-weight: normal; right: .2ex; padding: .5ex; margin: .5ex; width: min-content; min-width: 20ex; max-width: 50ex; color: var(--sklearn-color-text); box-shadow: 2pt 2pt 4pt #999; /* unfitted */ background: var(--sklearn-color-unfitted-level-0); border: .5pt solid var(--sklearn-color-unfitted-level-3); } .sk-estimator-doc-link.fitted span { /* fitted */ background: var(--sklearn-color-fitted-level-0); border: var(--sklearn-color-fitted-level-3); } .sk-estimator-doc-link:hover span { display: block; } /* \"?\"-specific style due to the `<a>` HTML tag */ #sk-container-id-2 a.estimator_doc_link { float: right; font-size: 1rem; line-height: 1em; font-family: monospace; background-color: var(--sklearn-color-background); border-radius: 1rem; height: 1rem; width: 1rem; text-decoration: none; /* unfitted */ color: var(--sklearn-color-unfitted-level-1); border: var(--sklearn-color-unfitted-level-1) 1pt solid; } #sk-container-id-2 a.estimator_doc_link.fitted { /* fitted */ border: var(--sklearn-color-fitted-level-1) 1pt solid; color: var(--sklearn-color-fitted-level-1); } /* On hover */ #sk-container-id-2 a.estimator_doc_link:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } #sk-container-id-2 a.estimator_doc_link.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-3); } NeuralNetwork(algorithm='gradient_descent', clip_max=5, early_stopping=True, hidden_nodes=[2], learning_rate=0.0001, max_attempts=100, max_iters=1000, random_state=3) In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. NeuralNetwork i Fitted NeuralNetwork(algorithm='gradient_descent', clip_max=5, early_stopping=True, hidden_nodes=[2], learning_rate=0.0001, max_attempts=100, max_iters=1000, random_state=3) # Predict labels for train set and assess accuracy y_train_pred = nn_model2 . predict ( X_train_scaled ) y_train_accuracy = accuracy_score ( y_train_hot , y_train_pred ) y_train_accuracy 0.625 # Predict labels for test set and assess accuracy y_test_pred = nn_model2 . predict ( X_test_scaled ) y_test_accuracy = accuracy_score ( y_test_hot , y_test_pred ) y_test_accuracy 0.5666666666666667 Example 7: Fitting a Logistic Regression to the Iris Data # # Initialize logistic regression object and fit object - attempt 1 lr_model1 = mlrose . LogisticRegression ( max_iters = 1000 , learning_rate = 0.0001 , early_stopping = True , clip_max = 5 , max_attempts = 100 , random_state = 3 ) lr_model1 . fit ( X_train_scaled , y_train_hot ) #sk-container-id-3 { /* Definition of color scheme common for light and dark mode */ --sklearn-color-text: black; --sklearn-color-line: gray; /* Definition of color scheme for unfitted estimators */ --sklearn-color-unfitted-level-0: #fff5e6; --sklearn-color-unfitted-level-1: #f6e4d2; --sklearn-color-unfitted-level-2: #ffe0b3; --sklearn-color-unfitted-level-3: chocolate; /* Definition of color scheme for fitted estimators */ --sklearn-color-fitted-level-0: #f0f8ff; --sklearn-color-fitted-level-1: #d4ebff; --sklearn-color-fitted-level-2: #b3dbfd; --sklearn-color-fitted-level-3: cornflowerblue; /* Specific color for light theme */ --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black))); --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white))); --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black))); --sklearn-color-icon: #696969; @media (prefers-color-scheme: dark) { /* Redefinition of color scheme for dark theme */ --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white))); --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111))); --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white))); --sklearn-color-icon: #878787; } } #sk-container-id-3 { color: var(--sklearn-color-text); } #sk-container-id-3 pre { padding: 0; } #sk-container-id-3 input.sk-hidden--visually { border: 0; clip: rect(1px 1px 1px 1px); clip: rect(1px, 1px, 1px, 1px); height: 1px; margin: -1px; overflow: hidden; padding: 0; position: absolute; width: 1px; } #sk-container-id-3 div.sk-dashed-wrapped { border: 1px dashed var(--sklearn-color-line); margin: 0 0.4em 0.5em 0.4em; box-sizing: border-box; padding-bottom: 0.4em; background-color: var(--sklearn-color-background); } #sk-container-id-3 div.sk-container { /* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */ display: inline-block !important; position: relative; } #sk-container-id-3 div.sk-text-repr-fallback { display: none; } div.sk-parallel-item, div.sk-serial, div.sk-item { /* draw centered vertical line to link estimators */ background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background)); background-size: 2px 100%; background-repeat: no-repeat; background-position: center center; } /* Parallel-specific style estimator block */ #sk-container-id-3 div.sk-parallel-item::after { content: \"\"; width: 100%; border-bottom: 2px solid var(--sklearn-color-text-on-default-background); flex-grow: 1; } #sk-container-id-3 div.sk-parallel { display: flex; align-items: stretch; justify-content: center; background-color: var(--sklearn-color-background); position: relative; } #sk-container-id-3 div.sk-parallel-item { display: flex; flex-direction: column; } #sk-container-id-3 div.sk-parallel-item:first-child::after { align-self: flex-end; width: 50%; } #sk-container-id-3 div.sk-parallel-item:last-child::after { align-self: flex-start; width: 50%; } #sk-container-id-3 div.sk-parallel-item:only-child::after { width: 0; } /* Serial-specific style estimator block */ #sk-container-id-3 div.sk-serial { display: flex; flex-direction: column; align-items: center; background-color: var(--sklearn-color-background); padding-right: 1em; padding-left: 1em; } /* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is clickable and can be expanded/collapsed. - Pipeline and ColumnTransformer use this feature and define the default style - Estimators will overwrite some part of the style using the `sk-estimator` class */ /* Pipeline and ColumnTransformer style (default) */ #sk-container-id-3 div.sk-toggleable { /* Default theme specific background. It is overwritten whether we have a specific estimator or a Pipeline/ColumnTransformer */ background-color: var(--sklearn-color-background); } /* Toggleable label */ #sk-container-id-3 label.sk-toggleable__label { cursor: pointer; display: block; width: 100%; margin-bottom: 0; padding: 0.5em; box-sizing: border-box; text-align: center; } #sk-container-id-3 label.sk-toggleable__label-arrow:before { /* Arrow on the left of the label */ content: \"\u25b8\"; float: left; margin-right: 0.25em; color: var(--sklearn-color-icon); } #sk-container-id-3 label.sk-toggleable__label-arrow:hover:before { color: var(--sklearn-color-text); } /* Toggleable content - dropdown */ #sk-container-id-3 div.sk-toggleable__content { max-height: 0; max-width: 0; overflow: hidden; text-align: left; /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-3 div.sk-toggleable__content.fitted { /* fitted */ background-color: var(--sklearn-color-fitted-level-0); } #sk-container-id-3 div.sk-toggleable__content pre { margin: 0.2em; border-radius: 0.25em; color: var(--sklearn-color-text); /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-3 div.sk-toggleable__content.fitted pre { /* unfitted */ background-color: var(--sklearn-color-fitted-level-0); } #sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content { /* Expand drop-down */ max-height: 200px; max-width: 100%; overflow: auto; } #sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before { content: \"\u25be\"; } /* Pipeline/ColumnTransformer-specific style */ #sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label { color: var(--sklearn-color-text); background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label { background-color: var(--sklearn-color-fitted-level-2); } /* Estimator-specific style */ /* Colorize estimator box */ #sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label { /* fitted */ background-color: var(--sklearn-color-fitted-level-2); } #sk-container-id-3 div.sk-label label.sk-toggleable__label, #sk-container-id-3 div.sk-label label { /* The background is the default theme color */ color: var(--sklearn-color-text-on-default-background); } /* On hover, darken the color of the background */ #sk-container-id-3 div.sk-label:hover label.sk-toggleable__label { color: var(--sklearn-color-text); background-color: var(--sklearn-color-unfitted-level-2); } /* Label box, darken color on hover, fitted */ #sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted { color: var(--sklearn-color-text); background-color: var(--sklearn-color-fitted-level-2); } /* Estimator label */ #sk-container-id-3 div.sk-label label { font-family: monospace; font-weight: bold; display: inline-block; line-height: 1.2em; } #sk-container-id-3 div.sk-label-container { text-align: center; } /* Estimator-specific */ #sk-container-id-3 div.sk-estimator { font-family: monospace; border: 1px dotted var(--sklearn-color-border-box); border-radius: 0.25em; box-sizing: border-box; margin-bottom: 0.5em; /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-3 div.sk-estimator.fitted { /* fitted */ background-color: var(--sklearn-color-fitted-level-0); } /* on hover */ #sk-container-id-3 div.sk-estimator:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-3 div.sk-estimator.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-2); } /* Specification for estimator info (e.g. \"i\" and \"?\") */ /* Common style for \"i\" and \"?\" */ .sk-estimator-doc-link, a:link.sk-estimator-doc-link, a:visited.sk-estimator-doc-link { float: right; font-size: smaller; line-height: 1em; font-family: monospace; background-color: var(--sklearn-color-background); border-radius: 1em; height: 1em; width: 1em; text-decoration: none !important; margin-left: 1ex; /* unfitted */ border: var(--sklearn-color-unfitted-level-1) 1pt solid; color: var(--sklearn-color-unfitted-level-1); } .sk-estimator-doc-link.fitted, a:link.sk-estimator-doc-link.fitted, a:visited.sk-estimator-doc-link.fitted { /* fitted */ border: var(--sklearn-color-fitted-level-1) 1pt solid; color: var(--sklearn-color-fitted-level-1); } /* On hover */ div.sk-estimator:hover .sk-estimator-doc-link:hover, .sk-estimator-doc-link:hover, div.sk-label-container:hover .sk-estimator-doc-link:hover, .sk-estimator-doc-link:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover, .sk-estimator-doc-link.fitted:hover, div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover, .sk-estimator-doc-link.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } /* Span, style for the box shown on hovering the info icon */ .sk-estimator-doc-link span { display: none; z-index: 9999; position: relative; font-weight: normal; right: .2ex; padding: .5ex; margin: .5ex; width: min-content; min-width: 20ex; max-width: 50ex; color: var(--sklearn-color-text); box-shadow: 2pt 2pt 4pt #999; /* unfitted */ background: var(--sklearn-color-unfitted-level-0); border: .5pt solid var(--sklearn-color-unfitted-level-3); } .sk-estimator-doc-link.fitted span { /* fitted */ background: var(--sklearn-color-fitted-level-0); border: var(--sklearn-color-fitted-level-3); } .sk-estimator-doc-link:hover span { display: block; } /* \"?\"-specific style due to the `<a>` HTML tag */ #sk-container-id-3 a.estimator_doc_link { float: right; font-size: 1rem; line-height: 1em; font-family: monospace; background-color: var(--sklearn-color-background); border-radius: 1rem; height: 1rem; width: 1rem; text-decoration: none; /* unfitted */ color: var(--sklearn-color-unfitted-level-1); border: var(--sklearn-color-unfitted-level-1) 1pt solid; } #sk-container-id-3 a.estimator_doc_link.fitted { /* fitted */ border: var(--sklearn-color-fitted-level-1) 1pt solid; color: var(--sklearn-color-fitted-level-1); } /* On hover */ #sk-container-id-3 a.estimator_doc_link:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } #sk-container-id-3 a.estimator_doc_link.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-3); } LogisticRegression(clip_max=5, early_stopping=True, learning_rate=0.0001, max_attempts=100, max_iters=1000, random_state=3) In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. LogisticRegression i Fitted LogisticRegression(clip_max=5, early_stopping=True, learning_rate=0.0001, max_attempts=100, max_iters=1000, random_state=3) # Predict labels for train set and assess accuracy y_train_pred = lr_model1 . predict ( X_train_scaled ) y_train_accuracy = accuracy_score ( y_train_hot , y_train_pred ) y_train_accuracy 0.19166666666666668 # Predict labels for test set and assess accuracy y_test_pred = lr_model1 . predict ( X_test_scaled ) y_test_accuracy = accuracy_score ( y_test_hot , y_test_pred ) y_test_accuracy 0.06666666666666667 # Initialize logistic regression object and fit object - attempt 2 lr_model2 = mlrose . LogisticRegression ( max_iters = 1000 , learning_rate = 0.01 , early_stopping = True , clip_max = 5 , max_attempts = 100 , random_state = 3 ) lr_model2 . fit ( X_train_scaled , y_train_hot ) #sk-container-id-4 { /* Definition of color scheme common for light and dark mode */ --sklearn-color-text: black; --sklearn-color-line: gray; /* Definition of color scheme for unfitted estimators */ --sklearn-color-unfitted-level-0: #fff5e6; --sklearn-color-unfitted-level-1: #f6e4d2; --sklearn-color-unfitted-level-2: #ffe0b3; --sklearn-color-unfitted-level-3: chocolate; /* Definition of color scheme for fitted estimators */ --sklearn-color-fitted-level-0: #f0f8ff; --sklearn-color-fitted-level-1: #d4ebff; --sklearn-color-fitted-level-2: #b3dbfd; --sklearn-color-fitted-level-3: cornflowerblue; /* Specific color for light theme */ --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black))); --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white))); --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black))); --sklearn-color-icon: #696969; @media (prefers-color-scheme: dark) { /* Redefinition of color scheme for dark theme */ --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white))); --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111))); --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white))); --sklearn-color-icon: #878787; } } #sk-container-id-4 { color: var(--sklearn-color-text); } #sk-container-id-4 pre { padding: 0; } #sk-container-id-4 input.sk-hidden--visually { border: 0; clip: rect(1px 1px 1px 1px); clip: rect(1px, 1px, 1px, 1px); height: 1px; margin: -1px; overflow: hidden; padding: 0; position: absolute; width: 1px; } #sk-container-id-4 div.sk-dashed-wrapped { border: 1px dashed var(--sklearn-color-line); margin: 0 0.4em 0.5em 0.4em; box-sizing: border-box; padding-bottom: 0.4em; background-color: var(--sklearn-color-background); } #sk-container-id-4 div.sk-container { /* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */ display: inline-block !important; position: relative; } #sk-container-id-4 div.sk-text-repr-fallback { display: none; } div.sk-parallel-item, div.sk-serial, div.sk-item { /* draw centered vertical line to link estimators */ background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background)); background-size: 2px 100%; background-repeat: no-repeat; background-position: center center; } /* Parallel-specific style estimator block */ #sk-container-id-4 div.sk-parallel-item::after { content: \"\"; width: 100%; border-bottom: 2px solid var(--sklearn-color-text-on-default-background); flex-grow: 1; } #sk-container-id-4 div.sk-parallel { display: flex; align-items: stretch; justify-content: center; background-color: var(--sklearn-color-background); position: relative; } #sk-container-id-4 div.sk-parallel-item { display: flex; flex-direction: column; } #sk-container-id-4 div.sk-parallel-item:first-child::after { align-self: flex-end; width: 50%; } #sk-container-id-4 div.sk-parallel-item:last-child::after { align-self: flex-start; width: 50%; } #sk-container-id-4 div.sk-parallel-item:only-child::after { width: 0; } /* Serial-specific style estimator block */ #sk-container-id-4 div.sk-serial { display: flex; flex-direction: column; align-items: center; background-color: var(--sklearn-color-background); padding-right: 1em; padding-left: 1em; } /* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is clickable and can be expanded/collapsed. - Pipeline and ColumnTransformer use this feature and define the default style - Estimators will overwrite some part of the style using the `sk-estimator` class */ /* Pipeline and ColumnTransformer style (default) */ #sk-container-id-4 div.sk-toggleable { /* Default theme specific background. It is overwritten whether we have a specific estimator or a Pipeline/ColumnTransformer */ background-color: var(--sklearn-color-background); } /* Toggleable label */ #sk-container-id-4 label.sk-toggleable__label { cursor: pointer; display: block; width: 100%; margin-bottom: 0; padding: 0.5em; box-sizing: border-box; text-align: center; } #sk-container-id-4 label.sk-toggleable__label-arrow:before { /* Arrow on the left of the label */ content: \"\u25b8\"; float: left; margin-right: 0.25em; color: var(--sklearn-color-icon); } #sk-container-id-4 label.sk-toggleable__label-arrow:hover:before { color: var(--sklearn-color-text); } /* Toggleable content - dropdown */ #sk-container-id-4 div.sk-toggleable__content { max-height: 0; max-width: 0; overflow: hidden; text-align: left; /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-4 div.sk-toggleable__content.fitted { /* fitted */ background-color: var(--sklearn-color-fitted-level-0); } #sk-container-id-4 div.sk-toggleable__content pre { margin: 0.2em; border-radius: 0.25em; color: var(--sklearn-color-text); /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-4 div.sk-toggleable__content.fitted pre { /* unfitted */ background-color: var(--sklearn-color-fitted-level-0); } #sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content { /* Expand drop-down */ max-height: 200px; max-width: 100%; overflow: auto; } #sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before { content: \"\u25be\"; } /* Pipeline/ColumnTransformer-specific style */ #sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label { color: var(--sklearn-color-text); background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label { background-color: var(--sklearn-color-fitted-level-2); } /* Estimator-specific style */ /* Colorize estimator box */ #sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label { /* fitted */ background-color: var(--sklearn-color-fitted-level-2); } #sk-container-id-4 div.sk-label label.sk-toggleable__label, #sk-container-id-4 div.sk-label label { /* The background is the default theme color */ color: var(--sklearn-color-text-on-default-background); } /* On hover, darken the color of the background */ #sk-container-id-4 div.sk-label:hover label.sk-toggleable__label { color: var(--sklearn-color-text); background-color: var(--sklearn-color-unfitted-level-2); } /* Label box, darken color on hover, fitted */ #sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted { color: var(--sklearn-color-text); background-color: var(--sklearn-color-fitted-level-2); } /* Estimator label */ #sk-container-id-4 div.sk-label label { font-family: monospace; font-weight: bold; display: inline-block; line-height: 1.2em; } #sk-container-id-4 div.sk-label-container { text-align: center; } /* Estimator-specific */ #sk-container-id-4 div.sk-estimator { font-family: monospace; border: 1px dotted var(--sklearn-color-border-box); border-radius: 0.25em; box-sizing: border-box; margin-bottom: 0.5em; /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-4 div.sk-estimator.fitted { /* fitted */ background-color: var(--sklearn-color-fitted-level-0); } /* on hover */ #sk-container-id-4 div.sk-estimator:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-4 div.sk-estimator.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-2); } /* Specification for estimator info (e.g. \"i\" and \"?\") */ /* Common style for \"i\" and \"?\" */ .sk-estimator-doc-link, a:link.sk-estimator-doc-link, a:visited.sk-estimator-doc-link { float: right; font-size: smaller; line-height: 1em; font-family: monospace; background-color: var(--sklearn-color-background); border-radius: 1em; height: 1em; width: 1em; text-decoration: none !important; margin-left: 1ex; /* unfitted */ border: var(--sklearn-color-unfitted-level-1) 1pt solid; color: var(--sklearn-color-unfitted-level-1); } .sk-estimator-doc-link.fitted, a:link.sk-estimator-doc-link.fitted, a:visited.sk-estimator-doc-link.fitted { /* fitted */ border: var(--sklearn-color-fitted-level-1) 1pt solid; color: var(--sklearn-color-fitted-level-1); } /* On hover */ div.sk-estimator:hover .sk-estimator-doc-link:hover, .sk-estimator-doc-link:hover, div.sk-label-container:hover .sk-estimator-doc-link:hover, .sk-estimator-doc-link:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover, .sk-estimator-doc-link.fitted:hover, div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover, .sk-estimator-doc-link.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } /* Span, style for the box shown on hovering the info icon */ .sk-estimator-doc-link span { display: none; z-index: 9999; position: relative; font-weight: normal; right: .2ex; padding: .5ex; margin: .5ex; width: min-content; min-width: 20ex; max-width: 50ex; color: var(--sklearn-color-text); box-shadow: 2pt 2pt 4pt #999; /* unfitted */ background: var(--sklearn-color-unfitted-level-0); border: .5pt solid var(--sklearn-color-unfitted-level-3); } .sk-estimator-doc-link.fitted span { /* fitted */ background: var(--sklearn-color-fitted-level-0); border: var(--sklearn-color-fitted-level-3); } .sk-estimator-doc-link:hover span { display: block; } /* \"?\"-specific style due to the `<a>` HTML tag */ #sk-container-id-4 a.estimator_doc_link { float: right; font-size: 1rem; line-height: 1em; font-family: monospace; background-color: var(--sklearn-color-background); border-radius: 1rem; height: 1rem; width: 1rem; text-decoration: none; /* unfitted */ color: var(--sklearn-color-unfitted-level-1); border: var(--sklearn-color-unfitted-level-1) 1pt solid; } #sk-container-id-4 a.estimator_doc_link.fitted { /* fitted */ border: var(--sklearn-color-fitted-level-1) 1pt solid; color: var(--sklearn-color-fitted-level-1); } /* On hover */ #sk-container-id-4 a.estimator_doc_link:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } #sk-container-id-4 a.estimator_doc_link.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-3); } LogisticRegression(clip_max=5, early_stopping=True, learning_rate=0.01, max_attempts=100, max_iters=1000, random_state=3) In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. LogisticRegression i Fitted LogisticRegression(clip_max=5, early_stopping=True, learning_rate=0.01, max_attempts=100, max_iters=1000, random_state=3) # Predict labels for train set and assess accuracy y_train_pred = lr_model2 . predict ( X_train_scaled ) y_train_accuracy = accuracy_score ( y_train_hot , y_train_pred ) y_train_accuracy 0.6833333333333333 # Predict labels for test set and assess accuracy y_test_pred = lr_model2 . predict ( X_test_scaled ) y_test_accuracy = accuracy_score ( y_test_hot , y_test_pred ) y_test_accuracy 0.7 Example 8: Fitting a Logistic Regression to the Iris Data using the NeuralNetwork() class # # Initialize neural network object and fit object - attempt 1 lr_nn_model1 = mlrose . NeuralNetwork ( hidden_nodes = [], activation = \"sigmoid\" , max_iters = 1000 , learning_rate = 0.0001 , early_stopping = True , clip_max = 5 , max_attempts = 100 , random_state = 3 ) lr_nn_model1 . fit ( X_train_scaled , y_train_hot ) #sk-container-id-5 { /* Definition of color scheme common for light and dark mode */ --sklearn-color-text: black; --sklearn-color-line: gray; /* Definition of color scheme for unfitted estimators */ --sklearn-color-unfitted-level-0: #fff5e6; --sklearn-color-unfitted-level-1: #f6e4d2; --sklearn-color-unfitted-level-2: #ffe0b3; --sklearn-color-unfitted-level-3: chocolate; /* Definition of color scheme for fitted estimators */ --sklearn-color-fitted-level-0: #f0f8ff; --sklearn-color-fitted-level-1: #d4ebff; --sklearn-color-fitted-level-2: #b3dbfd; --sklearn-color-fitted-level-3: cornflowerblue; /* Specific color for light theme */ --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black))); --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white))); --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black))); --sklearn-color-icon: #696969; @media (prefers-color-scheme: dark) { /* Redefinition of color scheme for dark theme */ --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white))); --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111))); --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white))); --sklearn-color-icon: #878787; } } #sk-container-id-5 { color: var(--sklearn-color-text); } #sk-container-id-5 pre { padding: 0; } #sk-container-id-5 input.sk-hidden--visually { border: 0; clip: rect(1px 1px 1px 1px); clip: rect(1px, 1px, 1px, 1px); height: 1px; margin: -1px; overflow: hidden; padding: 0; position: absolute; width: 1px; } #sk-container-id-5 div.sk-dashed-wrapped { border: 1px dashed var(--sklearn-color-line); margin: 0 0.4em 0.5em 0.4em; box-sizing: border-box; padding-bottom: 0.4em; background-color: var(--sklearn-color-background); } #sk-container-id-5 div.sk-container { /* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */ display: inline-block !important; position: relative; } #sk-container-id-5 div.sk-text-repr-fallback { display: none; } div.sk-parallel-item, div.sk-serial, div.sk-item { /* draw centered vertical line to link estimators */ background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background)); background-size: 2px 100%; background-repeat: no-repeat; background-position: center center; } /* Parallel-specific style estimator block */ #sk-container-id-5 div.sk-parallel-item::after { content: \"\"; width: 100%; border-bottom: 2px solid var(--sklearn-color-text-on-default-background); flex-grow: 1; } #sk-container-id-5 div.sk-parallel { display: flex; align-items: stretch; justify-content: center; background-color: var(--sklearn-color-background); position: relative; } #sk-container-id-5 div.sk-parallel-item { display: flex; flex-direction: column; } #sk-container-id-5 div.sk-parallel-item:first-child::after { align-self: flex-end; width: 50%; } #sk-container-id-5 div.sk-parallel-item:last-child::after { align-self: flex-start; width: 50%; } #sk-container-id-5 div.sk-parallel-item:only-child::after { width: 0; } /* Serial-specific style estimator block */ #sk-container-id-5 div.sk-serial { display: flex; flex-direction: column; align-items: center; background-color: var(--sklearn-color-background); padding-right: 1em; padding-left: 1em; } /* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is clickable and can be expanded/collapsed. - Pipeline and ColumnTransformer use this feature and define the default style - Estimators will overwrite some part of the style using the `sk-estimator` class */ /* Pipeline and ColumnTransformer style (default) */ #sk-container-id-5 div.sk-toggleable { /* Default theme specific background. It is overwritten whether we have a specific estimator or a Pipeline/ColumnTransformer */ background-color: var(--sklearn-color-background); } /* Toggleable label */ #sk-container-id-5 label.sk-toggleable__label { cursor: pointer; display: block; width: 100%; margin-bottom: 0; padding: 0.5em; box-sizing: border-box; text-align: center; } #sk-container-id-5 label.sk-toggleable__label-arrow:before { /* Arrow on the left of the label */ content: \"\u25b8\"; float: left; margin-right: 0.25em; color: var(--sklearn-color-icon); } #sk-container-id-5 label.sk-toggleable__label-arrow:hover:before { color: var(--sklearn-color-text); } /* Toggleable content - dropdown */ #sk-container-id-5 div.sk-toggleable__content { max-height: 0; max-width: 0; overflow: hidden; text-align: left; /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-5 div.sk-toggleable__content.fitted { /* fitted */ background-color: var(--sklearn-color-fitted-level-0); } #sk-container-id-5 div.sk-toggleable__content pre { margin: 0.2em; border-radius: 0.25em; color: var(--sklearn-color-text); /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-5 div.sk-toggleable__content.fitted pre { /* unfitted */ background-color: var(--sklearn-color-fitted-level-0); } #sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content { /* Expand drop-down */ max-height: 200px; max-width: 100%; overflow: auto; } #sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before { content: \"\u25be\"; } /* Pipeline/ColumnTransformer-specific style */ #sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label { color: var(--sklearn-color-text); background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-5 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label { background-color: var(--sklearn-color-fitted-level-2); } /* Estimator-specific style */ /* Colorize estimator box */ #sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-5 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label { /* fitted */ background-color: var(--sklearn-color-fitted-level-2); } #sk-container-id-5 div.sk-label label.sk-toggleable__label, #sk-container-id-5 div.sk-label label { /* The background is the default theme color */ color: var(--sklearn-color-text-on-default-background); } /* On hover, darken the color of the background */ #sk-container-id-5 div.sk-label:hover label.sk-toggleable__label { color: var(--sklearn-color-text); background-color: var(--sklearn-color-unfitted-level-2); } /* Label box, darken color on hover, fitted */ #sk-container-id-5 div.sk-label.fitted:hover label.sk-toggleable__label.fitted { color: var(--sklearn-color-text); background-color: var(--sklearn-color-fitted-level-2); } /* Estimator label */ #sk-container-id-5 div.sk-label label { font-family: monospace; font-weight: bold; display: inline-block; line-height: 1.2em; } #sk-container-id-5 div.sk-label-container { text-align: center; } /* Estimator-specific */ #sk-container-id-5 div.sk-estimator { font-family: monospace; border: 1px dotted var(--sklearn-color-border-box); border-radius: 0.25em; box-sizing: border-box; margin-bottom: 0.5em; /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-5 div.sk-estimator.fitted { /* fitted */ background-color: var(--sklearn-color-fitted-level-0); } /* on hover */ #sk-container-id-5 div.sk-estimator:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-5 div.sk-estimator.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-2); } /* Specification for estimator info (e.g. \"i\" and \"?\") */ /* Common style for \"i\" and \"?\" */ .sk-estimator-doc-link, a:link.sk-estimator-doc-link, a:visited.sk-estimator-doc-link { float: right; font-size: smaller; line-height: 1em; font-family: monospace; background-color: var(--sklearn-color-background); border-radius: 1em; height: 1em; width: 1em; text-decoration: none !important; margin-left: 1ex; /* unfitted */ border: var(--sklearn-color-unfitted-level-1) 1pt solid; color: var(--sklearn-color-unfitted-level-1); } .sk-estimator-doc-link.fitted, a:link.sk-estimator-doc-link.fitted, a:visited.sk-estimator-doc-link.fitted { /* fitted */ border: var(--sklearn-color-fitted-level-1) 1pt solid; color: var(--sklearn-color-fitted-level-1); } /* On hover */ div.sk-estimator:hover .sk-estimator-doc-link:hover, .sk-estimator-doc-link:hover, div.sk-label-container:hover .sk-estimator-doc-link:hover, .sk-estimator-doc-link:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover, .sk-estimator-doc-link.fitted:hover, div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover, .sk-estimator-doc-link.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } /* Span, style for the box shown on hovering the info icon */ .sk-estimator-doc-link span { display: none; z-index: 9999; position: relative; font-weight: normal; right: .2ex; padding: .5ex; margin: .5ex; width: min-content; min-width: 20ex; max-width: 50ex; color: var(--sklearn-color-text); box-shadow: 2pt 2pt 4pt #999; /* unfitted */ background: var(--sklearn-color-unfitted-level-0); border: .5pt solid var(--sklearn-color-unfitted-level-3); } .sk-estimator-doc-link.fitted span { /* fitted */ background: var(--sklearn-color-fitted-level-0); border: var(--sklearn-color-fitted-level-3); } .sk-estimator-doc-link:hover span { display: block; } /* \"?\"-specific style due to the `<a>` HTML tag */ #sk-container-id-5 a.estimator_doc_link { float: right; font-size: 1rem; line-height: 1em; font-family: monospace; background-color: var(--sklearn-color-background); border-radius: 1rem; height: 1rem; width: 1rem; text-decoration: none; /* unfitted */ color: var(--sklearn-color-unfitted-level-1); border: var(--sklearn-color-unfitted-level-1) 1pt solid; } #sk-container-id-5 a.estimator_doc_link.fitted { /* fitted */ border: var(--sklearn-color-fitted-level-1) 1pt solid; color: var(--sklearn-color-fitted-level-1); } /* On hover */ #sk-container-id-5 a.estimator_doc_link:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } #sk-container-id-5 a.estimator_doc_link.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-3); } NeuralNetwork(activation='sigmoid', clip_max=5, early_stopping=True, hidden_nodes=[], learning_rate=0.0001, max_attempts=100, max_iters=1000, random_state=3) In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. NeuralNetwork i Fitted NeuralNetwork(activation='sigmoid', clip_max=5, early_stopping=True, hidden_nodes=[], learning_rate=0.0001, max_attempts=100, max_iters=1000, random_state=3) # Predict labels for train set and assess accuracy y_train_pred = lr_nn_model1 . predict ( X_train_scaled ) y_train_accuracy = accuracy_score ( y_train_hot , y_train_pred ) y_train_accuracy 0.19166666666666668 # Predict labels for test set and assess accuracy y_test_pred = lr_nn_model1 . predict ( X_test_scaled ) y_test_accuracy = accuracy_score ( y_test_hot , y_test_pred ) y_test_accuracy 0.06666666666666667 # Initialize neural network object and fit object - attempt 2 lr_nn_model2 = mlrose . NeuralNetwork ( hidden_nodes = [], activation = \"sigmoid\" , max_iters = 1000 , learning_rate = 0.01 , early_stopping = True , clip_max = 5 , max_attempts = 100 , random_state = 3 ) lr_nn_model2 . fit ( X_train_scaled , y_train_hot ) #sk-container-id-6 { /* Definition of color scheme common for light and dark mode */ --sklearn-color-text: black; --sklearn-color-line: gray; /* Definition of color scheme for unfitted estimators */ --sklearn-color-unfitted-level-0: #fff5e6; --sklearn-color-unfitted-level-1: #f6e4d2; --sklearn-color-unfitted-level-2: #ffe0b3; --sklearn-color-unfitted-level-3: chocolate; /* Definition of color scheme for fitted estimators */ --sklearn-color-fitted-level-0: #f0f8ff; --sklearn-color-fitted-level-1: #d4ebff; --sklearn-color-fitted-level-2: #b3dbfd; --sklearn-color-fitted-level-3: cornflowerblue; /* Specific color for light theme */ --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black))); --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white))); --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black))); --sklearn-color-icon: #696969; @media (prefers-color-scheme: dark) { /* Redefinition of color scheme for dark theme */ --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white))); --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111))); --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white))); --sklearn-color-icon: #878787; } } #sk-container-id-6 { color: var(--sklearn-color-text); } #sk-container-id-6 pre { padding: 0; } #sk-container-id-6 input.sk-hidden--visually { border: 0; clip: rect(1px 1px 1px 1px); clip: rect(1px, 1px, 1px, 1px); height: 1px; margin: -1px; overflow: hidden; padding: 0; position: absolute; width: 1px; } #sk-container-id-6 div.sk-dashed-wrapped { border: 1px dashed var(--sklearn-color-line); margin: 0 0.4em 0.5em 0.4em; box-sizing: border-box; padding-bottom: 0.4em; background-color: var(--sklearn-color-background); } #sk-container-id-6 div.sk-container { /* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */ display: inline-block !important; position: relative; } #sk-container-id-6 div.sk-text-repr-fallback { display: none; } div.sk-parallel-item, div.sk-serial, div.sk-item { /* draw centered vertical line to link estimators */ background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background)); background-size: 2px 100%; background-repeat: no-repeat; background-position: center center; } /* Parallel-specific style estimator block */ #sk-container-id-6 div.sk-parallel-item::after { content: \"\"; width: 100%; border-bottom: 2px solid var(--sklearn-color-text-on-default-background); flex-grow: 1; } #sk-container-id-6 div.sk-parallel { display: flex; align-items: stretch; justify-content: center; background-color: var(--sklearn-color-background); position: relative; } #sk-container-id-6 div.sk-parallel-item { display: flex; flex-direction: column; } #sk-container-id-6 div.sk-parallel-item:first-child::after { align-self: flex-end; width: 50%; } #sk-container-id-6 div.sk-parallel-item:last-child::after { align-self: flex-start; width: 50%; } #sk-container-id-6 div.sk-parallel-item:only-child::after { width: 0; } /* Serial-specific style estimator block */ #sk-container-id-6 div.sk-serial { display: flex; flex-direction: column; align-items: center; background-color: var(--sklearn-color-background); padding-right: 1em; padding-left: 1em; } /* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is clickable and can be expanded/collapsed. - Pipeline and ColumnTransformer use this feature and define the default style - Estimators will overwrite some part of the style using the `sk-estimator` class */ /* Pipeline and ColumnTransformer style (default) */ #sk-container-id-6 div.sk-toggleable { /* Default theme specific background. It is overwritten whether we have a specific estimator or a Pipeline/ColumnTransformer */ background-color: var(--sklearn-color-background); } /* Toggleable label */ #sk-container-id-6 label.sk-toggleable__label { cursor: pointer; display: block; width: 100%; margin-bottom: 0; padding: 0.5em; box-sizing: border-box; text-align: center; } #sk-container-id-6 label.sk-toggleable__label-arrow:before { /* Arrow on the left of the label */ content: \"\u25b8\"; float: left; margin-right: 0.25em; color: var(--sklearn-color-icon); } #sk-container-id-6 label.sk-toggleable__label-arrow:hover:before { color: var(--sklearn-color-text); } /* Toggleable content - dropdown */ #sk-container-id-6 div.sk-toggleable__content { max-height: 0; max-width: 0; overflow: hidden; text-align: left; /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-6 div.sk-toggleable__content.fitted { /* fitted */ background-color: var(--sklearn-color-fitted-level-0); } #sk-container-id-6 div.sk-toggleable__content pre { margin: 0.2em; border-radius: 0.25em; color: var(--sklearn-color-text); /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-6 div.sk-toggleable__content.fitted pre { /* unfitted */ background-color: var(--sklearn-color-fitted-level-0); } #sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content { /* Expand drop-down */ max-height: 200px; max-width: 100%; overflow: auto; } #sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before { content: \"\u25be\"; } /* Pipeline/ColumnTransformer-specific style */ #sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label { color: var(--sklearn-color-text); background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-6 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label { background-color: var(--sklearn-color-fitted-level-2); } /* Estimator-specific style */ /* Colorize estimator box */ #sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-6 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label { /* fitted */ background-color: var(--sklearn-color-fitted-level-2); } #sk-container-id-6 div.sk-label label.sk-toggleable__label, #sk-container-id-6 div.sk-label label { /* The background is the default theme color */ color: var(--sklearn-color-text-on-default-background); } /* On hover, darken the color of the background */ #sk-container-id-6 div.sk-label:hover label.sk-toggleable__label { color: var(--sklearn-color-text); background-color: var(--sklearn-color-unfitted-level-2); } /* Label box, darken color on hover, fitted */ #sk-container-id-6 div.sk-label.fitted:hover label.sk-toggleable__label.fitted { color: var(--sklearn-color-text); background-color: var(--sklearn-color-fitted-level-2); } /* Estimator label */ #sk-container-id-6 div.sk-label label { font-family: monospace; font-weight: bold; display: inline-block; line-height: 1.2em; } #sk-container-id-6 div.sk-label-container { text-align: center; } /* Estimator-specific */ #sk-container-id-6 div.sk-estimator { font-family: monospace; border: 1px dotted var(--sklearn-color-border-box); border-radius: 0.25em; box-sizing: border-box; margin-bottom: 0.5em; /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-6 div.sk-estimator.fitted { /* fitted */ background-color: var(--sklearn-color-fitted-level-0); } /* on hover */ #sk-container-id-6 div.sk-estimator:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-6 div.sk-estimator.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-2); } /* Specification for estimator info (e.g. \"i\" and \"?\") */ /* Common style for \"i\" and \"?\" */ .sk-estimator-doc-link, a:link.sk-estimator-doc-link, a:visited.sk-estimator-doc-link { float: right; font-size: smaller; line-height: 1em; font-family: monospace; background-color: var(--sklearn-color-background); border-radius: 1em; height: 1em; width: 1em; text-decoration: none !important; margin-left: 1ex; /* unfitted */ border: var(--sklearn-color-unfitted-level-1) 1pt solid; color: var(--sklearn-color-unfitted-level-1); } .sk-estimator-doc-link.fitted, a:link.sk-estimator-doc-link.fitted, a:visited.sk-estimator-doc-link.fitted { /* fitted */ border: var(--sklearn-color-fitted-level-1) 1pt solid; color: var(--sklearn-color-fitted-level-1); } /* On hover */ div.sk-estimator:hover .sk-estimator-doc-link:hover, .sk-estimator-doc-link:hover, div.sk-label-container:hover .sk-estimator-doc-link:hover, .sk-estimator-doc-link:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover, .sk-estimator-doc-link.fitted:hover, div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover, .sk-estimator-doc-link.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } /* Span, style for the box shown on hovering the info icon */ .sk-estimator-doc-link span { display: none; z-index: 9999; position: relative; font-weight: normal; right: .2ex; padding: .5ex; margin: .5ex; width: min-content; min-width: 20ex; max-width: 50ex; color: var(--sklearn-color-text); box-shadow: 2pt 2pt 4pt #999; /* unfitted */ background: var(--sklearn-color-unfitted-level-0); border: .5pt solid var(--sklearn-color-unfitted-level-3); } .sk-estimator-doc-link.fitted span { /* fitted */ background: var(--sklearn-color-fitted-level-0); border: var(--sklearn-color-fitted-level-3); } .sk-estimator-doc-link:hover span { display: block; } /* \"?\"-specific style due to the `<a>` HTML tag */ #sk-container-id-6 a.estimator_doc_link { float: right; font-size: 1rem; line-height: 1em; font-family: monospace; background-color: var(--sklearn-color-background); border-radius: 1rem; height: 1rem; width: 1rem; text-decoration: none; /* unfitted */ color: var(--sklearn-color-unfitted-level-1); border: var(--sklearn-color-unfitted-level-1) 1pt solid; } #sk-container-id-6 a.estimator_doc_link.fitted { /* fitted */ border: var(--sklearn-color-fitted-level-1) 1pt solid; color: var(--sklearn-color-fitted-level-1); } /* On hover */ #sk-container-id-6 a.estimator_doc_link:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } #sk-container-id-6 a.estimator_doc_link.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-3); } NeuralNetwork(activation='sigmoid', clip_max=5, early_stopping=True, hidden_nodes=[], learning_rate=0.01, max_attempts=100, max_iters=1000, random_state=3) In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. NeuralNetwork i Fitted NeuralNetwork(activation='sigmoid', clip_max=5, early_stopping=True, hidden_nodes=[], learning_rate=0.01, max_attempts=100, max_iters=1000, random_state=3) # Predict labels for train set and assess accuracy y_train_pred = lr_nn_model2 . predict ( X_train_scaled ) y_train_accuracy = accuracy_score ( y_train_hot , y_train_pred ) y_train_accuracy 0.6833333333333333 # Predict labels for test set and assess accuracy y_test_pred = lr_nn_model2 . predict ( X_test_scaled ) y_test_accuracy = accuracy_score ( y_test_hot , y_test_pred ) y_test_accuracy 0.7","title":"2 - Fitness function in code"},{"location":"tutorial_examples/#overview","text":"mlrose_ky is a Python package for applying some of the most common randomized optimization and search algorithms to a range of different optimization problems, over both discrete- and continuous-valued parameter spaces. This notebook contains the examples used in the mlrose_ky tutorial.","title":"Overview"},{"location":"tutorial_examples/#import-libraries","text":"import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import MinMaxScaler , OneHotEncoder from sklearn.metrics import accuracy_score import mlrose_ky as mlrose","title":"Import Libraries"},{"location":"tutorial_examples/#example-1-8-queens-using-pre-defined-fitness-function","text":"# Initialize fitness function object using pre-defined class fitness = mlrose . Queens () # Define optimization problem object problem = mlrose . DiscreteOpt ( length = 8 , fitness_fn = fitness , maximize = False , max_val = 8 ) # Define decay schedule schedule = mlrose . ExpDecay () # Solve using simulated annealing - attempt 1 init_state = np . array ([ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 ]) best_state , best_fitness , _ = mlrose . simulated_annealing ( problem , schedule = schedule , max_iters = 1000 , init_state = init_state , random_state = 1 ) print ( \"The best state found is:\" , best_state ) The best state found is: [6 3 7 2 1 5 2 5] print ( \"The fitness at the best state is:\" , best_fitness ) The fitness at the best state is: 3.0 # Solve using simulated annealing - attempt 2 best_state , best_fitness , _ = mlrose . simulated_annealing ( problem , schedule = schedule , max_attempts = 100 , max_iters = 1000 , init_state = init_state , random_state = 1 ) best_state array([4, 2, 0, 6, 1, 7, 5, 3]) best_fitness 0.0","title":"Example 1: 8-Queens Using Pre-Defined Fitness Function"},{"location":"tutorial_examples/#example-2-8-queens-using-custom-fitness-function","text":"# Define alternative N-Queens fitness function for maximization problem def queens_max ( state ): # Initialize counter fitness = 0 # For all pairs of queens for i in range ( len ( state ) - 1 ): for j in range ( i + 1 , len ( state )): # Check for horizontal, diagonal-up and diagonal-down attacks if ( state [ j ] != state [ i ]) and ( state [ j ] != state [ i ] + ( j - i )) and ( state [ j ] != state [ i ] - ( j - i )): # If no attacks, then increment counter fitness += 1 return fitness # Check function is working correctly state = np . array ([ 1 , 4 , 1 , 3 , 5 , 5 , 2 , 7 ]) # The fitness of this state should be 22 queens_max ( state ) 22 # Initialize custom fitness function object fitness_cust = mlrose . CustomFitness ( queens_max ) # Define optimization problem object problem_cust = mlrose . DiscreteOpt ( length = 8 , fitness_fn = fitness_cust , max_val = 8 ) # Solve using simulated annealing - attempt 1 best_state , best_fitness , _ = mlrose . simulated_annealing ( problem_cust , schedule = schedule , max_iters = 1000 , init_state = init_state , random_state = 1 ) best_state array([6, 4, 7, 3, 6, 2, 5, 1]) best_fitness 26.0 # Solve using simulated annealing - attempt 2 best_state , best_fitness , _ = mlrose . simulated_annealing ( problem_cust , schedule = schedule , max_attempts = 100 , max_iters = 1000 , init_state = init_state , random_state = 1 ) best_state array([4, 1, 3, 5, 7, 2, 0, 6]) best_fitness 28.0","title":"Example 2: 8-Queens Using Custom Fitness Function"},{"location":"tutorial_examples/#example-3-travelling-salesperson-using-coordinate-defined-fitness-function","text":"# Create list of city coordinates coords_list = [( 1 , 1 ), ( 4 , 2 ), ( 5 , 2 ), ( 6 , 4 ), ( 4 , 4 ), ( 3 , 6 ), ( 1 , 5 ), ( 2 , 3 )] # Initialize fitness function object using coords_list fitness_coords = mlrose . TravellingSales ( coords = coords_list ) # Define optimization problem object problem_fit = mlrose . TSPOpt ( length = 8 , fitness_fn = fitness_coords ) # Solve using genetic algorithm - attempt 1 best_state , best_fitness , _ = mlrose . genetic_alg ( problem_fit , random_state = 2 ) best_state array([0, 7, 6, 5, 4, 3, 2, 1]) best_fitness 17.342617547667327 # Solve using genetic algorithm - attempt 2 best_state , best_fitness , _ = mlrose . genetic_alg ( problem_fit , mutation_prob = 0.2 , max_attempts = 100 , random_state = 2 ) best_state array([0, 7, 6, 5, 4, 3, 2, 1]) best_fitness 17.342617547667327","title":"Example 3: Travelling Salesperson Using Coordinate-Defined Fitness Function"},{"location":"tutorial_examples/#example-4-travelling-salesperson-using-distance-defined-fitness-function","text":"# Create list of distances between pairs of cities dist_list = [ ( 0 , 1 , 3.1623 ), ( 0 , 2 , 4.1231 ), ( 0 , 3 , 5.8310 ), ( 0 , 4 , 4.2426 ), ( 0 , 5 , 5.3852 ), ( 0 , 6 , 4.0000 ), ( 0 , 7 , 2.2361 ), ( 1 , 2 , 1.0000 ), ( 1 , 3 , 2.8284 ), ( 1 , 4 , 2.0000 ), ( 1 , 5 , 4.1231 ), ( 1 , 6 , 4.2426 ), ( 1 , 7 , 2.2361 ), ( 2 , 3 , 2.2361 ), ( 2 , 4 , 2.2361 ), ( 2 , 5 , 4.4721 ), ( 2 , 6 , 5.0000 ), ( 2 , 7 , 3.1623 ), ( 3 , 4 , 2.0000 ), ( 3 , 5 , 3.6056 ), ( 3 , 6 , 5.0990 ), ( 3 , 7 , 4.1231 ), ( 4 , 5 , 2.2361 ), ( 4 , 6 , 3.1623 ), ( 4 , 7 , 2.2361 ), ( 5 , 6 , 2.2361 ), ( 5 , 7 , 3.1623 ), ( 6 , 7 , 2.2361 ), ] # Initialize fitness function object using dist_list fitness_dists = mlrose . TravellingSales ( distances = dist_list ) # Define optimization problem object problem_fit2 = mlrose . TSPOpt ( length = 8 , fitness_fn = fitness_dists ) # Solve using genetic algorithm best_state , best_fitness , _ = mlrose . genetic_alg ( problem_fit2 , mutation_prob = 0.2 , max_attempts = 100 , random_state = 2 ) best_state array([7, 0, 1, 2, 3, 4, 5, 6]) best_fitness 17.3428","title":"Example 4: Travelling Salesperson Using Distance-Defined Fitness Function"},{"location":"tutorial_examples/#example-5-travelling-salesperson-defining-fitness-function-as-part-of-optimization-problem-definition-step","text":"# Create list of city coordinates coords_list = [( 1 , 1 ), ( 4 , 2 ), ( 5 , 2 ), ( 6 , 4 ), ( 4 , 4 ), ( 3 , 6 ), ( 1 , 5 ), ( 2 , 3 )] # Define optimization problem object problem_no_fit = mlrose . TSPOpt ( length = 8 , coords = coords_list ) # Solve using genetic algorithm best_state , best_fitness , _ = mlrose . genetic_alg ( problem_no_fit , mutation_prob = 0.2 , max_attempts = 100 , random_state = 2 ) best_state array([0, 7, 6, 5, 4, 3, 2, 1]) best_fitness 17.342617547667327","title":"Example 5: Travelling Salesperson Defining Fitness Function as Part of Optimization Problem Definition Step"},{"location":"tutorial_examples/#example-6-fitting-a-neural-network-to-the-iris-dataset","text":"# Load the Iris dataset data = load_iris () # Get feature values of first observation print ( data . data [ 0 ]) [5.1 3.5 1.4 0.2] # Get feature names print ( data . feature_names ) ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] # Get target value of first observation print ( data . target [ 0 ]) 0 # Get target name of first observation print ( data . target_names [ data . target [ 0 ]]) setosa # Get minimum feature values print ( np . min ( data . data , axis = 0 )) [4.3 2. 1. 0.1] # Get maximum feature values print ( np . max ( data . data , axis = 0 )) [7.9 4.4 6.9 2.5] # Get unique target values print ( np . unique ( data . target )) [0 1 2] # Split data into training and test sets X_train , X_test , y_train , y_test = train_test_split ( data . data , data . target , test_size = 0.2 , random_state = 3 ) # Normalize feature data scaler = MinMaxScaler () X_train_scaled = scaler . fit_transform ( X_train ) X_test_scaled = scaler . transform ( X_test ) # One hot encode target values one_hot = OneHotEncoder () y_train_hot = np . asarray ( one_hot . fit_transform ( y_train . reshape ( - 1 , 1 )) . todense ()) y_test_hot = np . asarray ( one_hot . transform ( y_test . reshape ( - 1 , 1 )) . todense ()) # Initialize neural network object and fit object - attempt 1 nn_model1 = mlrose . NeuralNetwork ( hidden_nodes = [ 2 ], max_iters = 1000 , learning_rate = 0.0001 , early_stopping = True , clip_max = 5 , max_attempts = 100 , random_state = 3 ) nn_model1 . fit ( X_train_scaled , y_train_hot ) #sk-container-id-1 { /* Definition of color scheme common for light and dark mode */ --sklearn-color-text: black; --sklearn-color-line: gray; /* Definition of color scheme for unfitted estimators */ --sklearn-color-unfitted-level-0: #fff5e6; --sklearn-color-unfitted-level-1: #f6e4d2; --sklearn-color-unfitted-level-2: #ffe0b3; --sklearn-color-unfitted-level-3: chocolate; /* Definition of color scheme for fitted estimators */ --sklearn-color-fitted-level-0: #f0f8ff; --sklearn-color-fitted-level-1: #d4ebff; --sklearn-color-fitted-level-2: #b3dbfd; --sklearn-color-fitted-level-3: cornflowerblue; /* Specific color for light theme */ --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black))); --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white))); --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black))); --sklearn-color-icon: #696969; @media (prefers-color-scheme: dark) { /* Redefinition of color scheme for dark theme */ --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white))); --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111))); --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white))); --sklearn-color-icon: #878787; } } #sk-container-id-1 { color: var(--sklearn-color-text); } #sk-container-id-1 pre { padding: 0; } #sk-container-id-1 input.sk-hidden--visually { border: 0; clip: rect(1px 1px 1px 1px); clip: rect(1px, 1px, 1px, 1px); height: 1px; margin: -1px; overflow: hidden; padding: 0; position: absolute; width: 1px; } #sk-container-id-1 div.sk-dashed-wrapped { border: 1px dashed var(--sklearn-color-line); margin: 0 0.4em 0.5em 0.4em; box-sizing: border-box; padding-bottom: 0.4em; background-color: var(--sklearn-color-background); } #sk-container-id-1 div.sk-container { /* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */ display: inline-block !important; position: relative; } #sk-container-id-1 div.sk-text-repr-fallback { display: none; } div.sk-parallel-item, div.sk-serial, div.sk-item { /* draw centered vertical line to link estimators */ background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background)); background-size: 2px 100%; background-repeat: no-repeat; background-position: center center; } /* Parallel-specific style estimator block */ #sk-container-id-1 div.sk-parallel-item::after { content: \"\"; width: 100%; border-bottom: 2px solid var(--sklearn-color-text-on-default-background); flex-grow: 1; } #sk-container-id-1 div.sk-parallel { display: flex; align-items: stretch; justify-content: center; background-color: var(--sklearn-color-background); position: relative; } #sk-container-id-1 div.sk-parallel-item { display: flex; flex-direction: column; } #sk-container-id-1 div.sk-parallel-item:first-child::after { align-self: flex-end; width: 50%; } #sk-container-id-1 div.sk-parallel-item:last-child::after { align-self: flex-start; width: 50%; } #sk-container-id-1 div.sk-parallel-item:only-child::after { width: 0; } /* Serial-specific style estimator block */ #sk-container-id-1 div.sk-serial { display: flex; flex-direction: column; align-items: center; background-color: var(--sklearn-color-background); padding-right: 1em; padding-left: 1em; } /* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is clickable and can be expanded/collapsed. - Pipeline and ColumnTransformer use this feature and define the default style - Estimators will overwrite some part of the style using the `sk-estimator` class */ /* Pipeline and ColumnTransformer style (default) */ #sk-container-id-1 div.sk-toggleable { /* Default theme specific background. It is overwritten whether we have a specific estimator or a Pipeline/ColumnTransformer */ background-color: var(--sklearn-color-background); } /* Toggleable label */ #sk-container-id-1 label.sk-toggleable__label { cursor: pointer; display: block; width: 100%; margin-bottom: 0; padding: 0.5em; box-sizing: border-box; text-align: center; } #sk-container-id-1 label.sk-toggleable__label-arrow:before { /* Arrow on the left of the label */ content: \"\u25b8\"; float: left; margin-right: 0.25em; color: var(--sklearn-color-icon); } #sk-container-id-1 label.sk-toggleable__label-arrow:hover:before { color: var(--sklearn-color-text); } /* Toggleable content - dropdown */ #sk-container-id-1 div.sk-toggleable__content { max-height: 0; max-width: 0; overflow: hidden; text-align: left; /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-1 div.sk-toggleable__content.fitted { /* fitted */ background-color: var(--sklearn-color-fitted-level-0); } #sk-container-id-1 div.sk-toggleable__content pre { margin: 0.2em; border-radius: 0.25em; color: var(--sklearn-color-text); /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-1 div.sk-toggleable__content.fitted pre { /* unfitted */ background-color: var(--sklearn-color-fitted-level-0); } #sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content { /* Expand drop-down */ max-height: 200px; max-width: 100%; overflow: auto; } #sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before { content: \"\u25be\"; } /* Pipeline/ColumnTransformer-specific style */ #sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label { color: var(--sklearn-color-text); background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label { background-color: var(--sklearn-color-fitted-level-2); } /* Estimator-specific style */ /* Colorize estimator box */ #sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label { /* fitted */ background-color: var(--sklearn-color-fitted-level-2); } #sk-container-id-1 div.sk-label label.sk-toggleable__label, #sk-container-id-1 div.sk-label label { /* The background is the default theme color */ color: var(--sklearn-color-text-on-default-background); } /* On hover, darken the color of the background */ #sk-container-id-1 div.sk-label:hover label.sk-toggleable__label { color: var(--sklearn-color-text); background-color: var(--sklearn-color-unfitted-level-2); } /* Label box, darken color on hover, fitted */ #sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted { color: var(--sklearn-color-text); background-color: var(--sklearn-color-fitted-level-2); } /* Estimator label */ #sk-container-id-1 div.sk-label label { font-family: monospace; font-weight: bold; display: inline-block; line-height: 1.2em; } #sk-container-id-1 div.sk-label-container { text-align: center; } /* Estimator-specific */ #sk-container-id-1 div.sk-estimator { font-family: monospace; border: 1px dotted var(--sklearn-color-border-box); border-radius: 0.25em; box-sizing: border-box; margin-bottom: 0.5em; /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-1 div.sk-estimator.fitted { /* fitted */ background-color: var(--sklearn-color-fitted-level-0); } /* on hover */ #sk-container-id-1 div.sk-estimator:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-1 div.sk-estimator.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-2); } /* Specification for estimator info (e.g. \"i\" and \"?\") */ /* Common style for \"i\" and \"?\" */ .sk-estimator-doc-link, a:link.sk-estimator-doc-link, a:visited.sk-estimator-doc-link { float: right; font-size: smaller; line-height: 1em; font-family: monospace; background-color: var(--sklearn-color-background); border-radius: 1em; height: 1em; width: 1em; text-decoration: none !important; margin-left: 1ex; /* unfitted */ border: var(--sklearn-color-unfitted-level-1) 1pt solid; color: var(--sklearn-color-unfitted-level-1); } .sk-estimator-doc-link.fitted, a:link.sk-estimator-doc-link.fitted, a:visited.sk-estimator-doc-link.fitted { /* fitted */ border: var(--sklearn-color-fitted-level-1) 1pt solid; color: var(--sklearn-color-fitted-level-1); } /* On hover */ div.sk-estimator:hover .sk-estimator-doc-link:hover, .sk-estimator-doc-link:hover, div.sk-label-container:hover .sk-estimator-doc-link:hover, .sk-estimator-doc-link:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover, .sk-estimator-doc-link.fitted:hover, div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover, .sk-estimator-doc-link.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } /* Span, style for the box shown on hovering the info icon */ .sk-estimator-doc-link span { display: none; z-index: 9999; position: relative; font-weight: normal; right: .2ex; padding: .5ex; margin: .5ex; width: min-content; min-width: 20ex; max-width: 50ex; color: var(--sklearn-color-text); box-shadow: 2pt 2pt 4pt #999; /* unfitted */ background: var(--sklearn-color-unfitted-level-0); border: .5pt solid var(--sklearn-color-unfitted-level-3); } .sk-estimator-doc-link.fitted span { /* fitted */ background: var(--sklearn-color-fitted-level-0); border: var(--sklearn-color-fitted-level-3); } .sk-estimator-doc-link:hover span { display: block; } /* \"?\"-specific style due to the `<a>` HTML tag */ #sk-container-id-1 a.estimator_doc_link { float: right; font-size: 1rem; line-height: 1em; font-family: monospace; background-color: var(--sklearn-color-background); border-radius: 1rem; height: 1rem; width: 1rem; text-decoration: none; /* unfitted */ color: var(--sklearn-color-unfitted-level-1); border: var(--sklearn-color-unfitted-level-1) 1pt solid; } #sk-container-id-1 a.estimator_doc_link.fitted { /* fitted */ border: var(--sklearn-color-fitted-level-1) 1pt solid; color: var(--sklearn-color-fitted-level-1); } /* On hover */ #sk-container-id-1 a.estimator_doc_link:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } #sk-container-id-1 a.estimator_doc_link.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-3); } NeuralNetwork(clip_max=5, early_stopping=True, hidden_nodes=[2], learning_rate=0.0001, max_attempts=100, max_iters=1000, random_state=3) In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. NeuralNetwork i Fitted NeuralNetwork(clip_max=5, early_stopping=True, hidden_nodes=[2], learning_rate=0.0001, max_attempts=100, max_iters=1000, random_state=3) # Predict labels for train set and assess accuracy y_train_pred = nn_model1 . predict ( X_train_scaled ) y_train_accuracy = accuracy_score ( y_train_hot , y_train_pred ) y_train_accuracy 0.45 # Predict labels for test set and assess accuracy y_test_pred = nn_model1 . predict ( X_test_scaled ) y_test_accuracy = accuracy_score ( y_test_hot , y_test_pred ) y_test_accuracy 0.5333333333333333 # Initialize neural network object and fit object - attempt 2 nn_model2 = mlrose . NeuralNetwork ( hidden_nodes = [ 2 ], algorithm = \"gradient_descent\" , max_iters = 1000 , learning_rate = 0.0001 , early_stopping = True , clip_max = 5 , max_attempts = 100 , random_state = 3 ) nn_model2 . fit ( X_train_scaled , y_train_hot ) #sk-container-id-2 { /* Definition of color scheme common for light and dark mode */ --sklearn-color-text: black; --sklearn-color-line: gray; /* Definition of color scheme for unfitted estimators */ --sklearn-color-unfitted-level-0: #fff5e6; --sklearn-color-unfitted-level-1: #f6e4d2; --sklearn-color-unfitted-level-2: #ffe0b3; --sklearn-color-unfitted-level-3: chocolate; /* Definition of color scheme for fitted estimators */ --sklearn-color-fitted-level-0: #f0f8ff; --sklearn-color-fitted-level-1: #d4ebff; --sklearn-color-fitted-level-2: #b3dbfd; --sklearn-color-fitted-level-3: cornflowerblue; /* Specific color for light theme */ --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black))); --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white))); --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black))); --sklearn-color-icon: #696969; @media (prefers-color-scheme: dark) { /* Redefinition of color scheme for dark theme */ --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white))); --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111))); --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white))); --sklearn-color-icon: #878787; } } #sk-container-id-2 { color: var(--sklearn-color-text); } #sk-container-id-2 pre { padding: 0; } #sk-container-id-2 input.sk-hidden--visually { border: 0; clip: rect(1px 1px 1px 1px); clip: rect(1px, 1px, 1px, 1px); height: 1px; margin: -1px; overflow: hidden; padding: 0; position: absolute; width: 1px; } #sk-container-id-2 div.sk-dashed-wrapped { border: 1px dashed var(--sklearn-color-line); margin: 0 0.4em 0.5em 0.4em; box-sizing: border-box; padding-bottom: 0.4em; background-color: var(--sklearn-color-background); } #sk-container-id-2 div.sk-container { /* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */ display: inline-block !important; position: relative; } #sk-container-id-2 div.sk-text-repr-fallback { display: none; } div.sk-parallel-item, div.sk-serial, div.sk-item { /* draw centered vertical line to link estimators */ background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background)); background-size: 2px 100%; background-repeat: no-repeat; background-position: center center; } /* Parallel-specific style estimator block */ #sk-container-id-2 div.sk-parallel-item::after { content: \"\"; width: 100%; border-bottom: 2px solid var(--sklearn-color-text-on-default-background); flex-grow: 1; } #sk-container-id-2 div.sk-parallel { display: flex; align-items: stretch; justify-content: center; background-color: var(--sklearn-color-background); position: relative; } #sk-container-id-2 div.sk-parallel-item { display: flex; flex-direction: column; } #sk-container-id-2 div.sk-parallel-item:first-child::after { align-self: flex-end; width: 50%; } #sk-container-id-2 div.sk-parallel-item:last-child::after { align-self: flex-start; width: 50%; } #sk-container-id-2 div.sk-parallel-item:only-child::after { width: 0; } /* Serial-specific style estimator block */ #sk-container-id-2 div.sk-serial { display: flex; flex-direction: column; align-items: center; background-color: var(--sklearn-color-background); padding-right: 1em; padding-left: 1em; } /* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is clickable and can be expanded/collapsed. - Pipeline and ColumnTransformer use this feature and define the default style - Estimators will overwrite some part of the style using the `sk-estimator` class */ /* Pipeline and ColumnTransformer style (default) */ #sk-container-id-2 div.sk-toggleable { /* Default theme specific background. It is overwritten whether we have a specific estimator or a Pipeline/ColumnTransformer */ background-color: var(--sklearn-color-background); } /* Toggleable label */ #sk-container-id-2 label.sk-toggleable__label { cursor: pointer; display: block; width: 100%; margin-bottom: 0; padding: 0.5em; box-sizing: border-box; text-align: center; } #sk-container-id-2 label.sk-toggleable__label-arrow:before { /* Arrow on the left of the label */ content: \"\u25b8\"; float: left; margin-right: 0.25em; color: var(--sklearn-color-icon); } #sk-container-id-2 label.sk-toggleable__label-arrow:hover:before { color: var(--sklearn-color-text); } /* Toggleable content - dropdown */ #sk-container-id-2 div.sk-toggleable__content { max-height: 0; max-width: 0; overflow: hidden; text-align: left; /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-2 div.sk-toggleable__content.fitted { /* fitted */ background-color: var(--sklearn-color-fitted-level-0); } #sk-container-id-2 div.sk-toggleable__content pre { margin: 0.2em; border-radius: 0.25em; color: var(--sklearn-color-text); /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-2 div.sk-toggleable__content.fitted pre { /* unfitted */ background-color: var(--sklearn-color-fitted-level-0); } #sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content { /* Expand drop-down */ max-height: 200px; max-width: 100%; overflow: auto; } #sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before { content: \"\u25be\"; } /* Pipeline/ColumnTransformer-specific style */ #sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label { color: var(--sklearn-color-text); background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label { background-color: var(--sklearn-color-fitted-level-2); } /* Estimator-specific style */ /* Colorize estimator box */ #sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label { /* fitted */ background-color: var(--sklearn-color-fitted-level-2); } #sk-container-id-2 div.sk-label label.sk-toggleable__label, #sk-container-id-2 div.sk-label label { /* The background is the default theme color */ color: var(--sklearn-color-text-on-default-background); } /* On hover, darken the color of the background */ #sk-container-id-2 div.sk-label:hover label.sk-toggleable__label { color: var(--sklearn-color-text); background-color: var(--sklearn-color-unfitted-level-2); } /* Label box, darken color on hover, fitted */ #sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted { color: var(--sklearn-color-text); background-color: var(--sklearn-color-fitted-level-2); } /* Estimator label */ #sk-container-id-2 div.sk-label label { font-family: monospace; font-weight: bold; display: inline-block; line-height: 1.2em; } #sk-container-id-2 div.sk-label-container { text-align: center; } /* Estimator-specific */ #sk-container-id-2 div.sk-estimator { font-family: monospace; border: 1px dotted var(--sklearn-color-border-box); border-radius: 0.25em; box-sizing: border-box; margin-bottom: 0.5em; /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-2 div.sk-estimator.fitted { /* fitted */ background-color: var(--sklearn-color-fitted-level-0); } /* on hover */ #sk-container-id-2 div.sk-estimator:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-2 div.sk-estimator.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-2); } /* Specification for estimator info (e.g. \"i\" and \"?\") */ /* Common style for \"i\" and \"?\" */ .sk-estimator-doc-link, a:link.sk-estimator-doc-link, a:visited.sk-estimator-doc-link { float: right; font-size: smaller; line-height: 1em; font-family: monospace; background-color: var(--sklearn-color-background); border-radius: 1em; height: 1em; width: 1em; text-decoration: none !important; margin-left: 1ex; /* unfitted */ border: var(--sklearn-color-unfitted-level-1) 1pt solid; color: var(--sklearn-color-unfitted-level-1); } .sk-estimator-doc-link.fitted, a:link.sk-estimator-doc-link.fitted, a:visited.sk-estimator-doc-link.fitted { /* fitted */ border: var(--sklearn-color-fitted-level-1) 1pt solid; color: var(--sklearn-color-fitted-level-1); } /* On hover */ div.sk-estimator:hover .sk-estimator-doc-link:hover, .sk-estimator-doc-link:hover, div.sk-label-container:hover .sk-estimator-doc-link:hover, .sk-estimator-doc-link:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover, .sk-estimator-doc-link.fitted:hover, div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover, .sk-estimator-doc-link.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } /* Span, style for the box shown on hovering the info icon */ .sk-estimator-doc-link span { display: none; z-index: 9999; position: relative; font-weight: normal; right: .2ex; padding: .5ex; margin: .5ex; width: min-content; min-width: 20ex; max-width: 50ex; color: var(--sklearn-color-text); box-shadow: 2pt 2pt 4pt #999; /* unfitted */ background: var(--sklearn-color-unfitted-level-0); border: .5pt solid var(--sklearn-color-unfitted-level-3); } .sk-estimator-doc-link.fitted span { /* fitted */ background: var(--sklearn-color-fitted-level-0); border: var(--sklearn-color-fitted-level-3); } .sk-estimator-doc-link:hover span { display: block; } /* \"?\"-specific style due to the `<a>` HTML tag */ #sk-container-id-2 a.estimator_doc_link { float: right; font-size: 1rem; line-height: 1em; font-family: monospace; background-color: var(--sklearn-color-background); border-radius: 1rem; height: 1rem; width: 1rem; text-decoration: none; /* unfitted */ color: var(--sklearn-color-unfitted-level-1); border: var(--sklearn-color-unfitted-level-1) 1pt solid; } #sk-container-id-2 a.estimator_doc_link.fitted { /* fitted */ border: var(--sklearn-color-fitted-level-1) 1pt solid; color: var(--sklearn-color-fitted-level-1); } /* On hover */ #sk-container-id-2 a.estimator_doc_link:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } #sk-container-id-2 a.estimator_doc_link.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-3); } NeuralNetwork(algorithm='gradient_descent', clip_max=5, early_stopping=True, hidden_nodes=[2], learning_rate=0.0001, max_attempts=100, max_iters=1000, random_state=3) In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. NeuralNetwork i Fitted NeuralNetwork(algorithm='gradient_descent', clip_max=5, early_stopping=True, hidden_nodes=[2], learning_rate=0.0001, max_attempts=100, max_iters=1000, random_state=3) # Predict labels for train set and assess accuracy y_train_pred = nn_model2 . predict ( X_train_scaled ) y_train_accuracy = accuracy_score ( y_train_hot , y_train_pred ) y_train_accuracy 0.625 # Predict labels for test set and assess accuracy y_test_pred = nn_model2 . predict ( X_test_scaled ) y_test_accuracy = accuracy_score ( y_test_hot , y_test_pred ) y_test_accuracy 0.5666666666666667","title":"Example 6: Fitting a Neural Network to the Iris Dataset"},{"location":"tutorial_examples/#example-7-fitting-a-logistic-regression-to-the-iris-data","text":"# Initialize logistic regression object and fit object - attempt 1 lr_model1 = mlrose . LogisticRegression ( max_iters = 1000 , learning_rate = 0.0001 , early_stopping = True , clip_max = 5 , max_attempts = 100 , random_state = 3 ) lr_model1 . fit ( X_train_scaled , y_train_hot ) #sk-container-id-3 { /* Definition of color scheme common for light and dark mode */ --sklearn-color-text: black; --sklearn-color-line: gray; /* Definition of color scheme for unfitted estimators */ --sklearn-color-unfitted-level-0: #fff5e6; --sklearn-color-unfitted-level-1: #f6e4d2; --sklearn-color-unfitted-level-2: #ffe0b3; --sklearn-color-unfitted-level-3: chocolate; /* Definition of color scheme for fitted estimators */ --sklearn-color-fitted-level-0: #f0f8ff; --sklearn-color-fitted-level-1: #d4ebff; --sklearn-color-fitted-level-2: #b3dbfd; --sklearn-color-fitted-level-3: cornflowerblue; /* Specific color for light theme */ --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black))); --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white))); --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black))); --sklearn-color-icon: #696969; @media (prefers-color-scheme: dark) { /* Redefinition of color scheme for dark theme */ --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white))); --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111))); --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white))); --sklearn-color-icon: #878787; } } #sk-container-id-3 { color: var(--sklearn-color-text); } #sk-container-id-3 pre { padding: 0; } #sk-container-id-3 input.sk-hidden--visually { border: 0; clip: rect(1px 1px 1px 1px); clip: rect(1px, 1px, 1px, 1px); height: 1px; margin: -1px; overflow: hidden; padding: 0; position: absolute; width: 1px; } #sk-container-id-3 div.sk-dashed-wrapped { border: 1px dashed var(--sklearn-color-line); margin: 0 0.4em 0.5em 0.4em; box-sizing: border-box; padding-bottom: 0.4em; background-color: var(--sklearn-color-background); } #sk-container-id-3 div.sk-container { /* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */ display: inline-block !important; position: relative; } #sk-container-id-3 div.sk-text-repr-fallback { display: none; } div.sk-parallel-item, div.sk-serial, div.sk-item { /* draw centered vertical line to link estimators */ background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background)); background-size: 2px 100%; background-repeat: no-repeat; background-position: center center; } /* Parallel-specific style estimator block */ #sk-container-id-3 div.sk-parallel-item::after { content: \"\"; width: 100%; border-bottom: 2px solid var(--sklearn-color-text-on-default-background); flex-grow: 1; } #sk-container-id-3 div.sk-parallel { display: flex; align-items: stretch; justify-content: center; background-color: var(--sklearn-color-background); position: relative; } #sk-container-id-3 div.sk-parallel-item { display: flex; flex-direction: column; } #sk-container-id-3 div.sk-parallel-item:first-child::after { align-self: flex-end; width: 50%; } #sk-container-id-3 div.sk-parallel-item:last-child::after { align-self: flex-start; width: 50%; } #sk-container-id-3 div.sk-parallel-item:only-child::after { width: 0; } /* Serial-specific style estimator block */ #sk-container-id-3 div.sk-serial { display: flex; flex-direction: column; align-items: center; background-color: var(--sklearn-color-background); padding-right: 1em; padding-left: 1em; } /* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is clickable and can be expanded/collapsed. - Pipeline and ColumnTransformer use this feature and define the default style - Estimators will overwrite some part of the style using the `sk-estimator` class */ /* Pipeline and ColumnTransformer style (default) */ #sk-container-id-3 div.sk-toggleable { /* Default theme specific background. It is overwritten whether we have a specific estimator or a Pipeline/ColumnTransformer */ background-color: var(--sklearn-color-background); } /* Toggleable label */ #sk-container-id-3 label.sk-toggleable__label { cursor: pointer; display: block; width: 100%; margin-bottom: 0; padding: 0.5em; box-sizing: border-box; text-align: center; } #sk-container-id-3 label.sk-toggleable__label-arrow:before { /* Arrow on the left of the label */ content: \"\u25b8\"; float: left; margin-right: 0.25em; color: var(--sklearn-color-icon); } #sk-container-id-3 label.sk-toggleable__label-arrow:hover:before { color: var(--sklearn-color-text); } /* Toggleable content - dropdown */ #sk-container-id-3 div.sk-toggleable__content { max-height: 0; max-width: 0; overflow: hidden; text-align: left; /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-3 div.sk-toggleable__content.fitted { /* fitted */ background-color: var(--sklearn-color-fitted-level-0); } #sk-container-id-3 div.sk-toggleable__content pre { margin: 0.2em; border-radius: 0.25em; color: var(--sklearn-color-text); /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-3 div.sk-toggleable__content.fitted pre { /* unfitted */ background-color: var(--sklearn-color-fitted-level-0); } #sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content { /* Expand drop-down */ max-height: 200px; max-width: 100%; overflow: auto; } #sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before { content: \"\u25be\"; } /* Pipeline/ColumnTransformer-specific style */ #sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label { color: var(--sklearn-color-text); background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label { background-color: var(--sklearn-color-fitted-level-2); } /* Estimator-specific style */ /* Colorize estimator box */ #sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label { /* fitted */ background-color: var(--sklearn-color-fitted-level-2); } #sk-container-id-3 div.sk-label label.sk-toggleable__label, #sk-container-id-3 div.sk-label label { /* The background is the default theme color */ color: var(--sklearn-color-text-on-default-background); } /* On hover, darken the color of the background */ #sk-container-id-3 div.sk-label:hover label.sk-toggleable__label { color: var(--sklearn-color-text); background-color: var(--sklearn-color-unfitted-level-2); } /* Label box, darken color on hover, fitted */ #sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted { color: var(--sklearn-color-text); background-color: var(--sklearn-color-fitted-level-2); } /* Estimator label */ #sk-container-id-3 div.sk-label label { font-family: monospace; font-weight: bold; display: inline-block; line-height: 1.2em; } #sk-container-id-3 div.sk-label-container { text-align: center; } /* Estimator-specific */ #sk-container-id-3 div.sk-estimator { font-family: monospace; border: 1px dotted var(--sklearn-color-border-box); border-radius: 0.25em; box-sizing: border-box; margin-bottom: 0.5em; /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-3 div.sk-estimator.fitted { /* fitted */ background-color: var(--sklearn-color-fitted-level-0); } /* on hover */ #sk-container-id-3 div.sk-estimator:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-3 div.sk-estimator.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-2); } /* Specification for estimator info (e.g. \"i\" and \"?\") */ /* Common style for \"i\" and \"?\" */ .sk-estimator-doc-link, a:link.sk-estimator-doc-link, a:visited.sk-estimator-doc-link { float: right; font-size: smaller; line-height: 1em; font-family: monospace; background-color: var(--sklearn-color-background); border-radius: 1em; height: 1em; width: 1em; text-decoration: none !important; margin-left: 1ex; /* unfitted */ border: var(--sklearn-color-unfitted-level-1) 1pt solid; color: var(--sklearn-color-unfitted-level-1); } .sk-estimator-doc-link.fitted, a:link.sk-estimator-doc-link.fitted, a:visited.sk-estimator-doc-link.fitted { /* fitted */ border: var(--sklearn-color-fitted-level-1) 1pt solid; color: var(--sklearn-color-fitted-level-1); } /* On hover */ div.sk-estimator:hover .sk-estimator-doc-link:hover, .sk-estimator-doc-link:hover, div.sk-label-container:hover .sk-estimator-doc-link:hover, .sk-estimator-doc-link:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover, .sk-estimator-doc-link.fitted:hover, div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover, .sk-estimator-doc-link.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } /* Span, style for the box shown on hovering the info icon */ .sk-estimator-doc-link span { display: none; z-index: 9999; position: relative; font-weight: normal; right: .2ex; padding: .5ex; margin: .5ex; width: min-content; min-width: 20ex; max-width: 50ex; color: var(--sklearn-color-text); box-shadow: 2pt 2pt 4pt #999; /* unfitted */ background: var(--sklearn-color-unfitted-level-0); border: .5pt solid var(--sklearn-color-unfitted-level-3); } .sk-estimator-doc-link.fitted span { /* fitted */ background: var(--sklearn-color-fitted-level-0); border: var(--sklearn-color-fitted-level-3); } .sk-estimator-doc-link:hover span { display: block; } /* \"?\"-specific style due to the `<a>` HTML tag */ #sk-container-id-3 a.estimator_doc_link { float: right; font-size: 1rem; line-height: 1em; font-family: monospace; background-color: var(--sklearn-color-background); border-radius: 1rem; height: 1rem; width: 1rem; text-decoration: none; /* unfitted */ color: var(--sklearn-color-unfitted-level-1); border: var(--sklearn-color-unfitted-level-1) 1pt solid; } #sk-container-id-3 a.estimator_doc_link.fitted { /* fitted */ border: var(--sklearn-color-fitted-level-1) 1pt solid; color: var(--sklearn-color-fitted-level-1); } /* On hover */ #sk-container-id-3 a.estimator_doc_link:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } #sk-container-id-3 a.estimator_doc_link.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-3); } LogisticRegression(clip_max=5, early_stopping=True, learning_rate=0.0001, max_attempts=100, max_iters=1000, random_state=3) In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. LogisticRegression i Fitted LogisticRegression(clip_max=5, early_stopping=True, learning_rate=0.0001, max_attempts=100, max_iters=1000, random_state=3) # Predict labels for train set and assess accuracy y_train_pred = lr_model1 . predict ( X_train_scaled ) y_train_accuracy = accuracy_score ( y_train_hot , y_train_pred ) y_train_accuracy 0.19166666666666668 # Predict labels for test set and assess accuracy y_test_pred = lr_model1 . predict ( X_test_scaled ) y_test_accuracy = accuracy_score ( y_test_hot , y_test_pred ) y_test_accuracy 0.06666666666666667 # Initialize logistic regression object and fit object - attempt 2 lr_model2 = mlrose . LogisticRegression ( max_iters = 1000 , learning_rate = 0.01 , early_stopping = True , clip_max = 5 , max_attempts = 100 , random_state = 3 ) lr_model2 . fit ( X_train_scaled , y_train_hot ) #sk-container-id-4 { /* Definition of color scheme common for light and dark mode */ --sklearn-color-text: black; --sklearn-color-line: gray; /* Definition of color scheme for unfitted estimators */ --sklearn-color-unfitted-level-0: #fff5e6; --sklearn-color-unfitted-level-1: #f6e4d2; --sklearn-color-unfitted-level-2: #ffe0b3; --sklearn-color-unfitted-level-3: chocolate; /* Definition of color scheme for fitted estimators */ --sklearn-color-fitted-level-0: #f0f8ff; --sklearn-color-fitted-level-1: #d4ebff; --sklearn-color-fitted-level-2: #b3dbfd; --sklearn-color-fitted-level-3: cornflowerblue; /* Specific color for light theme */ --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black))); --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white))); --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black))); --sklearn-color-icon: #696969; @media (prefers-color-scheme: dark) { /* Redefinition of color scheme for dark theme */ --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white))); --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111))); --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white))); --sklearn-color-icon: #878787; } } #sk-container-id-4 { color: var(--sklearn-color-text); } #sk-container-id-4 pre { padding: 0; } #sk-container-id-4 input.sk-hidden--visually { border: 0; clip: rect(1px 1px 1px 1px); clip: rect(1px, 1px, 1px, 1px); height: 1px; margin: -1px; overflow: hidden; padding: 0; position: absolute; width: 1px; } #sk-container-id-4 div.sk-dashed-wrapped { border: 1px dashed var(--sklearn-color-line); margin: 0 0.4em 0.5em 0.4em; box-sizing: border-box; padding-bottom: 0.4em; background-color: var(--sklearn-color-background); } #sk-container-id-4 div.sk-container { /* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */ display: inline-block !important; position: relative; } #sk-container-id-4 div.sk-text-repr-fallback { display: none; } div.sk-parallel-item, div.sk-serial, div.sk-item { /* draw centered vertical line to link estimators */ background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background)); background-size: 2px 100%; background-repeat: no-repeat; background-position: center center; } /* Parallel-specific style estimator block */ #sk-container-id-4 div.sk-parallel-item::after { content: \"\"; width: 100%; border-bottom: 2px solid var(--sklearn-color-text-on-default-background); flex-grow: 1; } #sk-container-id-4 div.sk-parallel { display: flex; align-items: stretch; justify-content: center; background-color: var(--sklearn-color-background); position: relative; } #sk-container-id-4 div.sk-parallel-item { display: flex; flex-direction: column; } #sk-container-id-4 div.sk-parallel-item:first-child::after { align-self: flex-end; width: 50%; } #sk-container-id-4 div.sk-parallel-item:last-child::after { align-self: flex-start; width: 50%; } #sk-container-id-4 div.sk-parallel-item:only-child::after { width: 0; } /* Serial-specific style estimator block */ #sk-container-id-4 div.sk-serial { display: flex; flex-direction: column; align-items: center; background-color: var(--sklearn-color-background); padding-right: 1em; padding-left: 1em; } /* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is clickable and can be expanded/collapsed. - Pipeline and ColumnTransformer use this feature and define the default style - Estimators will overwrite some part of the style using the `sk-estimator` class */ /* Pipeline and ColumnTransformer style (default) */ #sk-container-id-4 div.sk-toggleable { /* Default theme specific background. It is overwritten whether we have a specific estimator or a Pipeline/ColumnTransformer */ background-color: var(--sklearn-color-background); } /* Toggleable label */ #sk-container-id-4 label.sk-toggleable__label { cursor: pointer; display: block; width: 100%; margin-bottom: 0; padding: 0.5em; box-sizing: border-box; text-align: center; } #sk-container-id-4 label.sk-toggleable__label-arrow:before { /* Arrow on the left of the label */ content: \"\u25b8\"; float: left; margin-right: 0.25em; color: var(--sklearn-color-icon); } #sk-container-id-4 label.sk-toggleable__label-arrow:hover:before { color: var(--sklearn-color-text); } /* Toggleable content - dropdown */ #sk-container-id-4 div.sk-toggleable__content { max-height: 0; max-width: 0; overflow: hidden; text-align: left; /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-4 div.sk-toggleable__content.fitted { /* fitted */ background-color: var(--sklearn-color-fitted-level-0); } #sk-container-id-4 div.sk-toggleable__content pre { margin: 0.2em; border-radius: 0.25em; color: var(--sklearn-color-text); /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-4 div.sk-toggleable__content.fitted pre { /* unfitted */ background-color: var(--sklearn-color-fitted-level-0); } #sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content { /* Expand drop-down */ max-height: 200px; max-width: 100%; overflow: auto; } #sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before { content: \"\u25be\"; } /* Pipeline/ColumnTransformer-specific style */ #sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label { color: var(--sklearn-color-text); background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label { background-color: var(--sklearn-color-fitted-level-2); } /* Estimator-specific style */ /* Colorize estimator box */ #sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label { /* fitted */ background-color: var(--sklearn-color-fitted-level-2); } #sk-container-id-4 div.sk-label label.sk-toggleable__label, #sk-container-id-4 div.sk-label label { /* The background is the default theme color */ color: var(--sklearn-color-text-on-default-background); } /* On hover, darken the color of the background */ #sk-container-id-4 div.sk-label:hover label.sk-toggleable__label { color: var(--sklearn-color-text); background-color: var(--sklearn-color-unfitted-level-2); } /* Label box, darken color on hover, fitted */ #sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted { color: var(--sklearn-color-text); background-color: var(--sklearn-color-fitted-level-2); } /* Estimator label */ #sk-container-id-4 div.sk-label label { font-family: monospace; font-weight: bold; display: inline-block; line-height: 1.2em; } #sk-container-id-4 div.sk-label-container { text-align: center; } /* Estimator-specific */ #sk-container-id-4 div.sk-estimator { font-family: monospace; border: 1px dotted var(--sklearn-color-border-box); border-radius: 0.25em; box-sizing: border-box; margin-bottom: 0.5em; /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-4 div.sk-estimator.fitted { /* fitted */ background-color: var(--sklearn-color-fitted-level-0); } /* on hover */ #sk-container-id-4 div.sk-estimator:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-4 div.sk-estimator.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-2); } /* Specification for estimator info (e.g. \"i\" and \"?\") */ /* Common style for \"i\" and \"?\" */ .sk-estimator-doc-link, a:link.sk-estimator-doc-link, a:visited.sk-estimator-doc-link { float: right; font-size: smaller; line-height: 1em; font-family: monospace; background-color: var(--sklearn-color-background); border-radius: 1em; height: 1em; width: 1em; text-decoration: none !important; margin-left: 1ex; /* unfitted */ border: var(--sklearn-color-unfitted-level-1) 1pt solid; color: var(--sklearn-color-unfitted-level-1); } .sk-estimator-doc-link.fitted, a:link.sk-estimator-doc-link.fitted, a:visited.sk-estimator-doc-link.fitted { /* fitted */ border: var(--sklearn-color-fitted-level-1) 1pt solid; color: var(--sklearn-color-fitted-level-1); } /* On hover */ div.sk-estimator:hover .sk-estimator-doc-link:hover, .sk-estimator-doc-link:hover, div.sk-label-container:hover .sk-estimator-doc-link:hover, .sk-estimator-doc-link:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover, .sk-estimator-doc-link.fitted:hover, div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover, .sk-estimator-doc-link.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } /* Span, style for the box shown on hovering the info icon */ .sk-estimator-doc-link span { display: none; z-index: 9999; position: relative; font-weight: normal; right: .2ex; padding: .5ex; margin: .5ex; width: min-content; min-width: 20ex; max-width: 50ex; color: var(--sklearn-color-text); box-shadow: 2pt 2pt 4pt #999; /* unfitted */ background: var(--sklearn-color-unfitted-level-0); border: .5pt solid var(--sklearn-color-unfitted-level-3); } .sk-estimator-doc-link.fitted span { /* fitted */ background: var(--sklearn-color-fitted-level-0); border: var(--sklearn-color-fitted-level-3); } .sk-estimator-doc-link:hover span { display: block; } /* \"?\"-specific style due to the `<a>` HTML tag */ #sk-container-id-4 a.estimator_doc_link { float: right; font-size: 1rem; line-height: 1em; font-family: monospace; background-color: var(--sklearn-color-background); border-radius: 1rem; height: 1rem; width: 1rem; text-decoration: none; /* unfitted */ color: var(--sklearn-color-unfitted-level-1); border: var(--sklearn-color-unfitted-level-1) 1pt solid; } #sk-container-id-4 a.estimator_doc_link.fitted { /* fitted */ border: var(--sklearn-color-fitted-level-1) 1pt solid; color: var(--sklearn-color-fitted-level-1); } /* On hover */ #sk-container-id-4 a.estimator_doc_link:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } #sk-container-id-4 a.estimator_doc_link.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-3); } LogisticRegression(clip_max=5, early_stopping=True, learning_rate=0.01, max_attempts=100, max_iters=1000, random_state=3) In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. LogisticRegression i Fitted LogisticRegression(clip_max=5, early_stopping=True, learning_rate=0.01, max_attempts=100, max_iters=1000, random_state=3) # Predict labels for train set and assess accuracy y_train_pred = lr_model2 . predict ( X_train_scaled ) y_train_accuracy = accuracy_score ( y_train_hot , y_train_pred ) y_train_accuracy 0.6833333333333333 # Predict labels for test set and assess accuracy y_test_pred = lr_model2 . predict ( X_test_scaled ) y_test_accuracy = accuracy_score ( y_test_hot , y_test_pred ) y_test_accuracy 0.7","title":"Example 7: Fitting a Logistic Regression to the Iris Data"},{"location":"tutorial_examples/#example-8-fitting-a-logistic-regression-to-the-iris-data-using-the-neuralnetwork-class","text":"# Initialize neural network object and fit object - attempt 1 lr_nn_model1 = mlrose . NeuralNetwork ( hidden_nodes = [], activation = \"sigmoid\" , max_iters = 1000 , learning_rate = 0.0001 , early_stopping = True , clip_max = 5 , max_attempts = 100 , random_state = 3 ) lr_nn_model1 . fit ( X_train_scaled , y_train_hot ) #sk-container-id-5 { /* Definition of color scheme common for light and dark mode */ --sklearn-color-text: black; --sklearn-color-line: gray; /* Definition of color scheme for unfitted estimators */ --sklearn-color-unfitted-level-0: #fff5e6; --sklearn-color-unfitted-level-1: #f6e4d2; --sklearn-color-unfitted-level-2: #ffe0b3; --sklearn-color-unfitted-level-3: chocolate; /* Definition of color scheme for fitted estimators */ --sklearn-color-fitted-level-0: #f0f8ff; --sklearn-color-fitted-level-1: #d4ebff; --sklearn-color-fitted-level-2: #b3dbfd; --sklearn-color-fitted-level-3: cornflowerblue; /* Specific color for light theme */ --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black))); --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white))); --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black))); --sklearn-color-icon: #696969; @media (prefers-color-scheme: dark) { /* Redefinition of color scheme for dark theme */ --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white))); --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111))); --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white))); --sklearn-color-icon: #878787; } } #sk-container-id-5 { color: var(--sklearn-color-text); } #sk-container-id-5 pre { padding: 0; } #sk-container-id-5 input.sk-hidden--visually { border: 0; clip: rect(1px 1px 1px 1px); clip: rect(1px, 1px, 1px, 1px); height: 1px; margin: -1px; overflow: hidden; padding: 0; position: absolute; width: 1px; } #sk-container-id-5 div.sk-dashed-wrapped { border: 1px dashed var(--sklearn-color-line); margin: 0 0.4em 0.5em 0.4em; box-sizing: border-box; padding-bottom: 0.4em; background-color: var(--sklearn-color-background); } #sk-container-id-5 div.sk-container { /* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */ display: inline-block !important; position: relative; } #sk-container-id-5 div.sk-text-repr-fallback { display: none; } div.sk-parallel-item, div.sk-serial, div.sk-item { /* draw centered vertical line to link estimators */ background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background)); background-size: 2px 100%; background-repeat: no-repeat; background-position: center center; } /* Parallel-specific style estimator block */ #sk-container-id-5 div.sk-parallel-item::after { content: \"\"; width: 100%; border-bottom: 2px solid var(--sklearn-color-text-on-default-background); flex-grow: 1; } #sk-container-id-5 div.sk-parallel { display: flex; align-items: stretch; justify-content: center; background-color: var(--sklearn-color-background); position: relative; } #sk-container-id-5 div.sk-parallel-item { display: flex; flex-direction: column; } #sk-container-id-5 div.sk-parallel-item:first-child::after { align-self: flex-end; width: 50%; } #sk-container-id-5 div.sk-parallel-item:last-child::after { align-self: flex-start; width: 50%; } #sk-container-id-5 div.sk-parallel-item:only-child::after { width: 0; } /* Serial-specific style estimator block */ #sk-container-id-5 div.sk-serial { display: flex; flex-direction: column; align-items: center; background-color: var(--sklearn-color-background); padding-right: 1em; padding-left: 1em; } /* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is clickable and can be expanded/collapsed. - Pipeline and ColumnTransformer use this feature and define the default style - Estimators will overwrite some part of the style using the `sk-estimator` class */ /* Pipeline and ColumnTransformer style (default) */ #sk-container-id-5 div.sk-toggleable { /* Default theme specific background. It is overwritten whether we have a specific estimator or a Pipeline/ColumnTransformer */ background-color: var(--sklearn-color-background); } /* Toggleable label */ #sk-container-id-5 label.sk-toggleable__label { cursor: pointer; display: block; width: 100%; margin-bottom: 0; padding: 0.5em; box-sizing: border-box; text-align: center; } #sk-container-id-5 label.sk-toggleable__label-arrow:before { /* Arrow on the left of the label */ content: \"\u25b8\"; float: left; margin-right: 0.25em; color: var(--sklearn-color-icon); } #sk-container-id-5 label.sk-toggleable__label-arrow:hover:before { color: var(--sklearn-color-text); } /* Toggleable content - dropdown */ #sk-container-id-5 div.sk-toggleable__content { max-height: 0; max-width: 0; overflow: hidden; text-align: left; /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-5 div.sk-toggleable__content.fitted { /* fitted */ background-color: var(--sklearn-color-fitted-level-0); } #sk-container-id-5 div.sk-toggleable__content pre { margin: 0.2em; border-radius: 0.25em; color: var(--sklearn-color-text); /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-5 div.sk-toggleable__content.fitted pre { /* unfitted */ background-color: var(--sklearn-color-fitted-level-0); } #sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content { /* Expand drop-down */ max-height: 200px; max-width: 100%; overflow: auto; } #sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before { content: \"\u25be\"; } /* Pipeline/ColumnTransformer-specific style */ #sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label { color: var(--sklearn-color-text); background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-5 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label { background-color: var(--sklearn-color-fitted-level-2); } /* Estimator-specific style */ /* Colorize estimator box */ #sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-5 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label { /* fitted */ background-color: var(--sklearn-color-fitted-level-2); } #sk-container-id-5 div.sk-label label.sk-toggleable__label, #sk-container-id-5 div.sk-label label { /* The background is the default theme color */ color: var(--sklearn-color-text-on-default-background); } /* On hover, darken the color of the background */ #sk-container-id-5 div.sk-label:hover label.sk-toggleable__label { color: var(--sklearn-color-text); background-color: var(--sklearn-color-unfitted-level-2); } /* Label box, darken color on hover, fitted */ #sk-container-id-5 div.sk-label.fitted:hover label.sk-toggleable__label.fitted { color: var(--sklearn-color-text); background-color: var(--sklearn-color-fitted-level-2); } /* Estimator label */ #sk-container-id-5 div.sk-label label { font-family: monospace; font-weight: bold; display: inline-block; line-height: 1.2em; } #sk-container-id-5 div.sk-label-container { text-align: center; } /* Estimator-specific */ #sk-container-id-5 div.sk-estimator { font-family: monospace; border: 1px dotted var(--sklearn-color-border-box); border-radius: 0.25em; box-sizing: border-box; margin-bottom: 0.5em; /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-5 div.sk-estimator.fitted { /* fitted */ background-color: var(--sklearn-color-fitted-level-0); } /* on hover */ #sk-container-id-5 div.sk-estimator:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-5 div.sk-estimator.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-2); } /* Specification for estimator info (e.g. \"i\" and \"?\") */ /* Common style for \"i\" and \"?\" */ .sk-estimator-doc-link, a:link.sk-estimator-doc-link, a:visited.sk-estimator-doc-link { float: right; font-size: smaller; line-height: 1em; font-family: monospace; background-color: var(--sklearn-color-background); border-radius: 1em; height: 1em; width: 1em; text-decoration: none !important; margin-left: 1ex; /* unfitted */ border: var(--sklearn-color-unfitted-level-1) 1pt solid; color: var(--sklearn-color-unfitted-level-1); } .sk-estimator-doc-link.fitted, a:link.sk-estimator-doc-link.fitted, a:visited.sk-estimator-doc-link.fitted { /* fitted */ border: var(--sklearn-color-fitted-level-1) 1pt solid; color: var(--sklearn-color-fitted-level-1); } /* On hover */ div.sk-estimator:hover .sk-estimator-doc-link:hover, .sk-estimator-doc-link:hover, div.sk-label-container:hover .sk-estimator-doc-link:hover, .sk-estimator-doc-link:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover, .sk-estimator-doc-link.fitted:hover, div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover, .sk-estimator-doc-link.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } /* Span, style for the box shown on hovering the info icon */ .sk-estimator-doc-link span { display: none; z-index: 9999; position: relative; font-weight: normal; right: .2ex; padding: .5ex; margin: .5ex; width: min-content; min-width: 20ex; max-width: 50ex; color: var(--sklearn-color-text); box-shadow: 2pt 2pt 4pt #999; /* unfitted */ background: var(--sklearn-color-unfitted-level-0); border: .5pt solid var(--sklearn-color-unfitted-level-3); } .sk-estimator-doc-link.fitted span { /* fitted */ background: var(--sklearn-color-fitted-level-0); border: var(--sklearn-color-fitted-level-3); } .sk-estimator-doc-link:hover span { display: block; } /* \"?\"-specific style due to the `<a>` HTML tag */ #sk-container-id-5 a.estimator_doc_link { float: right; font-size: 1rem; line-height: 1em; font-family: monospace; background-color: var(--sklearn-color-background); border-radius: 1rem; height: 1rem; width: 1rem; text-decoration: none; /* unfitted */ color: var(--sklearn-color-unfitted-level-1); border: var(--sklearn-color-unfitted-level-1) 1pt solid; } #sk-container-id-5 a.estimator_doc_link.fitted { /* fitted */ border: var(--sklearn-color-fitted-level-1) 1pt solid; color: var(--sklearn-color-fitted-level-1); } /* On hover */ #sk-container-id-5 a.estimator_doc_link:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } #sk-container-id-5 a.estimator_doc_link.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-3); } NeuralNetwork(activation='sigmoid', clip_max=5, early_stopping=True, hidden_nodes=[], learning_rate=0.0001, max_attempts=100, max_iters=1000, random_state=3) In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. NeuralNetwork i Fitted NeuralNetwork(activation='sigmoid', clip_max=5, early_stopping=True, hidden_nodes=[], learning_rate=0.0001, max_attempts=100, max_iters=1000, random_state=3) # Predict labels for train set and assess accuracy y_train_pred = lr_nn_model1 . predict ( X_train_scaled ) y_train_accuracy = accuracy_score ( y_train_hot , y_train_pred ) y_train_accuracy 0.19166666666666668 # Predict labels for test set and assess accuracy y_test_pred = lr_nn_model1 . predict ( X_test_scaled ) y_test_accuracy = accuracy_score ( y_test_hot , y_test_pred ) y_test_accuracy 0.06666666666666667 # Initialize neural network object and fit object - attempt 2 lr_nn_model2 = mlrose . NeuralNetwork ( hidden_nodes = [], activation = \"sigmoid\" , max_iters = 1000 , learning_rate = 0.01 , early_stopping = True , clip_max = 5 , max_attempts = 100 , random_state = 3 ) lr_nn_model2 . fit ( X_train_scaled , y_train_hot ) #sk-container-id-6 { /* Definition of color scheme common for light and dark mode */ --sklearn-color-text: black; --sklearn-color-line: gray; /* Definition of color scheme for unfitted estimators */ --sklearn-color-unfitted-level-0: #fff5e6; --sklearn-color-unfitted-level-1: #f6e4d2; --sklearn-color-unfitted-level-2: #ffe0b3; --sklearn-color-unfitted-level-3: chocolate; /* Definition of color scheme for fitted estimators */ --sklearn-color-fitted-level-0: #f0f8ff; --sklearn-color-fitted-level-1: #d4ebff; --sklearn-color-fitted-level-2: #b3dbfd; --sklearn-color-fitted-level-3: cornflowerblue; /* Specific color for light theme */ --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black))); --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white))); --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black))); --sklearn-color-icon: #696969; @media (prefers-color-scheme: dark) { /* Redefinition of color scheme for dark theme */ --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white))); --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111))); --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white))); --sklearn-color-icon: #878787; } } #sk-container-id-6 { color: var(--sklearn-color-text); } #sk-container-id-6 pre { padding: 0; } #sk-container-id-6 input.sk-hidden--visually { border: 0; clip: rect(1px 1px 1px 1px); clip: rect(1px, 1px, 1px, 1px); height: 1px; margin: -1px; overflow: hidden; padding: 0; position: absolute; width: 1px; } #sk-container-id-6 div.sk-dashed-wrapped { border: 1px dashed var(--sklearn-color-line); margin: 0 0.4em 0.5em 0.4em; box-sizing: border-box; padding-bottom: 0.4em; background-color: var(--sklearn-color-background); } #sk-container-id-6 div.sk-container { /* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */ display: inline-block !important; position: relative; } #sk-container-id-6 div.sk-text-repr-fallback { display: none; } div.sk-parallel-item, div.sk-serial, div.sk-item { /* draw centered vertical line to link estimators */ background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background)); background-size: 2px 100%; background-repeat: no-repeat; background-position: center center; } /* Parallel-specific style estimator block */ #sk-container-id-6 div.sk-parallel-item::after { content: \"\"; width: 100%; border-bottom: 2px solid var(--sklearn-color-text-on-default-background); flex-grow: 1; } #sk-container-id-6 div.sk-parallel { display: flex; align-items: stretch; justify-content: center; background-color: var(--sklearn-color-background); position: relative; } #sk-container-id-6 div.sk-parallel-item { display: flex; flex-direction: column; } #sk-container-id-6 div.sk-parallel-item:first-child::after { align-self: flex-end; width: 50%; } #sk-container-id-6 div.sk-parallel-item:last-child::after { align-self: flex-start; width: 50%; } #sk-container-id-6 div.sk-parallel-item:only-child::after { width: 0; } /* Serial-specific style estimator block */ #sk-container-id-6 div.sk-serial { display: flex; flex-direction: column; align-items: center; background-color: var(--sklearn-color-background); padding-right: 1em; padding-left: 1em; } /* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is clickable and can be expanded/collapsed. - Pipeline and ColumnTransformer use this feature and define the default style - Estimators will overwrite some part of the style using the `sk-estimator` class */ /* Pipeline and ColumnTransformer style (default) */ #sk-container-id-6 div.sk-toggleable { /* Default theme specific background. It is overwritten whether we have a specific estimator or a Pipeline/ColumnTransformer */ background-color: var(--sklearn-color-background); } /* Toggleable label */ #sk-container-id-6 label.sk-toggleable__label { cursor: pointer; display: block; width: 100%; margin-bottom: 0; padding: 0.5em; box-sizing: border-box; text-align: center; } #sk-container-id-6 label.sk-toggleable__label-arrow:before { /* Arrow on the left of the label */ content: \"\u25b8\"; float: left; margin-right: 0.25em; color: var(--sklearn-color-icon); } #sk-container-id-6 label.sk-toggleable__label-arrow:hover:before { color: var(--sklearn-color-text); } /* Toggleable content - dropdown */ #sk-container-id-6 div.sk-toggleable__content { max-height: 0; max-width: 0; overflow: hidden; text-align: left; /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-6 div.sk-toggleable__content.fitted { /* fitted */ background-color: var(--sklearn-color-fitted-level-0); } #sk-container-id-6 div.sk-toggleable__content pre { margin: 0.2em; border-radius: 0.25em; color: var(--sklearn-color-text); /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-6 div.sk-toggleable__content.fitted pre { /* unfitted */ background-color: var(--sklearn-color-fitted-level-0); } #sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content { /* Expand drop-down */ max-height: 200px; max-width: 100%; overflow: auto; } #sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before { content: \"\u25be\"; } /* Pipeline/ColumnTransformer-specific style */ #sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label { color: var(--sklearn-color-text); background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-6 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label { background-color: var(--sklearn-color-fitted-level-2); } /* Estimator-specific style */ /* Colorize estimator box */ #sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-6 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label { /* fitted */ background-color: var(--sklearn-color-fitted-level-2); } #sk-container-id-6 div.sk-label label.sk-toggleable__label, #sk-container-id-6 div.sk-label label { /* The background is the default theme color */ color: var(--sklearn-color-text-on-default-background); } /* On hover, darken the color of the background */ #sk-container-id-6 div.sk-label:hover label.sk-toggleable__label { color: var(--sklearn-color-text); background-color: var(--sklearn-color-unfitted-level-2); } /* Label box, darken color on hover, fitted */ #sk-container-id-6 div.sk-label.fitted:hover label.sk-toggleable__label.fitted { color: var(--sklearn-color-text); background-color: var(--sklearn-color-fitted-level-2); } /* Estimator label */ #sk-container-id-6 div.sk-label label { font-family: monospace; font-weight: bold; display: inline-block; line-height: 1.2em; } #sk-container-id-6 div.sk-label-container { text-align: center; } /* Estimator-specific */ #sk-container-id-6 div.sk-estimator { font-family: monospace; border: 1px dotted var(--sklearn-color-border-box); border-radius: 0.25em; box-sizing: border-box; margin-bottom: 0.5em; /* unfitted */ background-color: var(--sklearn-color-unfitted-level-0); } #sk-container-id-6 div.sk-estimator.fitted { /* fitted */ background-color: var(--sklearn-color-fitted-level-0); } /* on hover */ #sk-container-id-6 div.sk-estimator:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-2); } #sk-container-id-6 div.sk-estimator.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-2); } /* Specification for estimator info (e.g. \"i\" and \"?\") */ /* Common style for \"i\" and \"?\" */ .sk-estimator-doc-link, a:link.sk-estimator-doc-link, a:visited.sk-estimator-doc-link { float: right; font-size: smaller; line-height: 1em; font-family: monospace; background-color: var(--sklearn-color-background); border-radius: 1em; height: 1em; width: 1em; text-decoration: none !important; margin-left: 1ex; /* unfitted */ border: var(--sklearn-color-unfitted-level-1) 1pt solid; color: var(--sklearn-color-unfitted-level-1); } .sk-estimator-doc-link.fitted, a:link.sk-estimator-doc-link.fitted, a:visited.sk-estimator-doc-link.fitted { /* fitted */ border: var(--sklearn-color-fitted-level-1) 1pt solid; color: var(--sklearn-color-fitted-level-1); } /* On hover */ div.sk-estimator:hover .sk-estimator-doc-link:hover, .sk-estimator-doc-link:hover, div.sk-label-container:hover .sk-estimator-doc-link:hover, .sk-estimator-doc-link:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover, .sk-estimator-doc-link.fitted:hover, div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover, .sk-estimator-doc-link.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } /* Span, style for the box shown on hovering the info icon */ .sk-estimator-doc-link span { display: none; z-index: 9999; position: relative; font-weight: normal; right: .2ex; padding: .5ex; margin: .5ex; width: min-content; min-width: 20ex; max-width: 50ex; color: var(--sklearn-color-text); box-shadow: 2pt 2pt 4pt #999; /* unfitted */ background: var(--sklearn-color-unfitted-level-0); border: .5pt solid var(--sklearn-color-unfitted-level-3); } .sk-estimator-doc-link.fitted span { /* fitted */ background: var(--sklearn-color-fitted-level-0); border: var(--sklearn-color-fitted-level-3); } .sk-estimator-doc-link:hover span { display: block; } /* \"?\"-specific style due to the `<a>` HTML tag */ #sk-container-id-6 a.estimator_doc_link { float: right; font-size: 1rem; line-height: 1em; font-family: monospace; background-color: var(--sklearn-color-background); border-radius: 1rem; height: 1rem; width: 1rem; text-decoration: none; /* unfitted */ color: var(--sklearn-color-unfitted-level-1); border: var(--sklearn-color-unfitted-level-1) 1pt solid; } #sk-container-id-6 a.estimator_doc_link.fitted { /* fitted */ border: var(--sklearn-color-fitted-level-1) 1pt solid; color: var(--sklearn-color-fitted-level-1); } /* On hover */ #sk-container-id-6 a.estimator_doc_link:hover { /* unfitted */ background-color: var(--sklearn-color-unfitted-level-3); color: var(--sklearn-color-background); text-decoration: none; } #sk-container-id-6 a.estimator_doc_link.fitted:hover { /* fitted */ background-color: var(--sklearn-color-fitted-level-3); } NeuralNetwork(activation='sigmoid', clip_max=5, early_stopping=True, hidden_nodes=[], learning_rate=0.01, max_attempts=100, max_iters=1000, random_state=3) In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. NeuralNetwork i Fitted NeuralNetwork(activation='sigmoid', clip_max=5, early_stopping=True, hidden_nodes=[], learning_rate=0.01, max_attempts=100, max_iters=1000, random_state=3) # Predict labels for train set and assess accuracy y_train_pred = lr_nn_model2 . predict ( X_train_scaled ) y_train_accuracy = accuracy_score ( y_train_hot , y_train_pred ) y_train_accuracy 0.6833333333333333 # Predict labels for test set and assess accuracy y_test_pred = lr_nn_model2 . predict ( X_test_scaled ) y_test_accuracy = accuracy_score ( y_test_hot , y_test_pred ) y_test_accuracy 0.7","title":"Example 8: Fitting a Logistic Regression to the Iris Data using the NeuralNetwork() class"}]}